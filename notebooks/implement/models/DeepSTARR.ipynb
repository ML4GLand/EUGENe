{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661f6b58",
   "metadata": {},
   "source": [
    "# Testing `DeepSTARR` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64213c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *11/05/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook for testing out the custom `DeepSTARR` model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base import BaseModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from eugene.models.base._utils import GetFlattenDim\n",
    "from eugene.models.base import BasicFullyConnectedModule, BasicConv1D\n",
    "\n",
    "\n",
    "class DeepSTARR(BaseModel):\n",
    "    \"\"\"DeepSTARR model from de Almeida et al., 2022; \n",
    "        see <https://www.nature.com/articles/s41588-022-01048-5>\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_len: int = 249,\n",
    "        output_dim = 2, \n",
    "        strand = \"ss\",\n",
    "        task = \"regression\",\n",
    "        aggr = None,\n",
    "        loss_fxn = \"mse\",\n",
    "        conv_kwargs = {},\n",
    "        fc_kwargs = {},\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            input_len, \n",
    "            output_dim, \n",
    "            strand=strand, \n",
    "            task=task, \n",
    "            aggr=aggr, \n",
    "            loss_fxn=loss_fxn, \n",
    "            **kwargs\n",
    "        )\n",
    "        self.conv_kwargs, self.fc_kwargs = self.kwarg_handler(conv_kwargs, fc_kwargs)\n",
    "        self.convnet = BasicConv1D(\n",
    "            input_len=input_len, \n",
    "            **self.conv_kwargs)\n",
    "        self.fcn = BasicFullyConnectedModule(\n",
    "            input_dim=self.convnet.flatten_dim, \n",
    "            output_dim=output_dim, \n",
    "            **self.fc_kwargs\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_rev_comp=None):\n",
    "        x = self.convnet(x)\n",
    "        x = x.view(x.size(0), self.convnet.flatten_dim)\n",
    "        x = self.fcn(x)\n",
    "        return x\n",
    "        \n",
    "    def kwarg_handler(self, conv_kwargs, fc_kwargs):\n",
    "        \"\"\"Sets default kwargs for conv and fc modules if not specified\"\"\"\n",
    "        conv_kwargs.setdefault(\"channels\", [4, 246, 60, 60, 120])\n",
    "        conv_kwargs.setdefault(\"conv_kernels\", [7, 3, 5, 3])\n",
    "        conv_kwargs.setdefault(\"conv_strides\", [1, 1, 1, 1])\n",
    "        conv_kwargs.setdefault(\"padding\", \"same\")\n",
    "        conv_kwargs.setdefault(\"pool_kernels\", [2, 2, 2, 2])\n",
    "        conv_kwargs.setdefault(\"omit_final_pool\", False)\n",
    "        conv_kwargs.setdefault(\"dropout_rates\", 0.0)\n",
    "        conv_kwargs.setdefault(\"batchnorm\", True)\n",
    "        fc_kwargs.setdefault(\"hidden_dims\", [256, 256])\n",
    "        fc_kwargs.setdefault(\"dropout_rate\", 0.4)\n",
    "        fc_kwargs.setdefault(\"batchnorm\", True)\n",
    "        return conv_kwargs, fc_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepSTARR(input_len=100, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b930191282746cfa85d20e85a6e363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | hp_metric | R2Score                   | 0     \n",
      "1 | convnet   | BasicConv1D               | 92.2 K\n",
      "2 | fcn       | BasicFullyConnectedModule | 251 K \n",
      "--------------------------------------------------------\n",
      "344 K     Trainable params\n",
      "0         Non-trainable params\n",
      "344 K     Total params\n",
      "1.377     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f965c75727248478640ad3f3c2a601e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744c34224274468b361fa274ab733bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f25f38696c14894907d3d64bffa1d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eu.train.fit(model, sdata, target_keys=[\"activity_0\", \"activity_1\"], epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
