{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    }
   ],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from os import PathLike\n",
    "from typing import Union, List\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_with_tune(\n",
    "    config: dict,\n",
    "    sdata = None,\n",
    "    target_keys: Union[str, List[str]] = None,\n",
    "    train_key: str = \"train_val\",\n",
    "    epochs: int = 10,\n",
    "    gpus: int = None,\n",
    "    num_workers: int = None,\n",
    "    log_dir: PathLike = None,\n",
    "    name: str = None,\n",
    "    version: str = None,\n",
    "    train_dataset: eu.dl.SeqDataset = None,\n",
    "    val_dataset: eu.dl.SeqDataset = None,\n",
    "    train_dataloader: DataLoader = None,\n",
    "    val_dataloader: DataLoader = None,\n",
    "    seq_transforms: List[str] = None,\n",
    "    transform_kwargs: dict = {},\n",
    "    seed: int = None,\n",
    "    verbosity = None,\n",
    "    **kwargs\n",
    "):\n",
    "    model = eu.models.get_model(config[\"arch\"], config)\n",
    "    gpus = gpus if gpus is not None else eu.settings.gpus\n",
    "    num_workers = num_workers if num_workers is not None else eu.settings.dl_num_workers\n",
    "    log_dir = log_dir if log_dir is not None else eu.settings.logging_dir\n",
    "    name = name if name is not None else config[\"arch\"]\n",
    "    seed_everything(seed, workers=True) if seed is not None else seed_everything(eu.settings.seed)\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    if train_dataloader is not None:\n",
    "        assert val_dataloader is not None\n",
    "    elif train_dataset is not None:\n",
    "        assert val_dataset is not None\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "    elif sdata is not None:\n",
    "        assert target_keys is not None\n",
    "        targs = sdata.seqs_annot[target_keys].values  \n",
    "        if len(targs.shape) == 1:\n",
    "            nan_mask = np.isnan(targs)\n",
    "        else:\n",
    "            nan_mask = np.any(np.isnan(targs), axis=1)\n",
    "        print(f\"Dropping {nan_mask.sum()} sequences with NaN targets.\")\n",
    "        sdata = sdata[~nan_mask]\n",
    "        train_idx = np.where(sdata.seqs_annot[train_key] == True)[0]\n",
    "        train_dataset = sdata[train_idx].to_dataset(\n",
    "            target_keys=target_keys,\n",
    "            seq_transforms=seq_transforms,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "        )\n",
    "        train_dataloader = train_dataset.to_dataloader(\n",
    "            batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    "        )\n",
    "        val_idx = np.where(sdata.seqs_annot[train_key] == False)[0]\n",
    "        val_dataset = sdata[val_idx].to_dataset(\n",
    "            target_keys=target_keys,\n",
    "            seq_transforms=seq_transforms,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "        )\n",
    "        val_dataloader = val_dataset.to_dataloader(\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No data provided to train on.\")\n",
    "    logger = TensorBoardLogger(log_dir, name=name, version=version)\n",
    "    callbacks = []\n",
    "    metrics = {\"loss\": \"val_loss\"}\n",
    "    callbacks.append(TuneReportCallback(metrics, on=\"validation_end\"))\n",
    "    print(callbacks)\n",
    "    trainer = Trainer(\n",
    "        max_epochs=epochs,\n",
    "        gpus=gpus,\n",
    "        logger=logger,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34406da6a5b0446d8db9847d44ca987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "\tohe_rev_seqs: None -> 1000 ohe_rev_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.reverse_complement_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": \"ss\",\n",
    "  \"aggr\": None,\n",
    "  \"lr\": 1e-3,\n",
    "  \"batch_size\": 64,\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, 16],\n",
    "    \"conv_kernels\": [3],\n",
    "    \"pool_kernels\": [2],\n",
    "    \"dropout_rates\": [0.1]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (hp_metric): R2Score()\n",
       "  (convnet): BasicConv1D(\n",
       "    (module): Sequential(\n",
       "      (0): Conv1d(4, 16, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fcn): BasicFullyConnectedModule(\n",
       "    (module): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = eu.models.get_model(config[\"arch\"], config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | hp_metric | R2Score                   | 0     \n",
      "1 | convnet   | BasicConv1D               | 208   \n",
      "2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "--------------------------------------------------------\n",
      "25.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.4 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db39ac295884fc4ab30b08726c7347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6998e9dcf774edd9482d9ee4ee27c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e173f9d1cce04d7cbe68e85f769ec5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902c16a330e4eafaa522a4f20c84309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31248cdf86df4f4ba14871769d63418c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2fb05421e44d89ad1576bbfcb0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d448ea3a99a47ce8d79d8ccb5727589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bf81645394473826ec9f52af0b798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622dcc399beb4d7fae9aafa75f67bb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ef77a592948b798eaa1f4d3dbf7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770edc647520430087e5f84a16a98f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1839eb1ca04a8692da9fb1483bb6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eu.train.fit(\n",
    "    model = model,\n",
    "    sdata = sdata,\n",
    "    target_keys = \"activity_0\", \n",
    "    train_key = \"train_val\",\n",
    "    epochs = 10,\n",
    "    gpus = 0,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tune.with_parameters(\n",
    "    hyperopt_with_tune,\n",
    "    sdata=sdata,\n",
    "    target_keys=\"activity_0\",\n",
    "    train_key=\"train_val\",\n",
    "    epochs=10,\n",
    "    gpus=0,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": tune.choice([\"ss\", \"ds\", \"ts\"]),\n",
    "  \"aggr\": tune.choice([\"max\", \"avg\"]),\n",
    "  \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "  \"batch_size\": tune.choice([32, 64, 128]),\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, 16],\n",
    "    \"conv_kernels\": [tune.choice([3, 5])],\n",
    "    \"pool_kernels\": [tune.choice([2, 4])],\n",
    "    \"dropout_rates\": [tune.choice([0.1, 0.2])]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 22:58:25,118\tWARNING services.py:1893 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.88gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-10-19 22:58:26,560\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "/home/vscode/.local/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py:644: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/ray-air/key-concepts.html#session\n",
      "\n",
      "  DeprecationWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-19 22:59:16 (running for 00:00:48.32)<br>Memory usage on this node: 4.0/7.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.41 GiB heap, 0.0/1.71 GiB objects<br>Current best trial: 8b3ae_00004 with loss=0.08021415770053864 and parameters={'arch': 'CNN', 'input_len': 100, 'output_dim': 1, 'strand': 'ts', 'aggr': 'max', 'lr': 0.09544208855570373, 'batch_size': 32, 'conv_kwargs': {'channels': [4, 16], 'conv_kernels': [3], 'pool_kernels': [2], 'dropout_rates': [0.1]}, 'fc_kwargs': {'hidden_dims': [32]}}<br>Result logdir: /workspaces/EUGENe/tests/notebooks/implement/eugene_logs/test<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th>aggr  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_kwargs/conv_...</th><th style=\"text-align: right;\">  conv_kwargs/dropo...</th><th style=\"text-align: right;\">  conv_kwargs/pool_...</th><th style=\"text-align: right;\">         lr</th><th>strand  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00000</td><td>TERMINATED</td><td>172.16.5.4:7710</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.000515969</td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        31.0604 </td><td style=\"text-align: right;\">0.0812072</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00001</td><td>TERMINATED</td><td>172.16.5.4:7808</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0022927  </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        23.2306 </td><td style=\"text-align: right;\">0.0842177</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00002</td><td>TERMINATED</td><td>172.16.5.4:7919</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0146573  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        13.4055 </td><td style=\"text-align: right;\">0.0802305</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00003</td><td>TERMINATED</td><td>172.16.5.4:8021</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0372549  </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.82449</td><td style=\"text-align: right;\">0.0891374</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00004</td><td>TERMINATED</td><td>172.16.5.4:8021</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0954421  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.16415</td><td style=\"text-align: right;\">0.0802142</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00005</td><td>TERMINATED</td><td>172.16.5.4:7710</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0656628  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         4.15178</td><td style=\"text-align: right;\">0.0802464</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00006</td><td>TERMINATED</td><td>172.16.5.4:7919</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00775331 </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         3.22029</td><td style=\"text-align: right;\">0.085105 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00007</td><td>TERMINATED</td><td>172.16.5.4:7808</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000389489</td><td>ds      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.68956</td><td style=\"text-align: right;\">0.082369 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00008</td><td>TERMINATED</td><td>172.16.5.4:7710</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0735737  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.39946</td><td style=\"text-align: right;\">0.759937 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8b3ae_00009</td><td>TERMINATED</td><td>172.16.5.4:7808</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000654435</td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.29624</td><td style=\"text-align: right;\">0.0821289</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7710)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fa851044790>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\u001b[2m\u001b[36m(pid=7808)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0.101     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f9f28e60f90>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7919)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 50.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 50.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 0.200     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fe8dc1b96d0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8021)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00000:\n",
      "  date: 2022-10-19_22-58-39\n",
      "  done: false\n",
      "  experiment_id: bee7ef4669814bf7a354bbd36ad9bd4f\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.1006212830543518\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7710\n",
      "  time_since_restore: 0.865459680557251\n",
      "  time_this_iter_s: 0.865459680557251\n",
      "  time_total_s: 0.865459680557251\n",
      "  timestamp: 1666220319\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00000\n",
      "  warmup_time: 0.0035009384155273438\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00001:\n",
      "  date: 2022-10-19_22-58-48\n",
      "  done: false\n",
      "  experiment_id: 34146fb72cc94908923422e52e6f5993\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08753872662782669\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7808\n",
      "  time_since_restore: 0.27142834663391113\n",
      "  time_this_iter_s: 0.27142834663391113\n",
      "  time_total_s: 0.27142834663391113\n",
      "  timestamp: 1666220328\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00001\n",
      "  warmup_time: 0.003590106964111328\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00002:\n",
      "  date: 2022-10-19_22-58-57\n",
      "  done: false\n",
      "  experiment_id: b17153f5c6e14c5c976cbcadd31cc891\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.3482355773448944\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7919\n",
      "  time_since_restore: 0.40052008628845215\n",
      "  time_this_iter_s: 0.40052008628845215\n",
      "  time_total_s: 0.40052008628845215\n",
      "  timestamp: 1666220337\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00002\n",
      "  warmup_time: 0.017160892486572266\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fe6df16ed50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00003:\n",
      "  date: 2022-10-19_22-59-07\n",
      "  done: false\n",
      "  experiment_id: 18a4210a9ca8418f990c3772efc7e4f9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.49401620030403137\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 8021\n",
      "  time_since_restore: 0.24023842811584473\n",
      "  time_this_iter_s: 0.24023842811584473\n",
      "  time_total_s: 0.24023842811584473\n",
      "  timestamp: 1666220347\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00003\n",
      "  warmup_time: 0.003967761993408203\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00003:\n",
      "  date: 2022-10-19_22-59-08\n",
      "  done: true\n",
      "  experiment_id: 18a4210a9ca8418f990c3772efc7e4f9\n",
      "  experiment_tag: 3_aggr=avg,batch_size=128,0=5,0=0.2000,0=4,lr=0.0373,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08913742005825043\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 8021\n",
      "  time_since_restore: 1.824488878250122\n",
      "  time_this_iter_s: 0.1521308422088623\n",
      "  time_total_s: 1.824488878250122\n",
      "  timestamp: 1666220348\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00003\n",
      "  warmup_time: 0.003967761993408203\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fe6ddb81050>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=8021)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00000:\n",
      "  date: 2022-10-19_22-59-09\n",
      "  done: true\n",
      "  experiment_id: bee7ef4669814bf7a354bbd36ad9bd4f\n",
      "  experiment_tag: 0_aggr=max,batch_size=128,0=3,0=0.2000,0=2,lr=0.0005,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08120723813772202\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7710\n",
      "  time_since_restore: 31.060357809066772\n",
      "  time_this_iter_s: 0.2776455879211426\n",
      "  time_total_s: 31.060357809066772\n",
      "  timestamp: 1666220349\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00000\n",
      "  warmup_time: 0.0035009384155273438\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fa8502c6bd0>]\n",
      "Result for hyperopt_with_tune_8b3ae_00004:\n",
      "  date: 2022-10-19_22-59-09\n",
      "  done: false\n",
      "  experiment_id: 18a4210a9ca8418f990c3772efc7e4f9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.10064372420310974\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 8021\n",
      "  time_since_restore: 0.7394359111785889\n",
      "  time_this_iter_s: 0.7394359111785889\n",
      "  time_total_s: 0.7394359111785889\n",
      "  timestamp: 1666220349\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00004\n",
      "  warmup_time: 0.003967761993408203\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 25.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 25.1 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00005:\n",
      "  date: 2022-10-19_22-59-10\n",
      "  done: false\n",
      "  experiment_id: bee7ef4669814bf7a354bbd36ad9bd4f\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.49822548031806946\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7710\n",
      "  time_since_restore: 0.7911510467529297\n",
      "  time_this_iter_s: 0.7911510467529297\n",
      "  time_total_s: 0.7911510467529297\n",
      "  timestamp: 1666220350\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00005\n",
      "  warmup_time: 0.0035009384155273438\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00002:\n",
      "  date: 2022-10-19_22-59-10\n",
      "  done: true\n",
      "  experiment_id: b17153f5c6e14c5c976cbcadd31cc891\n",
      "  experiment_tag: 2_aggr=avg,batch_size=64,0=5,0=0.1000,0=2,lr=0.0147,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08023054152727127\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7919\n",
      "  time_since_restore: 13.405469179153442\n",
      "  time_this_iter_s: 0.4621865749359131\n",
      "  time_total_s: 13.405469179153442\n",
      "  timestamp: 1666220350\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00002\n",
      "  warmup_time: 0.017160892486572266\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fe7b019c810>]\n",
      "Result for hyperopt_with_tune_8b3ae_00001:\n",
      "  date: 2022-10-19_22-59-11\n",
      "  done: true\n",
      "  experiment_id: 34146fb72cc94908923422e52e6f5993\n",
      "  experiment_tag: 1_aggr=max,batch_size=32,0=3,0=0.1000,0=2,lr=0.0023,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08421774953603745\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7808\n",
      "  time_since_restore: 23.230573415756226\n",
      "  time_this_iter_s: 0.5647876262664795\n",
      "  time_total_s: 23.230573415756226\n",
      "  timestamp: 1666220351\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00001\n",
      "  warmup_time: 0.003590106964111328\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7919)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0.051     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f9f28f0cad0>]\n",
      "Result for hyperopt_with_tune_8b3ae_00006:\n",
      "  date: 2022-10-19_22-59-11\n",
      "  done: false\n",
      "  experiment_id: b17153f5c6e14c5c976cbcadd31cc891\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08371169865131378\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7919\n",
      "  time_since_restore: 0.3613247871398926\n",
      "  time_this_iter_s: 0.3613247871398926\n",
      "  time_total_s: 0.3613247871398926\n",
      "  timestamp: 1666220351\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00006\n",
      "  warmup_time: 0.017160892486572266\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00007:\n",
      "  date: 2022-10-19_22-59-11\n",
      "  done: false\n",
      "  experiment_id: 34146fb72cc94908923422e52e6f5993\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.12046148627996445\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7808\n",
      "  time_since_restore: 0.3523218631744385\n",
      "  time_this_iter_s: 0.3523218631744385\n",
      "  time_total_s: 0.3523218631744385\n",
      "  timestamp: 1666220351\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00007\n",
      "  warmup_time: 0.003590106964111328\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00005:\n",
      "  date: 2022-10-19_22-59-13\n",
      "  done: true\n",
      "  experiment_id: bee7ef4669814bf7a354bbd36ad9bd4f\n",
      "  experiment_tag: 5_aggr=max,batch_size=64,0=3,0=0.1000,0=4,lr=0.0657,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.0802464485168457\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7710\n",
      "  time_since_restore: 4.151783466339111\n",
      "  time_this_iter_s: 0.3170301914215088\n",
      "  time_total_s: 4.151783466339111\n",
      "  timestamp: 1666220353\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00005\n",
      "  warmup_time: 0.0035009384155273438\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fa8500c1bd0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7710)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00007:\n",
      "  date: 2022-10-19_22-59-13\n",
      "  done: true\n",
      "  experiment_id: 34146fb72cc94908923422e52e6f5993\n",
      "  experiment_tag: 7_aggr=max,batch_size=128,0=5,0=0.2000,0=4,lr=0.0004,strand=ds\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08236898481845856\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7808\n",
      "  time_since_restore: 2.689558744430542\n",
      "  time_this_iter_s: 0.2525146007537842\n",
      "  time_total_s: 2.689558744430542\n",
      "  timestamp: 1666220353\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00007\n",
      "  warmup_time: 0.003590106964111328\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f9f28f24b50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m 0.102     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=7808)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8b3ae_00008:\n",
      "  date: 2022-10-19_22-59-14\n",
      "  done: false\n",
      "  experiment_id: bee7ef4669814bf7a354bbd36ad9bd4f\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.6516900658607483\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7710\n",
      "  time_since_restore: 0.4845616817474365\n",
      "  time_this_iter_s: 0.4845616817474365\n",
      "  time_total_s: 0.4845616817474365\n",
      "  timestamp: 1666220354\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00008\n",
      "  warmup_time: 0.0035009384155273438\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00006:\n",
      "  date: 2022-10-19_22-59-14\n",
      "  done: true\n",
      "  experiment_id: b17153f5c6e14c5c976cbcadd31cc891\n",
      "  experiment_tag: 6_aggr=avg,batch_size=64,0=5,0=0.2000,0=4,lr=0.0078,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08510497957468033\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7919\n",
      "  time_since_restore: 3.2202892303466797\n",
      "  time_this_iter_s: 0.44707512855529785\n",
      "  time_total_s: 3.2202892303466797\n",
      "  timestamp: 1666220354\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00006\n",
      "  warmup_time: 0.017160892486572266\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00009:\n",
      "  date: 2022-10-19_22-59-14\n",
      "  done: false\n",
      "  experiment_id: 34146fb72cc94908923422e52e6f5993\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.09722557663917542\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7808\n",
      "  time_since_restore: 0.5815680027008057\n",
      "  time_this_iter_s: 0.5815680027008057\n",
      "  time_total_s: 0.5815680027008057\n",
      "  timestamp: 1666220354\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8b3ae_00009\n",
      "  warmup_time: 0.003590106964111328\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00004:\n",
      "  date: 2022-10-19_22-59-14\n",
      "  done: false\n",
      "  experiment_id: 18a4210a9ca8418f990c3772efc7e4f9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 9\n",
      "  loss: 0.08018717169761658\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 8021\n",
      "  time_since_restore: 5.771925449371338\n",
      "  time_this_iter_s: 0.3890557289123535\n",
      "  time_total_s: 5.771925449371338\n",
      "  timestamp: 1666220354\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: 8b3ae_00004\n",
      "  warmup_time: 0.003967761993408203\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00004:\n",
      "  date: 2022-10-19_22-59-15\n",
      "  done: true\n",
      "  experiment_id: 18a4210a9ca8418f990c3772efc7e4f9\n",
      "  experiment_tag: 4_aggr=max,batch_size=32,0=3,0=0.1000,0=2,lr=0.0954,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08021415770053864\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 8021\n",
      "  time_since_restore: 6.164149284362793\n",
      "  time_this_iter_s: 0.3922238349914551\n",
      "  time_total_s: 6.164149284362793\n",
      "  timestamp: 1666220355\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00004\n",
      "  warmup_time: 0.003967761993408203\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00008:\n",
      "  date: 2022-10-19_22-59-16\n",
      "  done: true\n",
      "  experiment_id: bee7ef4669814bf7a354bbd36ad9bd4f\n",
      "  experiment_tag: 8_aggr=max,batch_size=128,0=3,0=0.2000,0=2,lr=0.0736,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.7599369883537292\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7710\n",
      "  time_since_restore: 2.3994555473327637\n",
      "  time_this_iter_s: 0.1865558624267578\n",
      "  time_total_s: 2.3994555473327637\n",
      "  timestamp: 1666220356\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00008\n",
      "  warmup_time: 0.0035009384155273438\n",
      "  \n",
      "Result for hyperopt_with_tune_8b3ae_00009:\n",
      "  date: 2022-10-19_22-59-16\n",
      "  done: true\n",
      "  experiment_id: 34146fb72cc94908923422e52e6f5993\n",
      "  experiment_tag: 9_aggr=avg,batch_size=128,0=5,0=0.1000,0=4,lr=0.0007,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08212889730930328\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 7808\n",
      "  time_since_restore: 2.296236991882324\n",
      "  time_this_iter_s: 0.15902924537658691\n",
      "  time_total_s: 2.296236991882324\n",
      "  timestamp: 1666220356\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8b3ae_00009\n",
      "  warmup_time: 0.003590106964111328\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 22:59:16,563\tINFO tune.py:759 -- Total run time: 48.71 seconds (48.29 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    config=tune_config,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=10,\n",
    "    local_dir=eu.settings.logging_dir,\n",
    "    keep_checkpoints_num=1,\n",
    "    checkpoint_score_attr=\"min-val_loss\",\n",
    "    name=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>config/output_dim</th>\n",
       "      <th>config/strand</th>\n",
       "      <th>config/aggr</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/batch_size</th>\n",
       "      <th>config/conv_kwargs/channels</th>\n",
       "      <th>config/conv_kwargs/conv_kernels</th>\n",
       "      <th>config/conv_kwargs/pool_kernels</th>\n",
       "      <th>config/conv_kwargs/dropout_rates</th>\n",
       "      <th>config/fc_kwargs/hidden_dims</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8b3ae_00000</th>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.277646</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>bee7ef4669814bf7a354bbd36ad9bd4f</td>\n",
       "      <td>2022-10-19_22-59-09</td>\n",
       "      <td>1666220349</td>\n",
       "      <td>31.060358</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00001</th>\n",
       "      <td>0.084218</td>\n",
       "      <td>0.564788</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>34146fb72cc94908923422e52e6f5993</td>\n",
       "      <td>2022-10-19_22-59-11</td>\n",
       "      <td>1666220351</td>\n",
       "      <td>23.230573</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>max</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>32</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00002</th>\n",
       "      <td>0.080231</td>\n",
       "      <td>0.462187</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>b17153f5c6e14c5c976cbcadd31cc891</td>\n",
       "      <td>2022-10-19_22-59-10</td>\n",
       "      <td>1666220350</td>\n",
       "      <td>13.405469</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00003</th>\n",
       "      <td>0.089137</td>\n",
       "      <td>0.152131</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>18a4210a9ca8418f990c3772efc7e4f9</td>\n",
       "      <td>2022-10-19_22-59-08</td>\n",
       "      <td>1666220348</td>\n",
       "      <td>1.824489</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00004</th>\n",
       "      <td>0.080214</td>\n",
       "      <td>0.392224</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>18a4210a9ca8418f990c3772efc7e4f9</td>\n",
       "      <td>2022-10-19_22-59-15</td>\n",
       "      <td>1666220355</td>\n",
       "      <td>6.164149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.095442</td>\n",
       "      <td>32</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00005</th>\n",
       "      <td>0.080246</td>\n",
       "      <td>0.317030</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>bee7ef4669814bf7a354bbd36ad9bd4f</td>\n",
       "      <td>2022-10-19_22-59-13</td>\n",
       "      <td>1666220353</td>\n",
       "      <td>4.151783</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00006</th>\n",
       "      <td>0.085105</td>\n",
       "      <td>0.447075</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>b17153f5c6e14c5c976cbcadd31cc891</td>\n",
       "      <td>2022-10-19_22-59-14</td>\n",
       "      <td>1666220354</td>\n",
       "      <td>3.220289</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00007</th>\n",
       "      <td>0.082369</td>\n",
       "      <td>0.252515</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>34146fb72cc94908923422e52e6f5993</td>\n",
       "      <td>2022-10-19_22-59-13</td>\n",
       "      <td>1666220353</td>\n",
       "      <td>2.689559</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ds</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00008</th>\n",
       "      <td>0.759937</td>\n",
       "      <td>0.186556</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>bee7ef4669814bf7a354bbd36ad9bd4f</td>\n",
       "      <td>2022-10-19_22-59-16</td>\n",
       "      <td>1666220356</td>\n",
       "      <td>2.399456</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.073574</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b3ae_00009</th>\n",
       "      <td>0.082129</td>\n",
       "      <td>0.159029</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>34146fb72cc94908923422e52e6f5993</td>\n",
       "      <td>2022-10-19_22-59-16</td>\n",
       "      <td>1666220356</td>\n",
       "      <td>2.296237</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s  done timesteps_total episodes_total  \\\n",
       "trial_id                                                                       \n",
       "8b3ae_00000  0.081207          0.277646  True            None           None   \n",
       "8b3ae_00001  0.084218          0.564788  True            None           None   \n",
       "8b3ae_00002  0.080231          0.462187  True            None           None   \n",
       "8b3ae_00003  0.089137          0.152131  True            None           None   \n",
       "8b3ae_00004  0.080214          0.392224  True            None           None   \n",
       "8b3ae_00005  0.080246          0.317030  True            None           None   \n",
       "8b3ae_00006  0.085105          0.447075  True            None           None   \n",
       "8b3ae_00007  0.082369          0.252515  True            None           None   \n",
       "8b3ae_00008  0.759937          0.186556  True            None           None   \n",
       "8b3ae_00009  0.082129          0.159029  True            None           None   \n",
       "\n",
       "             training_iteration                     experiment_id  \\\n",
       "trial_id                                                            \n",
       "8b3ae_00000                  10  bee7ef4669814bf7a354bbd36ad9bd4f   \n",
       "8b3ae_00001                  10  34146fb72cc94908923422e52e6f5993   \n",
       "8b3ae_00002                  10  b17153f5c6e14c5c976cbcadd31cc891   \n",
       "8b3ae_00003                  10  18a4210a9ca8418f990c3772efc7e4f9   \n",
       "8b3ae_00004                  10  18a4210a9ca8418f990c3772efc7e4f9   \n",
       "8b3ae_00005                  10  bee7ef4669814bf7a354bbd36ad9bd4f   \n",
       "8b3ae_00006                  10  b17153f5c6e14c5c976cbcadd31cc891   \n",
       "8b3ae_00007                  10  34146fb72cc94908923422e52e6f5993   \n",
       "8b3ae_00008                  10  bee7ef4669814bf7a354bbd36ad9bd4f   \n",
       "8b3ae_00009                  10  34146fb72cc94908923422e52e6f5993   \n",
       "\n",
       "                            date   timestamp  time_total_s  ...  \\\n",
       "trial_id                                                    ...   \n",
       "8b3ae_00000  2022-10-19_22-59-09  1666220349     31.060358  ...   \n",
       "8b3ae_00001  2022-10-19_22-59-11  1666220351     23.230573  ...   \n",
       "8b3ae_00002  2022-10-19_22-59-10  1666220350     13.405469  ...   \n",
       "8b3ae_00003  2022-10-19_22-59-08  1666220348      1.824489  ...   \n",
       "8b3ae_00004  2022-10-19_22-59-15  1666220355      6.164149  ...   \n",
       "8b3ae_00005  2022-10-19_22-59-13  1666220353      4.151783  ...   \n",
       "8b3ae_00006  2022-10-19_22-59-14  1666220354      3.220289  ...   \n",
       "8b3ae_00007  2022-10-19_22-59-13  1666220353      2.689559  ...   \n",
       "8b3ae_00008  2022-10-19_22-59-16  1666220356      2.399456  ...   \n",
       "8b3ae_00009  2022-10-19_22-59-16  1666220356      2.296237  ...   \n",
       "\n",
       "             config/output_dim config/strand config/aggr  config/lr  \\\n",
       "trial_id                                                              \n",
       "8b3ae_00000                  1            ts         max   0.000516   \n",
       "8b3ae_00001                  1            ss         max   0.002293   \n",
       "8b3ae_00002                  1            ts         avg   0.014657   \n",
       "8b3ae_00003                  1            ss         avg   0.037255   \n",
       "8b3ae_00004                  1            ts         max   0.095442   \n",
       "8b3ae_00005                  1            ts         max   0.065663   \n",
       "8b3ae_00006                  1            ss         avg   0.007753   \n",
       "8b3ae_00007                  1            ds         max   0.000389   \n",
       "8b3ae_00008                  1            ts         max   0.073574   \n",
       "8b3ae_00009                  1            ts         avg   0.000654   \n",
       "\n",
       "             config/batch_size  config/conv_kwargs/channels  \\\n",
       "trial_id                                                      \n",
       "8b3ae_00000                128                      [4, 16]   \n",
       "8b3ae_00001                 32                      [4, 16]   \n",
       "8b3ae_00002                 64                      [4, 16]   \n",
       "8b3ae_00003                128                      [4, 16]   \n",
       "8b3ae_00004                 32                      [4, 16]   \n",
       "8b3ae_00005                 64                      [4, 16]   \n",
       "8b3ae_00006                 64                      [4, 16]   \n",
       "8b3ae_00007                128                      [4, 16]   \n",
       "8b3ae_00008                128                      [4, 16]   \n",
       "8b3ae_00009                128                      [4, 16]   \n",
       "\n",
       "             config/conv_kwargs/conv_kernels config/conv_kwargs/pool_kernels  \\\n",
       "trial_id                                                                       \n",
       "8b3ae_00000                              [3]                             [2]   \n",
       "8b3ae_00001                              [3]                             [2]   \n",
       "8b3ae_00002                              [5]                             [2]   \n",
       "8b3ae_00003                              [5]                             [4]   \n",
       "8b3ae_00004                              [3]                             [2]   \n",
       "8b3ae_00005                              [3]                             [4]   \n",
       "8b3ae_00006                              [5]                             [4]   \n",
       "8b3ae_00007                              [5]                             [4]   \n",
       "8b3ae_00008                              [3]                             [2]   \n",
       "8b3ae_00009                              [5]                             [4]   \n",
       "\n",
       "            config/conv_kwargs/dropout_rates  config/fc_kwargs/hidden_dims  \n",
       "trial_id                                                                    \n",
       "8b3ae_00000                            [0.2]                          [32]  \n",
       "8b3ae_00001                            [0.1]                          [32]  \n",
       "8b3ae_00002                            [0.1]                          [32]  \n",
       "8b3ae_00003                            [0.2]                          [32]  \n",
       "8b3ae_00004                            [0.1]                          [32]  \n",
       "8b3ae_00005                            [0.1]                          [32]  \n",
       "8b3ae_00006                            [0.2]                          [32]  \n",
       "8b3ae_00007                            [0.2]                          [32]  \n",
       "8b3ae_00008                            [0.2]                          [32]  \n",
       "8b3ae_00009                            [0.1]                          [32]  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loss', 'time_this_iter_s', 'done', 'timesteps_total', 'episodes_total',\n",
       "       'training_iteration', 'experiment_id', 'date', 'timestamp',\n",
       "       'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore',\n",
       "       'timesteps_since_restore', 'iterations_since_restore', 'warmup_time',\n",
       "       'experiment_tag', 'config/arch', 'config/input_len',\n",
       "       'config/output_dim', 'config/strand', 'config/aggr', 'config/lr',\n",
       "       'config/batch_size', 'config/conv_kwargs/channels',\n",
       "       'config/conv_kwargs/conv_kernels', 'config/conv_kwargs/pool_kernels',\n",
       "       'config/conv_kwargs/dropout_rates', 'config/fc_kwargs/hidden_dims'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.results_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(config, train_dataset, val_dataset, epochs=10, gpus=0):\n",
    "  model =  eu.models.FCN(\n",
    "    input_len=100, \n",
    "    output_dim=1, \n",
    "    lr=config[\"lr\"]\n",
    "    )\n",
    "  train_dl = train_dataset.to_dataloader(batch_size=config[\"batch_size\"])\n",
    "  val_dl = val_dataset.to_dataloader(batch_size=config[\"batch_size\"])\n",
    "  trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    gpus=gpus,\n",
    "    progress_bar_refresh_rate=0,\n",
    "    callbacks=[callback])\n",
    "  trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tune.with_parameters(\n",
    "    train_tune,\n",
    "    train_dataset=sdataset_train,\n",
    "    val_dataset=sdataset_val,\n",
    "    epochs=10,\n",
    "    gpus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 03:34:19,999\tERROR experiment_analysis.py:486 -- No checkpoints have been found for trial train_tune_86a07_00000.\n"
     ]
    }
   ],
   "source": [
    "best_trial = analysis.best_trial  # Get best trial\n",
    "best_config = analysis.best_config  # Get best trial's hyperparameters\n",
    "best_logdir = analysis.best_logdir  # Get best trial's logdir\n",
    "best_checkpoint = analysis.best_checkpoint  # Get best trial's best checkpoint\n",
    "best_result = analysis.best_result  # Get best trial's last results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_tune_86a07_00000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.00010000831284081109, 'batch_size': 32}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vscode/ray_results/test/train_tune_86a07_00000_0_batch_size=32,lr=0.0001_2022-10-19_03-31-32'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.11025925725698471,\n",
       " 'time_this_iter_s': 0.11937999725341797,\n",
       " 'done': True,\n",
       " 'timesteps_total': None,\n",
       " 'episodes_total': None,\n",
       " 'training_iteration': 10,\n",
       " 'trial_id': '86a07_00000',\n",
       " 'experiment_id': '299e91b7003d4457b77be9e3b61a2294',\n",
       " 'date': '2022-10-19_03-31-51',\n",
       " 'timestamp': 1666150311,\n",
       " 'time_total_s': 10.0547194480896,\n",
       " 'pid': 23527,\n",
       " 'hostname': 'codespaces-97ce9f',\n",
       " 'node_ip': '172.16.5.4',\n",
       " 'config': {'lr': 0.00010000831284081109, 'batch_size': 32},\n",
       " 'time_since_restore': 10.0547194480896,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 10,\n",
       " 'warmup_time': 0.0036079883575439453,\n",
       " 'experiment_tag': '0_batch_size=32,lr=0.0001'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'Z\\xe4~\\xb8\\x0c\\\\\\x08\\x1b`', b'\\x93\\xa5x\\xa8\\xf9\\x18\\xaf \\xc7\\x85\\x1f\\xd3\\x93x\\x86k9\\xcd\\xfe C\\xf5\\x80\\xd7\\xd2\\x85\\xaeE\\x0e_\\xbeHF\\xe6\\xc2p\\xe0`W\\xab\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00', b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 /\\x934\\x9f\\xd5\\x8eo7\\x8d\\xe5rv\\xbc1z^B\\x17%\\x86\\x00\\x9cn\\x82\\xebF\\xf6*Qy\\t\\x1e']\n",
      "Bad pipe message: %s [b\"q8\\xf8\\xce\\xd9O\\xe02\\xaeh\\x96\\xe3\\ru'\\xa7\\xff\\xba O\\xe0\\xf9\\x88\\xe3\\x04{\\x1em\\xf6\\xe3Zt\\xb5\\x8e\\xa3-\\xd1\\xba=\\xd1`-\\xfdC\\xca\\xde$\\x1a\\x85V\\xc6\\x00\\x08\\x13\\x02\\x13\"]\n",
      "Bad pipe message: %s [b'\\x12\\xae\\xc8\\xf1gV\\r\\x002\\xe63\\x8d\\x9e\\x1b\\x14>O\\xcc\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0']\n",
      "Bad pipe message: %s [b\"$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\"]\n",
      "Bad pipe message: %s [b\"\\xce\\x0e\\xd6\\xb2]1\\xfd\\x82\\xb3\\x8b\\xbc\\x17Mr\\xfc\\x8f\\x80H\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\", b'\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'A=\\xdf\\xba{.\\xce\\xaa\\x8f\\xde\\x19\\x18T\\x19\\xa6\\x9c;\\xae\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t12']\n",
      "Bad pipe message: %s [b'0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b'\\xa4\\t\\xd2\\xfc\\xa5=\\x83\\x0c\\x1dE\\t\\xf5\\xd8\\xf1\\xca\\xadmL\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0']\n",
      "Bad pipe message: %s [b'\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c']\n",
      "Bad pipe message: %s [b'\\xdd\\xd0R\\xa0\\x8b\\x89&y\\x8b\\x1a\\xde\\xf03\\x12\\xa3\\xfa\\x07\\xfb\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007']\n",
      "Bad pipe message: %s [b'g\\x15<\\x7f\\xa9\\xecn\\xff\\x06\\x1d\\x14)\\x95sq\\xc8^\\xc2\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x00']\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02']\n",
      "Bad pipe message: %s [b\"\\x0c\\x93\\x9bEn\\xe04&\\x12H\\x08A\\x8fH;\\xfd\\xc7!\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\", b'\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 ']\n"
     ]
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
