{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from os import PathLike\n",
    "from typing import Union, List\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler, MedianStoppingRule, PopulationBasedTraining\n",
    "from ray.tune.search import BasicVariantGenerator\n",
    "from ray.tune.search.bayesopt import BayesOptSearch \n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_with_tune(\n",
    "    config: dict,\n",
    "    sdata = None,\n",
    "    target_keys: Union[str, List[str]] = None,\n",
    "    train_key: str = \"train_val\",\n",
    "    epochs: int = 10,\n",
    "    gpus: int = None,\n",
    "    num_workers: int = None,\n",
    "    log_dir: PathLike = None,\n",
    "    name: str = None,\n",
    "    version: str = None,\n",
    "    train_dataset: eu.dl.SeqDataset = None,\n",
    "    val_dataset: eu.dl.SeqDataset = None,\n",
    "    train_dataloader: DataLoader = None,\n",
    "    val_dataloader: DataLoader = None,\n",
    "    seq_transforms: List[str] = None,\n",
    "    transform_kwargs: dict = {},\n",
    "    seed: int = None,\n",
    "    verbosity = None,\n",
    "):\n",
    "    model = eu.models.get_model(config[\"arch\"], config)\n",
    "    gpus = gpus if gpus is not None else eu.settings.gpus\n",
    "    num_workers = num_workers if num_workers is not None else eu.settings.dl_num_workers\n",
    "    log_dir = log_dir if log_dir is not None else eu.settings.logging_dir\n",
    "    name = name if name is not None else config[\"arch\"]\n",
    "    seed_everything(seed, workers=True) if seed is not None else seed_everything(eu.settings.seed)\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    if train_dataloader is not None:\n",
    "        assert val_dataloader is not None\n",
    "    elif train_dataset is not None:\n",
    "        assert val_dataset is not None\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "    elif sdata is not None:\n",
    "        assert target_keys is not None\n",
    "        targs = sdata.seqs_annot[target_keys].values  \n",
    "        if len(targs.shape) == 1:\n",
    "            nan_mask = np.isnan(targs)\n",
    "        else:\n",
    "            nan_mask = np.any(np.isnan(targs), axis=1)\n",
    "        print(f\"Dropping {nan_mask.sum()} sequences with NaN targets.\")\n",
    "        sdata = sdata[~nan_mask]\n",
    "        train_idx = np.where(sdata.seqs_annot[train_key] == True)[0]\n",
    "        train_dataset = sdata[train_idx].to_dataset(\n",
    "            target_keys=target_keys,\n",
    "            seq_transforms=seq_transforms,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "        )\n",
    "        train_dataloader = train_dataset.to_dataloader(\n",
    "            batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    "        )\n",
    "        val_idx = np.where(sdata.seqs_annot[train_key] == False)[0]\n",
    "        val_dataset = sdata[val_idx].to_dataset(\n",
    "            target_keys=target_keys,\n",
    "            seq_transforms=seq_transforms,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "        )\n",
    "        val_dataloader = val_dataset.to_dataloader(\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No data provided to train on.\")\n",
    "    logger = TensorBoardLogger(log_dir, name=name, version=version)\n",
    "    callbacks = []\n",
    "    metrics = {\"loss\": \"val_loss\"}\n",
    "    callbacks.append(TuneReportCallback(metrics, on=\"validation_end\"))\n",
    "    trainer = Trainer(\n",
    "        max_epochs=epochs,\n",
    "        gpus=gpus,\n",
    "        logger=logger,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_dict = {\n",
    "    \"ASHAScheduler\": ASHAScheduler,\n",
    "    \"MedianStoppingRule\": MedianStoppingRule,\n",
    "    \"PopulationBasedTraining\": PopulationBasedTraining,\n",
    "    \n",
    "}\n",
    "default_scheduler_args = {\n",
    "    \"ASHAScheduler\": {\"metric\":\"loss\", \"mode\":\"min\", \"max_t\":10, \"grace_period\":1, \"reduction_factor\":2},\n",
    "    \"MedianStoppingRule\": {\"metric\":\"loss\", \"mode\":\"min\", \"grace_period\":1},\n",
    "    \"PopulationBasedTraining\": {\"metric\":\"loss\", \"mode\":\"min\", \"grace_period\":1},\n",
    "}\n",
    "algo_dict = {\n",
    "    \"BayesOptSearch\": BayesOptSearch,\n",
    "    \"HyperOptSearch\": HyperOptSearch,\n",
    "    \"BasicVariantGenerator\": BasicVariantGenerator,\n",
    "}\n",
    "default_algo_args = {\n",
    "    \"BayesOptSearch\": {\"metric\":\"loss\", \"mode\":\"min\"},\n",
    "    \"HyperOptSearch\": {\"metric\":\"loss\", \"mode\":\"min\"},\n",
    "    \"BasicVariantGenerator\": {},\n",
    "}\n",
    "\n",
    "def hyperopt(\n",
    "    config,\n",
    "    scheduler=\"ASHAScheduler\",\n",
    "    algorithm=\"BasicVariantGenerator\",\n",
    "    sdata = None,\n",
    "    target_keys: Union[str, List[str]] = None,\n",
    "    train_key: str = \"train_val\",\n",
    "    epochs: int = 10,\n",
    "    gpus: int = None,\n",
    "    num_workers: int = None,\n",
    "    log_dir: PathLike = None,\n",
    "    name: str = None,\n",
    "    version: str = None,\n",
    "    train_dataset: eu.dl.SeqDataset = None,\n",
    "    val_dataset: eu.dl.SeqDataset = None,\n",
    "    train_dataloader: DataLoader = None,\n",
    "    val_dataloader: DataLoader = None,\n",
    "    seq_transforms: List[str] = None,\n",
    "    transform_kwargs: dict = {},\n",
    "    seed: int = None,\n",
    "    verbosity = None,\n",
    "    scheduler_kwargs: dict = None,\n",
    "    algorithm_kwargs: dict = None,\n",
    "    **kwargs\n",
    "):\n",
    "    trainable = tune.with_parameters(\n",
    "        hyperopt_with_tune,\n",
    "        sdata=sdata,\n",
    "        target_keys=target_keys,\n",
    "        train_key=train_key,\n",
    "        epochs=epochs,\n",
    "        gpus=gpus,\n",
    "        num_workers=num_workers,\n",
    "        log_dir=log_dir,\n",
    "        name=name,\n",
    "        version=version,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        seq_transforms=seq_transforms,\n",
    "        transform_kwargs=transform_kwargs,\n",
    "        seed=seed,\n",
    "        verbosity=verbosity,\n",
    "        **kwargs\n",
    "    )\n",
    "    if scheduler_kwargs is None or len(scheduler_kwargs) == 0:\n",
    "        scheduler_kwargs = default_scheduler_args[scheduler]\n",
    "    scheduler = scheduler_dict[scheduler](**scheduler_kwargs)\n",
    "    if algorithm_kwargs is None or len(algorithm_kwargs) == 0:\n",
    "        algorithm_kwargs = default_algo_args[algorithm]\n",
    "    algo = algo_dict[algorithm](**algorithm_kwargs)\n",
    "    analysis = tune.run(\n",
    "        trainable,\n",
    "        config=config,\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=10,\n",
    "        local_dir=eu.settings.logging_dir,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"min-val_loss\",\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T20:59:54.949224Z",
     "iopub.status.busy": "2022-11-02T20:59:54.946934Z",
     "iopub.status.idle": "2022-11-02T21:00:27.646356Z",
     "shell.execute_reply": "2022-11-02T21:00:27.645632Z",
     "shell.execute_reply.started": "2022-11-02T20:59:54.949168Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU: 0\n",
      "GPUs: Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "import eugene as eu\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T21:00:27.648340Z",
     "iopub.status.busy": "2022-11-02T21:00:27.647691Z",
     "iopub.status.idle": "2022-11-02T21:00:27.725814Z",
     "shell.execute_reply": "2022-11-02T21:00:27.725248Z",
     "shell.execute_reply.started": "2022-11-02T21:00:27.648322Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7d4e733f0d4c98a17845b9180a3454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "\tohe_rev_seqs: None -> 1000 ohe_rev_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.reverse_complement_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T21:01:09.330871Z",
     "iopub.status.busy": "2022-11-02T21:01:09.330633Z",
     "iopub.status.idle": "2022-11-02T21:01:09.335438Z",
     "shell.execute_reply": "2022-11-02T21:01:09.334927Z",
     "shell.execute_reply.started": "2022-11-02T21:01:09.330853Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": tune.choice([\"ss\", \"ds\", \"ts\"]),\n",
    "  \"aggr\": tune.choice([\"max\", \"avg\"]),\n",
    "  \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "  \"batch_size\": tune.choice([32, 64, 128]),\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, tune.choice([16, 32])],\n",
    "    \"conv_kernels\": [tune.choice([3, 5])],\n",
    "    \"pool_kernels\": [tune.choice([2, 4])],\n",
    "    \"dropout_rates\": [tune.choice([0.1, 0.2])]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T21:01:12.450951Z",
     "iopub.status.busy": "2022-11-02T21:01:12.450327Z",
     "iopub.status.idle": "2022-11-02T21:06:59.598501Z",
     "shell.execute_reply": "2022-11-02T21:06:59.597847Z",
     "shell.execute_reply.started": "2022-11-02T21:01:12.450928Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 14:01:22,304\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-11-02 14:01:30,271\tWARNING function_trainable.py:620 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2022-11-02 14:01:30,276\tINFO tensorboardx.py:170 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2022-11-02 14:01:30,277\tWARNING callback.py:109 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-02 14:06:55 (running for 00:05:25.57)<br>Memory usage on this node: 32.0/503.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/16 CPUs, 1.0/1 GPUs, 0.0/327.12 GiB heap, 0.0/144.19 GiB objects<br>Current best trial: 8627f_00004 with loss=0.08036139607429504 and parameters={'arch': 'CNN', 'input_len': 100, 'output_dim': 1, 'strand': 'ss', 'aggr': 'max', 'lr': 0.07035298762986339, 'batch_size': 32, 'conv_kwargs': {'channels': [4, 32], 'conv_kernels': [5], 'pool_kernels': [4], 'dropout_rates': [0.2]}, 'fc_kwargs': {'hidden_dims': [32]}}<br>Result logdir: /cellar/users/aklie/projects/EUGENe/tests/notebooks/implement/eugene_logs/hyperopt_with_tune_2022-11-02_14-01-30<br>Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc                    </th><th>aggr  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_kwargs/chann...</th><th style=\"text-align: right;\">  conv_kwargs/conv_...</th><th style=\"text-align: right;\">  conv_kwargs/dropo...</th><th style=\"text-align: right;\">  conv_kwargs/pool_...</th><th style=\"text-align: right;\">         lr</th><th>strand  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyperopt_with_tune_8627f_00006</td><td>RUNNING   </td><td>192.168.169.178:1581800</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0173673  </td><td>ds      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00007</td><td>PENDING   </td><td>                       </td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0735737  </td><td>ts      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00008</td><td>PENDING   </td><td>                       </td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000677624</td><td>ds      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00009</td><td>PENDING   </td><td>                       </td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000574087</td><td>ds      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00000</td><td>TERMINATED</td><td>192.168.169.178:1549144</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000515969</td><td>ts      </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.0601 </td><td style=\"text-align: right;\">0.091529 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00001</td><td>TERMINATED</td><td>192.168.169.178:1549349</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0179881  </td><td>ts      </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.53674</td><td style=\"text-align: right;\">0.313833 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00002</td><td>TERMINATED</td><td>192.168.169.178:1580561</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000127382</td><td>ds      </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.59566</td><td style=\"text-align: right;\">0.0830846</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00003</td><td>TERMINATED</td><td>192.168.169.178:1580743</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00218172 </td><td>ts      </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.5154 </td><td style=\"text-align: right;\">0.0807496</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00004</td><td>TERMINATED</td><td>192.168.169.178:1580931</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.070353   </td><td>ss      </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.56855</td><td style=\"text-align: right;\">0.0803614</td></tr>\n",
       "<tr><td>hyperopt_with_tune_8627f_00005</td><td>TERMINATED</td><td>192.168.169.178:1581551</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0074696  </td><td>ss      </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.39162</td><td style=\"text-align: right;\">0.313791 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1549144)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1549144)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1549144)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1549144)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1549144)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 25.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 25.1 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549144)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00000:\n",
      "  date: 2022-11-02_14-02-32\n",
      "  done: false\n",
      "  experiment_id: 2e7721277c4c4f99a3d1df90f323e115\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08124096691608429\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1549144\n",
      "  time_since_restore: 10.965399265289307\n",
      "  time_this_iter_s: 10.965399265289307\n",
      "  time_total_s: 10.965399265289307\n",
      "  timestamp: 1667422952\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8627f_00000\n",
      "  warmup_time: 0.005463838577270508\n",
      "  \n",
      "Result for hyperopt_with_tune_8627f_00000:\n",
      "  date: 2022-11-02_14-02-32\n",
      "  done: true\n",
      "  experiment_id: 2e7721277c4c4f99a3d1df90f323e115\n",
      "  experiment_tag: 0_aggr=max,batch_size=128,1=16,0=3,0=0.1000,0=4,lr=0.0005,strand=ts\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.0915289968252182\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1549144\n",
      "  time_since_restore: 11.060132503509521\n",
      "  time_this_iter_s: 0.09473323822021484\n",
      "  time_total_s: 11.060132503509521\n",
      "  timestamp: 1667422952\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8627f_00000\n",
      "  warmup_time: 0.005463838577270508\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=1549349)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1549349)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1549349)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1549349)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1549349)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00001:\n",
      "  date: 2022-11-02_14-03-24\n",
      "  done: false\n",
      "  experiment_id: dfbfe43baa90421693bb357a156c6903\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.3260733485221863\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1549349\n",
      "  time_since_restore: 3.4219534397125244\n",
      "  time_this_iter_s: 3.4219534397125244\n",
      "  time_total_s: 3.4219534397125244\n",
      "  timestamp: 1667423004\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8627f_00001\n",
      "  warmup_time: 0.008169174194335938\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1549349)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00001:\n",
      "  date: 2022-11-02_14-03-24\n",
      "  done: true\n",
      "  experiment_id: dfbfe43baa90421693bb357a156c6903\n",
      "  experiment_tag: 1_aggr=avg,batch_size=128,1=16,0=3,0=0.2000,0=2,lr=0.0180,strand=ts\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.31383270025253296\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1549349\n",
      "  time_since_restore: 3.5367414951324463\n",
      "  time_this_iter_s: 0.11478805541992188\n",
      "  time_total_s: 3.5367414951324463\n",
      "  timestamp: 1667423004\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8627f_00001\n",
      "  warmup_time: 0.008169174194335938\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=1580561)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1580561)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1580561)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1580561)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1580561)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 12.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 12.6 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m 0.050     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580561)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00002:\n",
      "  date: 2022-11-02_14-04-09\n",
      "  done: false\n",
      "  experiment_id: e710b855531c4d869364136d0c2273e4\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.09047027677297592\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1580561\n",
      "  time_since_restore: 3.423060655593872\n",
      "  time_this_iter_s: 3.423060655593872\n",
      "  time_total_s: 3.423060655593872\n",
      "  timestamp: 1667423049\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8627f_00002\n",
      "  warmup_time: 0.007308244705200195\n",
      "  \n",
      "Result for hyperopt_with_tune_8627f_00002:\n",
      "  date: 2022-11-02_14-04-09\n",
      "  done: true\n",
      "  experiment_id: e710b855531c4d869364136d0c2273e4\n",
      "  experiment_tag: 2_aggr=max,batch_size=32,1=16,0=3,0=0.2000,0=4,lr=0.0001,strand=ds\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.0830845832824707\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1580561\n",
      "  time_since_restore: 3.595662832260132\n",
      "  time_this_iter_s: 0.17260217666625977\n",
      "  time_total_s: 3.595662832260132\n",
      "  timestamp: 1667423049\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8627f_00002\n",
      "  warmup_time: 0.007308244705200195\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=1580743)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1580743)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1580743)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1580743)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1580743)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00003:\n",
      "  date: 2022-11-02_14-04-54\n",
      "  done: false\n",
      "  experiment_id: 9a3154adf22c4a5c90ce556ff8a8a644\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08110418170690536\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1580743\n",
      "  time_since_restore: 3.4137845039367676\n",
      "  time_this_iter_s: 3.4137845039367676\n",
      "  time_total_s: 3.4137845039367676\n",
      "  timestamp: 1667423094\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8627f_00003\n",
      "  warmup_time: 0.008467674255371094\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 25.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 25.1 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580743)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00003:\n",
      "  date: 2022-11-02_14-04-55\n",
      "  done: true\n",
      "  experiment_id: 9a3154adf22c4a5c90ce556ff8a8a644\n",
      "  experiment_tag: 3_aggr=avg,batch_size=128,1=16,0=3,0=0.1000,0=4,lr=0.0022,strand=ts\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.08074960112571716\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1580743\n",
      "  time_since_restore: 3.5154025554656982\n",
      "  time_this_iter_s: 0.10161805152893066\n",
      "  time_total_s: 3.5154025554656982\n",
      "  timestamp: 1667423095\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8627f_00003\n",
      "  warmup_time: 0.008467674255371094\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=1580931)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1580931)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1580931)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1580931)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1580931)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 1 | convnet   | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 25.3 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 25.3 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m 0.101     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1580931)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00004:\n",
      "  date: 2022-11-02_14-05-39\n",
      "  done: false\n",
      "  experiment_id: ae6c42fddaeb41d9b1cbc9966d5e97cf\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.1307164579629898\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1580931\n",
      "  time_since_restore: 3.4134771823883057\n",
      "  time_this_iter_s: 3.4134771823883057\n",
      "  time_total_s: 3.4134771823883057\n",
      "  timestamp: 1667423139\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8627f_00004\n",
      "  warmup_time: 0.01111292839050293\n",
      "  \n",
      "Result for hyperopt_with_tune_8627f_00004:\n",
      "  date: 2022-11-02_14-05-39\n",
      "  done: true\n",
      "  experiment_id: ae6c42fddaeb41d9b1cbc9966d5e97cf\n",
      "  experiment_tag: 4_aggr=max,batch_size=32,1=32,0=5,0=0.2000,0=4,lr=0.0704,strand=ss\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.08036139607429504\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1580931\n",
      "  time_since_restore: 3.568549394607544\n",
      "  time_this_iter_s: 0.15507221221923828\n",
      "  time_total_s: 3.568549394607544\n",
      "  timestamp: 1667423139\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8627f_00004\n",
      "  warmup_time: 0.01111292839050293\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=1581551)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1581551)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1581551)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1581551)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1581551)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00005:\n",
      "  date: 2022-11-02_14-06-19\n",
      "  done: false\n",
      "  experiment_id: dfab6a2f080a4a73b113bd614fe06049\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.36915791034698486\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1581551\n",
      "  time_since_restore: 3.284954786300659\n",
      "  time_this_iter_s: 3.284954786300659\n",
      "  time_total_s: 3.284954786300659\n",
      "  timestamp: 1667423179\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8627f_00005\n",
      "  warmup_time: 0.004786968231201172\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 1 | convnet   | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 25.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 25.1 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581551)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_8627f_00005:\n",
      "  date: 2022-11-02_14-06-19\n",
      "  done: true\n",
      "  experiment_id: dfab6a2f080a4a73b113bd614fe06049\n",
      "  experiment_tag: 5_aggr=avg,batch_size=64,1=32,0=3,0=0.2000,0=4,lr=0.0075,strand=ss\n",
      "  hostname: carter-gpu-01\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.31379082798957825\n",
      "  node_ip: 192.168.169.178\n",
      "  pid: 1581551\n",
      "  time_since_restore: 3.3916239738464355\n",
      "  time_this_iter_s: 0.10666918754577637\n",
      "  time_total_s: 3.3916239738464355\n",
      "  timestamp: 1667423179\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8627f_00005\n",
      "  warmup_time: 0.004786968231201172\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=1581800)\u001b[0m GPU is available: True\n",
      "\u001b[2m\u001b[36m(pid=1581800)\u001b[0m Number of GPUs: 1\n",
      "\u001b[2m\u001b[36m(pid=1581800)\u001b[0m Current GPU: 0\n",
      "\u001b[2m\u001b[36m(pid=1581800)\u001b[0m GPUs: Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1581800)\u001b[0m Global seed set to 13\n",
      "2022-11-02 14:06:37,818\tWARNING tune.py:687 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m Set SLURM handle signals.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 1 | convnet   | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m /cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=1581800)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "2022-11-02 14:06:59,567\tERROR tune.py:754 -- Trials did not complete: [hyperopt_with_tune_8627f_00006, hyperopt_with_tune_8627f_00007, hyperopt_with_tune_8627f_00008, hyperopt_with_tune_8627f_00009]\n",
      "2022-11-02 14:06:59,568\tINFO tune.py:759 -- Total run time: 329.30 seconds (325.56 seconds for the tuning loop).\n",
      "2022-11-02 14:06:59,568\tWARNING tune.py:765 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xa1\\xcdpc\\xd5\\xe6\\xe8\\x9b\\x00\\xa7~G\\xd4\\x03\\xcdR\\xd0\\x16 ok%\\t\\xe2\\xe2\\xc7\\xb78;\\x11g\\xe2\\xbb\\xf8\\x97\\xf6)U\\xc8\\x7f\\x00\\xdc/\\x04\\xe1\\xb8\\x89\\xc3\\xc0_\\xa2\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03', b'-\\x00\\x02']\n",
      "Bad pipe message: %s [b'd\\x9b\\xe9\\x0b\"\\x98\\x07\\xebEf\\xdf\"8n\\x0b\\xbd\\x18\\x8a *L\\xf2\\xb4\\xa3B\\xd8\\xbc\\rs\\x17\\x00\\xeb\\xf5\\x84\\xb7\\xff\\x9eT|\\xf2\\xc0\\xbb\\'\\x07@\\xc1G\\xb8w@2\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t1', b'.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04', b'\\x03\\x06', b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b\"\\xb6\\xdcD\\xf2\\x18\\xa1\\x07\\x10\\x1dN\\x80\\xf0\\r\\xf5\\xe0\\xa9r\\xec\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\"]\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xd2\\xe1\\x8c\\xfb/\\xe0s@\\xf1[\\xac\\xb9\\xca\\x18\\x86nx\\xd7m-\\x9b\\xd9']\n",
      "Bad pipe message: %s [b'\\x06\\x04\\x01\\x05']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x89\\x1f-d\\xbf\\xed\\xca\\xf6\\x00\\xcd\\xa1\\xc9\\xf3>\\xa8p\"/\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xbbv\\xec\\xd9SE\\xfb\\xe4\\x88D}`\\xc7\\xdc6,\\xfe\\xe6\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84']\n",
      "Bad pipe message: %s [b'\\xb2\\xb1\\x8ac\\x9e\\xc11\\xf1$gJ@\\xf8Qq\\x8e\\xbf\\xa7\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n']\n",
      "Bad pipe message: %s [b'mb\\x07\\xce\\x89\\xe5\\x03\\xf0`o\\xe6\\xcd~Q\\xca\\x8d\\x92\\x88\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1']\n"
     ]
    }
   ],
   "source": [
    "res = eu.train.hyperopt(\n",
    "    tune_config,\n",
    "    sdata=sdata,\n",
    "    target_keys=\"activity_0\",\n",
    "    train_key=\"train_val\",\n",
    "    epochs=2,\n",
    "    gpus=1,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T21:09:04.665376Z",
     "iopub.status.busy": "2022-11-02T21:09:04.665135Z",
     "iopub.status.idle": "2022-11-02T21:09:04.754030Z",
     "shell.execute_reply": "2022-11-02T21:09:04.753601Z",
     "shell.execute_reply.started": "2022-11-02T21:09:04.665358Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>config/output_dim</th>\n",
       "      <th>config/strand</th>\n",
       "      <th>config/aggr</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/batch_size</th>\n",
       "      <th>config/conv_kwargs/channels</th>\n",
       "      <th>config/conv_kwargs/conv_kernels</th>\n",
       "      <th>config/conv_kwargs/pool_kernels</th>\n",
       "      <th>config/conv_kwargs/dropout_rates</th>\n",
       "      <th>config/fc_kwargs/hidden_dims</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8627f_00000</th>\n",
       "      <td>0.091529</td>\n",
       "      <td>0.094733</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2e7721277c4c4f99a3d1df90f323e115</td>\n",
       "      <td>2022-11-02_14-02-32</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>11.060133</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>128.0</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00001</th>\n",
       "      <td>0.313833</td>\n",
       "      <td>0.114788</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>dfbfe43baa90421693bb357a156c6903</td>\n",
       "      <td>2022-11-02_14-03-24</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>3.536741</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>128.0</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00002</th>\n",
       "      <td>0.083085</td>\n",
       "      <td>0.172602</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>e710b855531c4d869364136d0c2273e4</td>\n",
       "      <td>2022-11-02_14-04-09</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>3.595663</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ds</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00003</th>\n",
       "      <td>0.080750</td>\n",
       "      <td>0.101618</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9a3154adf22c4a5c90ce556ff8a8a644</td>\n",
       "      <td>2022-11-02_14-04-55</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>3.515403</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>128.0</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00004</th>\n",
       "      <td>0.080361</td>\n",
       "      <td>0.155072</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ae6c42fddaeb41d9b1cbc9966d5e97cf</td>\n",
       "      <td>2022-11-02_14-05-39</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>3.568549</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ss</td>\n",
       "      <td>max</td>\n",
       "      <td>0.070353</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[4, 32]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00005</th>\n",
       "      <td>0.313791</td>\n",
       "      <td>0.106669</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>dfab6a2f080a4a73b113bd614fe06049</td>\n",
       "      <td>2022-11-02_14-06-19</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>3.391624</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.007470</td>\n",
       "      <td>64.0</td>\n",
       "      <td>[4, 32]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5b8879f3f77143efa4996117c51c098d</td>\n",
       "      <td>2022-11-02_14-06-55</td>\n",
       "      <td>1.667423e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ds</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>128.0</td>\n",
       "      <td>[4, 32]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00008</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627f_00009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s  done  timesteps_total  \\\n",
       "trial_id                                                         \n",
       "8627f_00000  0.091529          0.094733  True              NaN   \n",
       "8627f_00001  0.313833          0.114788  True              NaN   \n",
       "8627f_00002  0.083085          0.172602  True              NaN   \n",
       "8627f_00003  0.080750          0.101618  True              NaN   \n",
       "8627f_00004  0.080361          0.155072  True              NaN   \n",
       "8627f_00005  0.313791          0.106669  True              NaN   \n",
       "8627f_00006       NaN               NaN   NaN              NaN   \n",
       "8627f_00007       NaN               NaN   NaN              NaN   \n",
       "8627f_00008       NaN               NaN   NaN              NaN   \n",
       "8627f_00009       NaN               NaN   NaN              NaN   \n",
       "\n",
       "             episodes_total  training_iteration  \\\n",
       "trial_id                                          \n",
       "8627f_00000             NaN                 2.0   \n",
       "8627f_00001             NaN                 2.0   \n",
       "8627f_00002             NaN                 2.0   \n",
       "8627f_00003             NaN                 2.0   \n",
       "8627f_00004             NaN                 2.0   \n",
       "8627f_00005             NaN                 2.0   \n",
       "8627f_00006             NaN                 NaN   \n",
       "8627f_00007             NaN                 NaN   \n",
       "8627f_00008             NaN                 NaN   \n",
       "8627f_00009             NaN                 NaN   \n",
       "\n",
       "                                experiment_id                 date  \\\n",
       "trial_id                                                             \n",
       "8627f_00000  2e7721277c4c4f99a3d1df90f323e115  2022-11-02_14-02-32   \n",
       "8627f_00001  dfbfe43baa90421693bb357a156c6903  2022-11-02_14-03-24   \n",
       "8627f_00002  e710b855531c4d869364136d0c2273e4  2022-11-02_14-04-09   \n",
       "8627f_00003  9a3154adf22c4a5c90ce556ff8a8a644  2022-11-02_14-04-55   \n",
       "8627f_00004  ae6c42fddaeb41d9b1cbc9966d5e97cf  2022-11-02_14-05-39   \n",
       "8627f_00005  dfab6a2f080a4a73b113bd614fe06049  2022-11-02_14-06-19   \n",
       "8627f_00006  5b8879f3f77143efa4996117c51c098d  2022-11-02_14-06-55   \n",
       "8627f_00007                               NaN                  NaN   \n",
       "8627f_00008                               NaN                  NaN   \n",
       "8627f_00009                               NaN                  NaN   \n",
       "\n",
       "                timestamp  time_total_s  ...  config/output_dim config/strand  \\\n",
       "trial_id                                 ...                                    \n",
       "8627f_00000  1.667423e+09     11.060133  ...                1.0            ts   \n",
       "8627f_00001  1.667423e+09      3.536741  ...                1.0            ts   \n",
       "8627f_00002  1.667423e+09      3.595663  ...                1.0            ds   \n",
       "8627f_00003  1.667423e+09      3.515403  ...                1.0            ts   \n",
       "8627f_00004  1.667423e+09      3.568549  ...                1.0            ss   \n",
       "8627f_00005  1.667423e+09      3.391624  ...                1.0            ss   \n",
       "8627f_00006  1.667423e+09           NaN  ...                1.0            ds   \n",
       "8627f_00007           NaN           NaN  ...                NaN           NaN   \n",
       "8627f_00008           NaN           NaN  ...                NaN           NaN   \n",
       "8627f_00009           NaN           NaN  ...                NaN           NaN   \n",
       "\n",
       "            config/aggr  config/lr  config/batch_size  \\\n",
       "trial_id                                                \n",
       "8627f_00000         max   0.000516              128.0   \n",
       "8627f_00001         avg   0.017988              128.0   \n",
       "8627f_00002         max   0.000127               32.0   \n",
       "8627f_00003         avg   0.002182              128.0   \n",
       "8627f_00004         max   0.070353               32.0   \n",
       "8627f_00005         avg   0.007470               64.0   \n",
       "8627f_00006         avg   0.017367              128.0   \n",
       "8627f_00007         NaN        NaN                NaN   \n",
       "8627f_00008         NaN        NaN                NaN   \n",
       "8627f_00009         NaN        NaN                NaN   \n",
       "\n",
       "             config/conv_kwargs/channels  config/conv_kwargs/conv_kernels  \\\n",
       "trial_id                                                                    \n",
       "8627f_00000                      [4, 16]                              [3]   \n",
       "8627f_00001                      [4, 16]                              [3]   \n",
       "8627f_00002                      [4, 16]                              [3]   \n",
       "8627f_00003                      [4, 16]                              [3]   \n",
       "8627f_00004                      [4, 32]                              [5]   \n",
       "8627f_00005                      [4, 32]                              [3]   \n",
       "8627f_00006                      [4, 32]                              [3]   \n",
       "8627f_00007                          NaN                              NaN   \n",
       "8627f_00008                          NaN                              NaN   \n",
       "8627f_00009                          NaN                              NaN   \n",
       "\n",
       "            config/conv_kwargs/pool_kernels config/conv_kwargs/dropout_rates  \\\n",
       "trial_id                                                                       \n",
       "8627f_00000                             [4]                            [0.1]   \n",
       "8627f_00001                             [2]                            [0.2]   \n",
       "8627f_00002                             [4]                            [0.2]   \n",
       "8627f_00003                             [4]                            [0.1]   \n",
       "8627f_00004                             [4]                            [0.2]   \n",
       "8627f_00005                             [4]                            [0.2]   \n",
       "8627f_00006                             [2]                            [0.2]   \n",
       "8627f_00007                             NaN                              NaN   \n",
       "8627f_00008                             NaN                              NaN   \n",
       "8627f_00009                             NaN                              NaN   \n",
       "\n",
       "             config/fc_kwargs/hidden_dims  \n",
       "trial_id                                   \n",
       "8627f_00000                          [32]  \n",
       "8627f_00001                          [32]  \n",
       "8627f_00002                          [32]  \n",
       "8627f_00003                          [32]  \n",
       "8627f_00004                          [32]  \n",
       "8627f_00005                          [32]  \n",
       "8627f_00006                          [32]  \n",
       "8627f_00007                           NaN  \n",
       "8627f_00008                           NaN  \n",
       "8627f_00009                           NaN  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_kwargs = dict(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "algo_kwargs = dict(\n",
    "  metric=\"loss\",\n",
    "  mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 19:56:02,240\tWARNING function_trainable.py:620 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-02 19:57:03 (running for 00:01:00.79)<br>Memory usage on this node: 6.3/7.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.41 GiB heap, 0.0/1.71 GiB objects<br>Current best trial: 60de2_00002 with loss=0.08121834695339203 and parameters={'arch': 'CNN', 'input_len': 100, 'output_dim': 1, 'strand': 'ss', 'aggr': 'max', 'lr': 0.007002007511197214, 'batch_size': 64, 'conv_kwargs': {'channels': [4, 16], 'conv_kernels': [3], 'pool_kernels': [4], 'dropout_rates': [0.2]}, 'fc_kwargs': {'hidden_dims': [32]}}<br>Result logdir: /workspaces/EUGENe/tests/notebooks/implement/eugene_logs/hyperopt_with_tune_2022-11-02_19-56-02<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc             </th><th>aggr  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_kwargs/chann...</th><th style=\"text-align: right;\">  conv_kwargs/conv_...</th><th style=\"text-align: right;\">  conv_kwargs/dropo...</th><th style=\"text-align: right;\">  conv_kwargs/pool_...</th><th style=\"text-align: right;\">         lr</th><th>strand  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyperopt_with_tune_60de2_00000</td><td>TERMINATED</td><td>172.16.5.4:24109</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.000111809</td><td>ts      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        40.2773 </td><td style=\"text-align: right;\">0.0817512</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00001</td><td>TERMINATED</td><td>172.16.5.4:24272</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.000358523</td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        27.5459 </td><td style=\"text-align: right;\">0.0837515</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00002</td><td>TERMINATED</td><td>172.16.5.4:24391</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00700201 </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        11.9237 </td><td style=\"text-align: right;\">0.0812183</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00003</td><td>TERMINATED</td><td>172.16.5.4:24514</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0248472  </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.69063</td><td style=\"text-align: right;\">0.175421 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00004</td><td>TERMINATED</td><td>172.16.5.4:24391</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0141074  </td><td>ds      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         8.41024</td><td style=\"text-align: right;\">0.0823958</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00005</td><td>TERMINATED</td><td>172.16.5.4:24514</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.000377087</td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.07208</td><td style=\"text-align: right;\">0.0814036</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00006</td><td>TERMINATED</td><td>172.16.5.4:24272</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00143636 </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.70398</td><td style=\"text-align: right;\">0.0843701</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00007</td><td>TERMINATED</td><td>172.16.5.4:24109</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0108183  </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.09573</td><td style=\"text-align: right;\">0.113748 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00008</td><td>TERMINATED</td><td>172.16.5.4:24514</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00012049 </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.22298</td><td style=\"text-align: right;\">0.0826188</td></tr>\n",
       "<tr><td>hyperopt_with_tune_60de2_00009</td><td>TERMINATED</td><td>172.16.5.4:24272</td><td>avg   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0149502  </td><td>ts      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.32482</td><td style=\"text-align: right;\">0.0857183</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24109)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 1 | convnet         | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 101 K     Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 101 K     Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 0.405     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24272)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 1 | convnet   | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24391)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 12.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 12.6 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 0.050     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24514)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_60de2_00001:\n",
      "  date: 2022-11-02_19-56-27\n",
      "  done: false\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08339817076921463\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 0.3202328681945801\n",
      "  time_this_iter_s: 0.3202328681945801\n",
      "  time_total_s: 0.3202328681945801\n",
      "  timestamp: 1667418987\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00001\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00002:\n",
      "  date: 2022-11-02_19-56-39\n",
      "  done: false\n",
      "  experiment_id: 3d658409235d40f79fce35448e716d1e\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08155816048383713\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24391\n",
      "  time_since_restore: 0.6753432750701904\n",
      "  time_this_iter_s: 0.6753432750701904\n",
      "  time_total_s: 0.6753432750701904\n",
      "  timestamp: 1667418999\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00002\n",
      "  warmup_time: 0.003935098648071289\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00000:\n",
      "  date: 2022-11-02_19-56-16\n",
      "  done: false\n",
      "  experiment_id: ca3dbadda46740fc9238f2f4cf26d15f\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.0829751044511795\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24109\n",
      "  time_since_restore: 0.4180793762207031\n",
      "  time_this_iter_s: 0.4180793762207031\n",
      "  time_total_s: 0.4180793762207031\n",
      "  timestamp: 1667418976\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00000\n",
      "  warmup_time: 0.004333019256591797\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 12.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 12.6 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0.050     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_60de2_00003:\n",
      "  date: 2022-11-02_19-56-49\n",
      "  done: false\n",
      "  experiment_id: 9df1b8a804044f9db4616ac0959882e3\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.40783026814460754\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24514\n",
      "  time_since_restore: 0.3414194583892822\n",
      "  time_this_iter_s: 0.3414194583892822\n",
      "  time_total_s: 0.3414194583892822\n",
      "  timestamp: 1667419009\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00003\n",
      "  warmup_time: 0.007612466812133789\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00002:\n",
      "  date: 2022-11-02_19-56-50\n",
      "  done: true\n",
      "  experiment_id: 3d658409235d40f79fce35448e716d1e\n",
      "  experiment_tag: 2_aggr=max,batch_size=64,1=16,0=3,0=0.2000,0=4,lr=0.0070,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08121834695339203\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24391\n",
      "  time_since_restore: 11.923672676086426\n",
      "  time_this_iter_s: 0.29253602027893066\n",
      "  time_total_s: 11.923672676086426\n",
      "  timestamp: 1667419010\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00002\n",
      "  warmup_time: 0.003935098648071289\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 1 | convnet   | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24391)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "Result for hyperopt_with_tune_60de2_00003:\n",
      "  date: 2022-11-02_19-56-53\n",
      "  done: true\n",
      "  experiment_id: 9df1b8a804044f9db4616ac0959882e3\n",
      "  experiment_tag: 3_aggr=max,batch_size=128,1=16,0=3,0=0.2000,0=4,lr=0.0248,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.17542074620723724\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24514\n",
      "  time_since_restore: 3.6906309127807617\n",
      "  time_this_iter_s: 0.14952635765075684\n",
      "  time_total_s: 3.6906309127807617\n",
      "  timestamp: 1667419013\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00003\n",
      "  warmup_time: 0.007612466812133789\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "Result for hyperopt_with_tune_60de2_00004:\n",
      "  date: 2022-11-02_19-56-53\n",
      "  done: false\n",
      "  experiment_id: 3d658409235d40f79fce35448e716d1e\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08326132595539093\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24391\n",
      "  time_since_restore: 2.061593532562256\n",
      "  time_this_iter_s: 2.061593532562256\n",
      "  time_total_s: 2.061593532562256\n",
      "  timestamp: 1667419013\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00004\n",
      "  warmup_time: 0.003935098648071289\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 25.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 25.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_60de2_00001:\n",
      "  date: 2022-11-02_19-56-54\n",
      "  done: false\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08375154435634613\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 27.545893669128418\n",
      "  time_this_iter_s: 4.034074068069458\n",
      "  time_total_s: 27.545893669128418\n",
      "  timestamp: 1667419014\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00001\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00005:\n",
      "  date: 2022-11-02_19-56-54\n",
      "  done: false\n",
      "  experiment_id: 9df1b8a804044f9db4616ac0959882e3\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08837398886680603\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24514\n",
      "  time_since_restore: 1.608764886856079\n",
      "  time_this_iter_s: 1.608764886856079\n",
      "  time_total_s: 1.608764886856079\n",
      "  timestamp: 1667419014\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00005\n",
      "  warmup_time: 0.007612466812133789\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00001:\n",
      "  date: 2022-11-02_19-56-54\n",
      "  done: true\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  experiment_tag: 1_aggr=max,batch_size=32,1=32,0=3,0=0.1000,0=2,lr=0.0004,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08375154435634613\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 27.545893669128418\n",
      "  time_this_iter_s: 4.034074068069458\n",
      "  time_total_s: 27.545893669128418\n",
      "  timestamp: 1667419014\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00001\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 12.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 12.6 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0.050     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "Result for hyperopt_with_tune_60de2_00000:\n",
      "  date: 2022-11-02_19-56-55\n",
      "  done: false\n",
      "  experiment_id: ca3dbadda46740fc9238f2f4cf26d15f\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.08183571696281433\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24109\n",
      "  time_since_restore: 39.323641300201416\n",
      "  time_this_iter_s: 2.2880141735076904\n",
      "  time_total_s: 39.323641300201416\n",
      "  timestamp: 1667419015\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 60de2_00000\n",
      "  warmup_time: 0.004333019256591797\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_60de2_00006:\n",
      "  date: 2022-11-02_19-56-55\n",
      "  done: false\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.0864991843700409\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 0.7326774597167969\n",
      "  time_this_iter_s: 0.7326774597167969\n",
      "  time_total_s: 0.7326774597167969\n",
      "  timestamp: 1667419015\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00006\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00000:\n",
      "  date: 2022-11-02_19-56-56\n",
      "  done: true\n",
      "  experiment_id: ca3dbadda46740fc9238f2f4cf26d15f\n",
      "  experiment_tag: 0_aggr=max,batch_size=32,1=32,0=3,0=0.1000,0=2,lr=0.0001,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08175120502710342\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24109\n",
      "  time_since_restore: 40.2772753238678\n",
      "  time_this_iter_s: 0.9536340236663818\n",
      "  time_total_s: 40.2772753238678\n",
      "  timestamp: 1667419016\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00000\n",
      "  warmup_time: 0.004333019256591797\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m 0.101     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24109)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_60de2_00005:\n",
      "  date: 2022-11-02_19-56-57\n",
      "  done: true\n",
      "  experiment_id: 9df1b8a804044f9db4616ac0959882e3\n",
      "  experiment_tag: 5_aggr=max,batch_size=64,1=16,0=5,0=0.1000,0=2,lr=0.0004,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08140359073877335\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24514\n",
      "  time_since_restore: 4.072077035903931\n",
      "  time_this_iter_s: 0.71238112449646\n",
      "  time_total_s: 4.072077035903931\n",
      "  timestamp: 1667419017\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00005\n",
      "  warmup_time: 0.007612466812133789\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00007:\n",
      "  date: 2022-11-02_19-56-57\n",
      "  done: false\n",
      "  experiment_id: ca3dbadda46740fc9238f2f4cf26d15f\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.37217476963996887\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24109\n",
      "  time_since_restore: 1.2666597366333008\n",
      "  time_this_iter_s: 1.2666597366333008\n",
      "  time_total_s: 1.2666597366333008\n",
      "  timestamp: 1667419017\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00007\n",
      "  warmup_time: 0.004333019256591797\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24514)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "Result for hyperopt_with_tune_60de2_00008:\n",
      "  date: 2022-11-02_19-56-57\n",
      "  done: false\n",
      "  experiment_id: 9df1b8a804044f9db4616ac0959882e3\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.19150777161121368\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24514\n",
      "  time_since_restore: 0.5997583866119385\n",
      "  time_this_iter_s: 0.5997583866119385\n",
      "  time_total_s: 0.5997583866119385\n",
      "  timestamp: 1667419017\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00008\n",
      "  warmup_time: 0.007612466812133789\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00006:\n",
      "  date: 2022-11-02_19-56-58\n",
      "  done: true\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  experiment_tag: 6_aggr=max,batch_size=32,1=16,0=3,0=0.2000,0=4,lr=0.0014,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08437011390924454\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 3.7039785385131836\n",
      "  time_this_iter_s: 0.869901180267334\n",
      "  time_total_s: 3.7039785385131836\n",
      "  timestamp: 1667419018\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00006\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 1 | convnet         | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 50.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 50.6 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=24272)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "Result for hyperopt_with_tune_60de2_00007:\n",
      "  date: 2022-11-02_19-56-59\n",
      "  done: true\n",
      "  experiment_id: ca3dbadda46740fc9238f2f4cf26d15f\n",
      "  experiment_tag: 7_aggr=max,batch_size=64,1=16,0=3,0=0.1000,0=2,lr=0.0108,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.1137479767203331\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24109\n",
      "  time_since_restore: 3.09572696685791\n",
      "  time_this_iter_s: 0.5966131687164307\n",
      "  time_total_s: 3.09572696685791\n",
      "  timestamp: 1667419019\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00007\n",
      "  warmup_time: 0.004333019256591797\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00008:\n",
      "  date: 2022-11-02_19-56-59\n",
      "  done: true\n",
      "  experiment_id: 9df1b8a804044f9db4616ac0959882e3\n",
      "  experiment_tag: 8_aggr=avg,batch_size=128,1=16,0=5,0=0.2000,0=4,lr=0.0001,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08261878788471222\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24514\n",
      "  time_since_restore: 2.222977638244629\n",
      "  time_this_iter_s: 0.26865315437316895\n",
      "  time_total_s: 2.222977638244629\n",
      "  timestamp: 1667419019\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00008\n",
      "  warmup_time: 0.007612466812133789\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00004:\n",
      "  date: 2022-11-02_19-56-59\n",
      "  done: false\n",
      "  experiment_id: 3d658409235d40f79fce35448e716d1e\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08239581435918808\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24391\n",
      "  time_since_restore: 8.410242557525635\n",
      "  time_this_iter_s: 1.605276107788086\n",
      "  time_total_s: 8.410242557525635\n",
      "  timestamp: 1667419019\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00004\n",
      "  warmup_time: 0.003935098648071289\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00004:\n",
      "  date: 2022-11-02_19-56-59\n",
      "  done: true\n",
      "  experiment_id: 3d658409235d40f79fce35448e716d1e\n",
      "  experiment_tag: 4_aggr=max,batch_size=32,1=32,0=3,0=0.2000,0=2,lr=0.0141,strand=ds\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08239581435918808\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24391\n",
      "  time_since_restore: 8.410242557525635\n",
      "  time_this_iter_s: 1.605276107788086\n",
      "  time_total_s: 8.410242557525635\n",
      "  timestamp: 1667419019\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00004\n",
      "  warmup_time: 0.003935098648071289\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00009:\n",
      "  date: 2022-11-02_19-56-59\n",
      "  done: false\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.2821178734302521\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 1.1856622695922852\n",
      "  time_this_iter_s: 1.1856622695922852\n",
      "  time_total_s: 1.1856622695922852\n",
      "  timestamp: 1667419019\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 60de2_00009\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n",
      "Result for hyperopt_with_tune_60de2_00009:\n",
      "  date: 2022-11-02_19-57-02\n",
      "  done: true\n",
      "  experiment_id: 35112c00fbb949268e427840934b0ad9\n",
      "  experiment_tag: 9_aggr=avg,batch_size=32,1=32,0=5,0=0.1000,0=4,lr=0.0150,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08571826666593552\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 24272\n",
      "  time_since_restore: 4.3248231410980225\n",
      "  time_this_iter_s: 0.3999185562133789\n",
      "  time_total_s: 4.3248231410980225\n",
      "  timestamp: 1667419022\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 60de2_00009\n",
      "  warmup_time: 0.0037529468536376953\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 19:57:03,324\tINFO tune.py:759 -- Total run time: 61.08 seconds (60.77 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "res = eu.train.hyperopt(\n",
    "    tune_config,\n",
    "    sdata=sdata,\n",
    "    algorithm=\"BayesOptSearch\",\n",
    "    target_keys=\"activity_0\",\n",
    "    train_key=\"train_val\",\n",
    "    epochs=5,\n",
    "    gpus=0,\n",
    "    num_workers=0,\n",
    "    algorithm_kwargs=algo_kwargs,\n",
    "    scheduler_kwargs=scheduler_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': 'CNN',\n",
       " 'input_len': 100,\n",
       " 'output_dim': 1,\n",
       " 'strand': 'ds',\n",
       " 'aggr': 'avg',\n",
       " 'lr': 0.00044313707981392264,\n",
       " 'batch_size': 32,\n",
       " 'conv_kwargs': {'channels': [4, 16],\n",
       "  'conv_kernels': [5],\n",
       "  'pool_kernels': [4],\n",
       "  'dropout_rates': [0.1]},\n",
       " 'fc_kwargs': {'hidden_dims': [32]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tune.with_parameters(\n",
    "    hyperopt_with_tune,\n",
    "    sdata=sdata,\n",
    "    target_keys=\"activity_0\",\n",
    "    train_key=\"train_val\",\n",
    "    epochs=10,\n",
    "    gpus=0,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": tune.choice([\"ss\", \"ds\", \"ts\"]),\n",
    "  \"aggr\": tune.choice([\"max\", \"avg\"]),\n",
    "  \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "  \"batch_size\": tune.choice([32, 64, 128]),\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, 16],\n",
    "    \"conv_kernels\": [tune.choice([3, 5])],\n",
    "    \"pool_kernels\": [tune.choice([2, 4])],\n",
    "    \"dropout_rates\": [tune.choice([0.1, 0.2])]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:14:11,580\tWARNING services.py:1893 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.97gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-11-01 20:14:12,966\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "/home/vscode/.local/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py:644: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/ray-air/key-concepts.html#session\n",
      "\n",
      "  DeprecationWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-01 20:14:57 (running for 00:00:43.66)<br>Memory usage on this node: 4.7/7.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.59 GiB heap, 0.0/1.79 GiB objects<br>Current best trial: c133d_00009 with loss=0.07941604405641556 and parameters={'arch': 'CNN', 'input_len': 100, 'output_dim': 1, 'strand': 'ts', 'aggr': 'avg', 'lr': 0.0006544351060732511, 'batch_size': 128, 'conv_kwargs': {'channels': [4, 16], 'conv_kernels': [5], 'pool_kernels': [4], 'dropout_rates': [0.1]}, 'fc_kwargs': {'hidden_dims': [32]}}<br>Result logdir: /workspaces/EUGENe/tests/notebooks/implement/eugene_logs/test<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th>aggr  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_kwargs/conv_...</th><th style=\"text-align: right;\">  conv_kwargs/dropo...</th><th style=\"text-align: right;\">  conv_kwargs/pool_...</th><th style=\"text-align: right;\">         lr</th><th>strand  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyperopt_with_tune_c133d_00000</td><td>TERMINATED</td><td>172.16.5.4:6119</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.000515969</td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        27.7774 </td><td style=\"text-align: right;\">0.0812072</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00001</td><td>TERMINATED</td><td>172.16.5.4:6237</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0022927  </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        19.5342 </td><td style=\"text-align: right;\">0.0842177</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00002</td><td>TERMINATED</td><td>172.16.5.4:6354</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0146573  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        10.6593 </td><td style=\"text-align: right;\">0.0802305</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00003</td><td>TERMINATED</td><td>172.16.5.4:6459</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0372549  </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.73107</td><td style=\"text-align: right;\">0.0891374</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00004</td><td>TERMINATED</td><td>172.16.5.4:6459</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0954421  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.44144</td><td style=\"text-align: right;\">0.0802142</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00005</td><td>TERMINATED</td><td>172.16.5.4:6119</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0656628  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         3.69808</td><td style=\"text-align: right;\">0.0802464</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00006</td><td>TERMINATED</td><td>172.16.5.4:6354</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00775331 </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         3.4115 </td><td style=\"text-align: right;\">0.085105 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00007</td><td>TERMINATED</td><td>172.16.5.4:6237</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000389489</td><td>ds      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.96298</td><td style=\"text-align: right;\">0.082369 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00008</td><td>TERMINATED</td><td>172.16.5.4:6354</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0735737  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.9596 </td><td style=\"text-align: right;\">0.0837441</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00009</td><td>TERMINATED</td><td>172.16.5.4:6119</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000654435</td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.9034 </td><td style=\"text-align: right;\">0.079416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3984575a10>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\u001b[2m\u001b[36m(pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0.101     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f81bace6f90>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0.200     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3741e5b810>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6459)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00001:\n",
      "  date: 2022-11-01_20-14-33\n",
      "  done: false\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08753872662782669\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 0.20910406112670898\n",
      "  time_this_iter_s: 0.20910406112670898\n",
      "  time_total_s: 0.20910406112670898\n",
      "  timestamp: 1667333673\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00001\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00000:\n",
      "  date: 2022-11-01_20-14-24\n",
      "  done: false\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.1006212830543518\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 0.761347770690918\n",
      "  time_this_iter_s: 0.761347770690918\n",
      "  time_total_s: 0.761347770690918\n",
      "  timestamp: 1667333664\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00000\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00002:\n",
      "  date: 2022-11-01_20-14-41\n",
      "  done: false\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.3482355773448944\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 0.2775743007659912\n",
      "  time_this_iter_s: 0.2775743007659912\n",
      "  time_total_s: 0.2775743007659912\n",
      "  timestamp: 1667333681\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00002\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fae4c3cdf10>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00003:\n",
      "  date: 2022-11-01_20-14-50\n",
      "  done: false\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.49401620030403137\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 0.2556467056274414\n",
      "  time_this_iter_s: 0.2556467056274414\n",
      "  time_total_s: 0.2556467056274414\n",
      "  timestamp: 1667333690\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00003\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00003:\n",
      "  date: 2022-11-01_20-14-51\n",
      "  done: true\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  experiment_tag: 3_aggr=avg,batch_size=128,0=5,0=0.2000,0=4,lr=0.0373,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08913742005825043\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 1.7310707569122314\n",
      "  time_this_iter_s: 0.11130189895629883\n",
      "  time_total_s: 1.7310707569122314\n",
      "  timestamp: 1667333691\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00003\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fae46dbe290>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00000:\n",
      "  date: 2022-11-01_20-14-51\n",
      "  done: true\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  experiment_tag: 0_aggr=max,batch_size=128,0=3,0=0.2000,0=2,lr=0.0005,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08120723813772202\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 27.777360677719116\n",
      "  time_this_iter_s: 0.3084733486175537\n",
      "  time_total_s: 27.777360677719116\n",
      "  timestamp: 1667333691\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00000\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f397e7beed0>]\n",
      "Result for hyperopt_with_tune_c133d_00002:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: true\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  experiment_tag: 2_aggr=avg,batch_size=64,0=5,0=0.1000,0=2,lr=0.0147,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08023054152727127\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 10.659316539764404\n",
      "  time_this_iter_s: 0.34508705139160156\n",
      "  time_total_s: 10.659316539764404\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00002\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.1 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00004:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: false\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.10064372420310974\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 0.5659947395324707\n",
      "  time_this_iter_s: 0.5659947395324707\n",
      "  time_total_s: 0.5659947395324707\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00004\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3741f59250>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00005:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: false\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.49822548031806946\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 0.40572214126586914\n",
      "  time_this_iter_s: 0.40572214126586914\n",
      "  time_total_s: 0.40572214126586914\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00005\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00006:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: false\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08371169865131378\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 0.2671666145324707\n",
      "  time_this_iter_s: 0.2671666145324707\n",
      "  time_total_s: 0.2671666145324707\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00006\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00001:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: true\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  experiment_tag: 1_aggr=max,batch_size=32,0=3,0=0.1000,0=2,lr=0.0023,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08421774953603745\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 19.534242153167725\n",
      "  time_this_iter_s: 0.34664058685302734\n",
      "  time_total_s: 19.534242153167725\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00001\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f81bad89390>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00007:\n",
      "  date: 2022-11-01_20-14-53\n",
      "  done: false\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.12046148627996445\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 0.38976120948791504\n",
      "  time_this_iter_s: 0.38976120948791504\n",
      "  time_total_s: 0.38976120948791504\n",
      "  timestamp: 1667333693\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00007\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00006:\n",
      "  date: 2022-11-01_20-14-55\n",
      "  done: true\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  experiment_tag: 6_aggr=avg,batch_size=64,0=5,0=0.2000,0=4,lr=0.0078,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08510497957468033\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 3.4114990234375\n",
      "  time_this_iter_s: 0.24724721908569336\n",
      "  time_total_s: 3.4114990234375\n",
      "  timestamp: 1667333695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00006\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00005:\n",
      "  date: 2022-11-01_20-14-55\n",
      "  done: true\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  experiment_tag: 5_aggr=max,batch_size=64,0=3,0=0.1000,0=4,lr=0.0657,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.0802464485168457\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 3.6980841159820557\n",
      "  time_this_iter_s: 0.38837718963623047\n",
      "  time_total_s: 3.6980841159820557\n",
      "  timestamp: 1667333695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00005\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3741f971d0>]\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f39846b4fd0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0.102     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00007:\n",
      "  date: 2022-11-01_20-14-55\n",
      "  done: true\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  experiment_tag: 7_aggr=max,batch_size=128,0=5,0=0.2000,0=4,lr=0.0004,strand=ds\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08236898481845856\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 2.9629783630371094\n",
      "  time_this_iter_s: 0.21939635276794434\n",
      "  time_total_s: 2.9629783630371094\n",
      "  timestamp: 1667333695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00007\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00009:\n",
      "  date: 2022-11-01_20-14-56\n",
      "  done: false\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.10714306682348251\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 0.2818934917449951\n",
      "  time_this_iter_s: 0.2818934917449951\n",
      "  time_total_s: 0.2818934917449951\n",
      "  timestamp: 1667333696\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00009\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00008:\n",
      "  date: 2022-11-01_20-14-56\n",
      "  done: false\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.41163933277130127\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 0.378922700881958\n",
      "  time_this_iter_s: 0.378922700881958\n",
      "  time_total_s: 0.378922700881958\n",
      "  timestamp: 1667333696\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00008\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00004:\n",
      "  date: 2022-11-01_20-14-57\n",
      "  done: true\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  experiment_tag: 4_aggr=max,batch_size=32,0=3,0=0.1000,0=2,lr=0.0954,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08021415770053864\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 5.441444158554077\n",
      "  time_this_iter_s: 0.3250539302825928\n",
      "  time_total_s: 5.441444158554077\n",
      "  timestamp: 1667333697\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00004\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00009:\n",
      "  date: 2022-11-01_20-14-57\n",
      "  done: true\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  experiment_tag: 9_aggr=avg,batch_size=128,0=5,0=0.1000,0=4,lr=0.0007,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.07941604405641556\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 1.903404712677002\n",
      "  time_this_iter_s: 0.1506028175354004\n",
      "  time_total_s: 1.903404712677002\n",
      "  timestamp: 1667333697\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00009\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00008:\n",
      "  date: 2022-11-01_20-14-57\n",
      "  done: true\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  experiment_tag: 8_aggr=max,batch_size=128,0=3,0=0.2000,0=2,lr=0.0736,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08374407887458801\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 1.959604263305664\n",
      "  time_this_iter_s: 0.11938953399658203\n",
      "  time_total_s: 1.959604263305664\n",
      "  timestamp: 1667333697\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00008\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:14:57,962\tINFO tune.py:759 -- Total run time: 43.97 seconds (43.63 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    config=tune_config,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=10,\n",
    "    local_dir=eu.settings.logging_dir,\n",
    "    keep_checkpoints_num=1,\n",
    "    checkpoint_score_attr=\"min-val_loss\",\n",
    "    name=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>config/output_dim</th>\n",
       "      <th>config/strand</th>\n",
       "      <th>config/aggr</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/batch_size</th>\n",
       "      <th>config/conv_kwargs/channels</th>\n",
       "      <th>config/conv_kwargs/conv_kernels</th>\n",
       "      <th>config/conv_kwargs/pool_kernels</th>\n",
       "      <th>config/conv_kwargs/dropout_rates</th>\n",
       "      <th>config/fc_kwargs/hidden_dims</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c133d_00000</th>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.308473</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>a96250e996b1433ebcf48bb5b6a3d41b</td>\n",
       "      <td>2022-11-01_20-14-51</td>\n",
       "      <td>1667333691</td>\n",
       "      <td>27.777361</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00001</th>\n",
       "      <td>0.084218</td>\n",
       "      <td>0.346641</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>5cc423fa139b475bb55725e4c27d3c03</td>\n",
       "      <td>2022-11-01_20-14-52</td>\n",
       "      <td>1667333692</td>\n",
       "      <td>19.534242</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>max</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>32</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00002</th>\n",
       "      <td>0.080231</td>\n",
       "      <td>0.345087</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2033aa1d5f7047aaabba2e6f654f3661</td>\n",
       "      <td>2022-11-01_20-14-52</td>\n",
       "      <td>1667333692</td>\n",
       "      <td>10.659317</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00003</th>\n",
       "      <td>0.089137</td>\n",
       "      <td>0.111302</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0e2662855e564568801e80eac2f05e4a</td>\n",
       "      <td>2022-11-01_20-14-51</td>\n",
       "      <td>1667333691</td>\n",
       "      <td>1.731071</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00004</th>\n",
       "      <td>0.080214</td>\n",
       "      <td>0.325054</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0e2662855e564568801e80eac2f05e4a</td>\n",
       "      <td>2022-11-01_20-14-57</td>\n",
       "      <td>1667333697</td>\n",
       "      <td>5.441444</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.095442</td>\n",
       "      <td>32</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00005</th>\n",
       "      <td>0.080246</td>\n",
       "      <td>0.388377</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>a96250e996b1433ebcf48bb5b6a3d41b</td>\n",
       "      <td>2022-11-01_20-14-55</td>\n",
       "      <td>1667333695</td>\n",
       "      <td>3.698084</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00006</th>\n",
       "      <td>0.085105</td>\n",
       "      <td>0.247247</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2033aa1d5f7047aaabba2e6f654f3661</td>\n",
       "      <td>2022-11-01_20-14-55</td>\n",
       "      <td>1667333695</td>\n",
       "      <td>3.411499</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00007</th>\n",
       "      <td>0.082369</td>\n",
       "      <td>0.219396</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>5cc423fa139b475bb55725e4c27d3c03</td>\n",
       "      <td>2022-11-01_20-14-55</td>\n",
       "      <td>1667333695</td>\n",
       "      <td>2.962978</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ds</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00008</th>\n",
       "      <td>0.083744</td>\n",
       "      <td>0.119390</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2033aa1d5f7047aaabba2e6f654f3661</td>\n",
       "      <td>2022-11-01_20-14-57</td>\n",
       "      <td>1667333697</td>\n",
       "      <td>1.959604</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.073574</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00009</th>\n",
       "      <td>0.079416</td>\n",
       "      <td>0.150603</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>a96250e996b1433ebcf48bb5b6a3d41b</td>\n",
       "      <td>2022-11-01_20-14-57</td>\n",
       "      <td>1667333697</td>\n",
       "      <td>1.903405</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s  done timesteps_total episodes_total  \\\n",
       "trial_id                                                                       \n",
       "c133d_00000  0.081207          0.308473  True            None           None   \n",
       "c133d_00001  0.084218          0.346641  True            None           None   \n",
       "c133d_00002  0.080231          0.345087  True            None           None   \n",
       "c133d_00003  0.089137          0.111302  True            None           None   \n",
       "c133d_00004  0.080214          0.325054  True            None           None   \n",
       "c133d_00005  0.080246          0.388377  True            None           None   \n",
       "c133d_00006  0.085105          0.247247  True            None           None   \n",
       "c133d_00007  0.082369          0.219396  True            None           None   \n",
       "c133d_00008  0.083744          0.119390  True            None           None   \n",
       "c133d_00009  0.079416          0.150603  True            None           None   \n",
       "\n",
       "             training_iteration                     experiment_id  \\\n",
       "trial_id                                                            \n",
       "c133d_00000                  10  a96250e996b1433ebcf48bb5b6a3d41b   \n",
       "c133d_00001                  10  5cc423fa139b475bb55725e4c27d3c03   \n",
       "c133d_00002                  10  2033aa1d5f7047aaabba2e6f654f3661   \n",
       "c133d_00003                  10  0e2662855e564568801e80eac2f05e4a   \n",
       "c133d_00004                  10  0e2662855e564568801e80eac2f05e4a   \n",
       "c133d_00005                  10  a96250e996b1433ebcf48bb5b6a3d41b   \n",
       "c133d_00006                  10  2033aa1d5f7047aaabba2e6f654f3661   \n",
       "c133d_00007                  10  5cc423fa139b475bb55725e4c27d3c03   \n",
       "c133d_00008                  10  2033aa1d5f7047aaabba2e6f654f3661   \n",
       "c133d_00009                  10  a96250e996b1433ebcf48bb5b6a3d41b   \n",
       "\n",
       "                            date   timestamp  time_total_s  ...  \\\n",
       "trial_id                                                    ...   \n",
       "c133d_00000  2022-11-01_20-14-51  1667333691     27.777361  ...   \n",
       "c133d_00001  2022-11-01_20-14-52  1667333692     19.534242  ...   \n",
       "c133d_00002  2022-11-01_20-14-52  1667333692     10.659317  ...   \n",
       "c133d_00003  2022-11-01_20-14-51  1667333691      1.731071  ...   \n",
       "c133d_00004  2022-11-01_20-14-57  1667333697      5.441444  ...   \n",
       "c133d_00005  2022-11-01_20-14-55  1667333695      3.698084  ...   \n",
       "c133d_00006  2022-11-01_20-14-55  1667333695      3.411499  ...   \n",
       "c133d_00007  2022-11-01_20-14-55  1667333695      2.962978  ...   \n",
       "c133d_00008  2022-11-01_20-14-57  1667333697      1.959604  ...   \n",
       "c133d_00009  2022-11-01_20-14-57  1667333697      1.903405  ...   \n",
       "\n",
       "             config/output_dim config/strand config/aggr  config/lr  \\\n",
       "trial_id                                                              \n",
       "c133d_00000                  1            ts         max   0.000516   \n",
       "c133d_00001                  1            ss         max   0.002293   \n",
       "c133d_00002                  1            ts         avg   0.014657   \n",
       "c133d_00003                  1            ss         avg   0.037255   \n",
       "c133d_00004                  1            ts         max   0.095442   \n",
       "c133d_00005                  1            ts         max   0.065663   \n",
       "c133d_00006                  1            ss         avg   0.007753   \n",
       "c133d_00007                  1            ds         max   0.000389   \n",
       "c133d_00008                  1            ts         max   0.073574   \n",
       "c133d_00009                  1            ts         avg   0.000654   \n",
       "\n",
       "             config/batch_size  config/conv_kwargs/channels  \\\n",
       "trial_id                                                      \n",
       "c133d_00000                128                      [4, 16]   \n",
       "c133d_00001                 32                      [4, 16]   \n",
       "c133d_00002                 64                      [4, 16]   \n",
       "c133d_00003                128                      [4, 16]   \n",
       "c133d_00004                 32                      [4, 16]   \n",
       "c133d_00005                 64                      [4, 16]   \n",
       "c133d_00006                 64                      [4, 16]   \n",
       "c133d_00007                128                      [4, 16]   \n",
       "c133d_00008                128                      [4, 16]   \n",
       "c133d_00009                128                      [4, 16]   \n",
       "\n",
       "             config/conv_kwargs/conv_kernels config/conv_kwargs/pool_kernels  \\\n",
       "trial_id                                                                       \n",
       "c133d_00000                              [3]                             [2]   \n",
       "c133d_00001                              [3]                             [2]   \n",
       "c133d_00002                              [5]                             [2]   \n",
       "c133d_00003                              [5]                             [4]   \n",
       "c133d_00004                              [3]                             [2]   \n",
       "c133d_00005                              [3]                             [4]   \n",
       "c133d_00006                              [5]                             [4]   \n",
       "c133d_00007                              [5]                             [4]   \n",
       "c133d_00008                              [3]                             [2]   \n",
       "c133d_00009                              [5]                             [4]   \n",
       "\n",
       "            config/conv_kwargs/dropout_rates  config/fc_kwargs/hidden_dims  \n",
       "trial_id                                                                    \n",
       "c133d_00000                            [0.2]                          [32]  \n",
       "c133d_00001                            [0.1]                          [32]  \n",
       "c133d_00002                            [0.1]                          [32]  \n",
       "c133d_00003                            [0.2]                          [32]  \n",
       "c133d_00004                            [0.1]                          [32]  \n",
       "c133d_00005                            [0.1]                          [32]  \n",
       "c133d_00006                            [0.2]                          [32]  \n",
       "c133d_00007                            [0.2]                          [32]  \n",
       "c133d_00008                            [0.2]                          [32]  \n",
       "c133d_00009                            [0.1]                          [32]  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": \"ss\",\n",
    "  \"aggr\": None,\n",
    "  \"lr\": 1e-3,\n",
    "  \"batch_size\": 64,\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, 16],\n",
    "    \"conv_kernels\": [3],\n",
    "    \"pool_kernels\": [2],\n",
    "    \"dropout_rates\": [0.1]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (hp_metric): R2Score()\n",
       "  (convnet): BasicConv1D(\n",
       "    (module): Sequential(\n",
       "      (0): Conv1d(4, 16, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fcn): BasicFullyConnectedModule(\n",
       "    (module): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = eu.models.get_model(config[\"arch\"], config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | hp_metric | R2Score                   | 0     \n",
      "1 | convnet   | BasicConv1D               | 208   \n",
      "2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "--------------------------------------------------------\n",
      "25.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.4 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db39ac295884fc4ab30b08726c7347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6998e9dcf774edd9482d9ee4ee27c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e173f9d1cce04d7cbe68e85f769ec5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902c16a330e4eafaa522a4f20c84309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31248cdf86df4f4ba14871769d63418c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2fb05421e44d89ad1576bbfcb0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d448ea3a99a47ce8d79d8ccb5727589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bf81645394473826ec9f52af0b798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622dcc399beb4d7fae9aafa75f67bb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ef77a592948b798eaa1f4d3dbf7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770edc647520430087e5f84a16a98f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1839eb1ca04a8692da9fb1483bb6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eu.train.fit(\n",
    "    model = model,\n",
    "    sdata = sdata,\n",
    "    target_keys = \"activity_0\", \n",
    "    train_key = \"train_val\",\n",
    "    epochs = 10,\n",
    "    gpus = 0,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loss', 'time_this_iter_s', 'done', 'timesteps_total', 'episodes_total',\n",
       "       'training_iteration', 'experiment_id', 'date', 'timestamp',\n",
       "       'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore',\n",
       "       'timesteps_since_restore', 'iterations_since_restore', 'warmup_time',\n",
       "       'experiment_tag', 'config/arch', 'config/input_len',\n",
       "       'config/output_dim', 'config/strand', 'config/aggr', 'config/lr',\n",
       "       'config/batch_size', 'config/conv_kwargs/channels',\n",
       "       'config/conv_kwargs/conv_kernels', 'config/conv_kwargs/pool_kernels',\n",
       "       'config/conv_kwargs/dropout_rates', 'config/fc_kwargs/hidden_dims'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.results_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(config, train_dataset, val_dataset, epochs=10, gpus=0):\n",
    "  model =  eu.models.FCN(\n",
    "    input_len=100, \n",
    "    output_dim=1, \n",
    "    lr=config[\"lr\"]\n",
    "    )\n",
    "  train_dl = train_dataset.to_dataloader(batch_size=config[\"batch_size\"])\n",
    "  val_dl = val_dataset.to_dataloader(batch_size=config[\"batch_size\"])\n",
    "  trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    gpus=gpus,\n",
    "    progress_bar_refresh_rate=0,\n",
    "    callbacks=[callback])\n",
    "  trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tune.with_parameters(\n",
    "    train_tune,\n",
    "    train_dataset=sdataset_train,\n",
    "    val_dataset=sdataset_val,\n",
    "    epochs=10,\n",
    "    gpus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 03:34:19,999\tERROR experiment_analysis.py:486 -- No checkpoints have been found for trial train_tune_86a07_00000.\n"
     ]
    }
   ],
   "source": [
    "best_trial = analysis.best_trial  # Get best trial\n",
    "best_config = analysis.best_config  # Get best trial's hyperparameters\n",
    "best_logdir = analysis.best_logdir  # Get best trial's logdir\n",
    "best_checkpoint = analysis.best_checkpoint  # Get best trial's best checkpoint\n",
    "best_result = analysis.best_result  # Get best trial's last results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_tune_86a07_00000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.00010000831284081109, 'batch_size': 32}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vscode/ray_results/test/train_tune_86a07_00000_0_batch_size=32,lr=0.0001_2022-10-19_03-31-32'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.11025925725698471,\n",
       " 'time_this_iter_s': 0.11937999725341797,\n",
       " 'done': True,\n",
       " 'timesteps_total': None,\n",
       " 'episodes_total': None,\n",
       " 'training_iteration': 10,\n",
       " 'trial_id': '86a07_00000',\n",
       " 'experiment_id': '299e91b7003d4457b77be9e3b61a2294',\n",
       " 'date': '2022-10-19_03-31-51',\n",
       " 'timestamp': 1666150311,\n",
       " 'time_total_s': 10.0547194480896,\n",
       " 'pid': 23527,\n",
       " 'hostname': 'codespaces-97ce9f',\n",
       " 'node_ip': '172.16.5.4',\n",
       " 'config': {'lr': 0.00010000831284081109, 'batch_size': 32},\n",
       " 'time_since_restore': 10.0547194480896,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 10,\n",
       " 'warmup_time': 0.0036079883575439453,\n",
       " 'experiment_tag': '0_batch_size=32,lr=0.0001'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'Z\\xe4~\\xb8\\x0c\\\\\\x08\\x1b`', b'\\x93\\xa5x\\xa8\\xf9\\x18\\xaf \\xc7\\x85\\x1f\\xd3\\x93x\\x86k9\\xcd\\xfe C\\xf5\\x80\\xd7\\xd2\\x85\\xaeE\\x0e_\\xbeHF\\xe6\\xc2p\\xe0`W\\xab\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00', b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 /\\x934\\x9f\\xd5\\x8eo7\\x8d\\xe5rv\\xbc1z^B\\x17%\\x86\\x00\\x9cn\\x82\\xebF\\xf6*Qy\\t\\x1e']\n",
      "Bad pipe message: %s [b\"q8\\xf8\\xce\\xd9O\\xe02\\xaeh\\x96\\xe3\\ru'\\xa7\\xff\\xba O\\xe0\\xf9\\x88\\xe3\\x04{\\x1em\\xf6\\xe3Zt\\xb5\\x8e\\xa3-\\xd1\\xba=\\xd1`-\\xfdC\\xca\\xde$\\x1a\\x85V\\xc6\\x00\\x08\\x13\\x02\\x13\"]\n",
      "Bad pipe message: %s [b'\\x12\\xae\\xc8\\xf1gV\\r\\x002\\xe63\\x8d\\x9e\\x1b\\x14>O\\xcc\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0']\n",
      "Bad pipe message: %s [b\"$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\"]\n",
      "Bad pipe message: %s [b\"\\xce\\x0e\\xd6\\xb2]1\\xfd\\x82\\xb3\\x8b\\xbc\\x17Mr\\xfc\\x8f\\x80H\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\", b'\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'A=\\xdf\\xba{.\\xce\\xaa\\x8f\\xde\\x19\\x18T\\x19\\xa6\\x9c;\\xae\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t12']\n",
      "Bad pipe message: %s [b'0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b'\\xa4\\t\\xd2\\xfc\\xa5=\\x83\\x0c\\x1dE\\t\\xf5\\xd8\\xf1\\xca\\xadmL\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0']\n",
      "Bad pipe message: %s [b'\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c']\n",
      "Bad pipe message: %s [b'\\xdd\\xd0R\\xa0\\x8b\\x89&y\\x8b\\x1a\\xde\\xf03\\x12\\xa3\\xfa\\x07\\xfb\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007']\n",
      "Bad pipe message: %s [b'g\\x15<\\x7f\\xa9\\xecn\\xff\\x06\\x1d\\x14)\\x95sq\\xc8^\\xc2\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x00']\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02']\n",
      "Bad pipe message: %s [b\"\\x0c\\x93\\x9bEn\\xe04&\\x12H\\x08A\\x8fH;\\xfd\\xc7!\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\", b'\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 ']\n"
     ]
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 eugene_dev",
   "language": "python",
   "name": "eugene_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
