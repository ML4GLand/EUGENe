{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from os import PathLike\n",
    "from typing import Union, List\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler, MedianStoppingRule, PopulationBasedTraining\n",
    "from ray.tune.search import BasicVariantGenerator\n",
    "from ray.tune.search.bayesopt import BayesOptSearch \n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_with_tune(\n",
    "    config: dict,\n",
    "    sdata = None,\n",
    "    target_keys: Union[str, List[str]] = None,\n",
    "    train_key: str = \"train_val\",\n",
    "    epochs: int = 10,\n",
    "    gpus: int = None,\n",
    "    num_workers: int = None,\n",
    "    log_dir: PathLike = None,\n",
    "    name: str = None,\n",
    "    version: str = None,\n",
    "    train_dataset: eu.dl.SeqDataset = None,\n",
    "    val_dataset: eu.dl.SeqDataset = None,\n",
    "    train_dataloader: DataLoader = None,\n",
    "    val_dataloader: DataLoader = None,\n",
    "    seq_transforms: List[str] = None,\n",
    "    transform_kwargs: dict = {},\n",
    "    seed: int = None,\n",
    "    verbosity = None,\n",
    "):\n",
    "    model = eu.models.get_model(config[\"arch\"], config)\n",
    "    gpus = gpus if gpus is not None else eu.settings.gpus\n",
    "    num_workers = num_workers if num_workers is not None else eu.settings.dl_num_workers\n",
    "    log_dir = log_dir if log_dir is not None else eu.settings.logging_dir\n",
    "    name = name if name is not None else config[\"arch\"]\n",
    "    seed_everything(seed, workers=True) if seed is not None else seed_everything(eu.settings.seed)\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    if train_dataloader is not None:\n",
    "        assert val_dataloader is not None\n",
    "    elif train_dataset is not None:\n",
    "        assert val_dataset is not None\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "    elif sdata is not None:\n",
    "        assert target_keys is not None\n",
    "        targs = sdata.seqs_annot[target_keys].values  \n",
    "        if len(targs.shape) == 1:\n",
    "            nan_mask = np.isnan(targs)\n",
    "        else:\n",
    "            nan_mask = np.any(np.isnan(targs), axis=1)\n",
    "        print(f\"Dropping {nan_mask.sum()} sequences with NaN targets.\")\n",
    "        sdata = sdata[~nan_mask]\n",
    "        train_idx = np.where(sdata.seqs_annot[train_key] == True)[0]\n",
    "        train_dataset = sdata[train_idx].to_dataset(\n",
    "            target_keys=target_keys,\n",
    "            seq_transforms=seq_transforms,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "        )\n",
    "        train_dataloader = train_dataset.to_dataloader(\n",
    "            batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    "        )\n",
    "        val_idx = np.where(sdata.seqs_annot[train_key] == False)[0]\n",
    "        val_dataset = sdata[val_idx].to_dataset(\n",
    "            target_keys=target_keys,\n",
    "            seq_transforms=seq_transforms,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "        )\n",
    "        val_dataloader = val_dataset.to_dataloader(\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No data provided to train on.\")\n",
    "    logger = TensorBoardLogger(log_dir, name=name, version=version)\n",
    "    callbacks = []\n",
    "    metrics = {\"loss\": \"val_loss\"}\n",
    "    callbacks.append(TuneReportCallback(metrics, on=\"validation_end\"))\n",
    "    trainer = Trainer(\n",
    "        max_epochs=epochs,\n",
    "        gpus=gpus,\n",
    "        logger=logger,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_dict = {\n",
    "    \"ASHAScheduler\": ASHAScheduler,\n",
    "    \"MedianStoppingRule\": MedianStoppingRule,\n",
    "    \"PopulationBasedTraining\": PopulationBasedTraining,\n",
    "    \n",
    "}\n",
    "default_scheduler_args = {\n",
    "    \"ASHAScheduler\": {\"max_t\": 10},\n",
    "    \"MedianStoppingRule\": {},\n",
    "    \"PopulationBasedTraining\": {},\n",
    "}\n",
    "algo_dict = {\n",
    "    \"BayesOptSearch\": BayesOptSearch,\n",
    "    \"HyperOptSearch\": HyperOptSearch,\n",
    "    \"BasicVariantGenerator\": BasicVariantGenerator,\n",
    "}\n",
    "default_algo_args = {\n",
    "    \"BayesOptSearch\": {},\n",
    "    \"HyperOptSearch\": {},\n",
    "    \"BasicVariantGenerator\": {},\n",
    "}\n",
    "\n",
    "def hyperopt(\n",
    "    config,\n",
    "    scheduler=\"ASHAScheduler\",\n",
    "    algorithm=\"BasicVariantGenerator\",\n",
    "    sdata = None,\n",
    "    target_keys: Union[str, List[str]] = None,\n",
    "    train_key: str = \"train_val\",\n",
    "    epochs: int = 10,\n",
    "    gpus: int = None,\n",
    "    num_workers: int = None,\n",
    "    log_dir: PathLike = None,\n",
    "    name: str = None,\n",
    "    version: str = None,\n",
    "    train_dataset: eu.dl.SeqDataset = None,\n",
    "    val_dataset: eu.dl.SeqDataset = None,\n",
    "    train_dataloader: DataLoader = None,\n",
    "    val_dataloader: DataLoader = None,\n",
    "    seq_transforms: List[str] = None,\n",
    "    transform_kwargs: dict = {},\n",
    "    seed: int = None,\n",
    "    verbosity = None,\n",
    "    scheduler_kwargs: dict = None,\n",
    "    algorithm_kwargs: dict = None,\n",
    "    **kwargs\n",
    "):\n",
    "    trainable = tune.with_parameters(\n",
    "        hyperopt_with_tune,\n",
    "        sdata=sdata,\n",
    "        target_keys=target_keys,\n",
    "        train_key=train_key,\n",
    "        epochs=epochs,\n",
    "        gpus=gpus,\n",
    "        num_workers=num_workers,\n",
    "        log_dir=log_dir,\n",
    "        name=name,\n",
    "        version=version,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        seq_transforms=seq_transforms,\n",
    "        transform_kwargs=transform_kwargs,\n",
    "        seed=seed,\n",
    "        verbosity=verbosity,\n",
    "        **kwargs\n",
    "    )\n",
    "    if scheduler_kwargs is None or len(scheduler_kwargs) == 0:\n",
    "        scheduler_kwargs = default_scheduler_args[scheduler]\n",
    "    scheduler = scheduler_dict[scheduler](**scheduler_kwargs)\n",
    "    if algorithm_kwargs is None or len(algorithm_kwargs) == 0:\n",
    "        algorithm_kwargs = default_algo_args[algorithm]\n",
    "    algo = algo_dict[algorithm](**algorithm_kwargs)\n",
    "    analysis = tune.run(\n",
    "        trainable,\n",
    "        config=config,\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=10,\n",
    "        local_dir=eu.settings.logging_dir,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"min-val_loss\",\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10a003c1b804c37bd33ea6fb8de91d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "\tohe_rev_seqs: None -> 1000 ohe_rev_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.reverse_complement_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_kwargs = dict(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "algo_kwargs = dict(\n",
    "  metric=\"loss\",\n",
    "  mode=\"min\",\n",
    ")\n",
    "tune_config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": tune.choice([\"ss\", \"ds\", \"ts\"]),\n",
    "  \"aggr\": tune.choice([\"max\", \"avg\"]),\n",
    "  \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "  \"batch_size\": tune.choice([32, 64, 128]),\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, tune.choice([16, 32])],\n",
    "    \"conv_kernels\": [tune.choice([3, 5])],\n",
    "    \"pool_kernels\": [tune.choice([2, 4])],\n",
    "    \"dropout_rates\": [tune.choice([0.1, 0.2])]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-02 19:48:50 (running for 00:00:48.59)<br>Memory usage on this node: 4.9/7.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.41 GiB heap, 0.0/1.71 GiB objects<br>Current best trial: 427ba_00004 with loss=0.08033579587936401 and parameters={'arch': 'CNN', 'input_len': 100, 'output_dim': 1, 'strand': 'ts', 'aggr': 'avg', 'lr': 0.0041561055695126005, 'batch_size': 128, 'conv_kwargs': {'channels': [4, 16], 'conv_kernels': [5], 'pool_kernels': [4], 'dropout_rates': [0.1]}, 'fc_kwargs': {'hidden_dims': [32]}}<br>Result logdir: /workspaces/EUGENe/tests/notebooks/implement/eugene_logs/hyperopt_with_tune_2022-11-02_19-48-01<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc             </th><th>aggr  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_kwargs/chann...</th><th style=\"text-align: right;\">  conv_kwargs/conv_...</th><th style=\"text-align: right;\">  conv_kwargs/dropo...</th><th style=\"text-align: right;\">  conv_kwargs/pool_...</th><th style=\"text-align: right;\">        lr</th><th>strand  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyperopt_with_tune_427ba_00000</td><td>TERMINATED</td><td>172.16.5.4:19936</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0373778 </td><td>ds      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        32.3856 </td><td style=\"text-align: right;\">0.184033 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00001</td><td>TERMINATED</td><td>172.16.5.4:20098</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0093288 </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        21.1177 </td><td style=\"text-align: right;\">0.168963 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00002</td><td>TERMINATED</td><td>172.16.5.4:20187</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0409189 </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        11.5729 </td><td style=\"text-align: right;\">0.163224 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00003</td><td>TERMINATED</td><td>172.16.5.4:20300</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0405104 </td><td>ts      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.49163</td><td style=\"text-align: right;\">0.24677  </td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00004</td><td>TERMINATED</td><td>172.16.5.4:20098</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00415611</td><td>ts      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.02991</td><td style=\"text-align: right;\">0.0803358</td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00005</td><td>TERMINATED</td><td>172.16.5.4:20187</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0211388 </td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.01562</td><td style=\"text-align: right;\">0.0889913</td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00006</td><td>TERMINATED</td><td>172.16.5.4:20300</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.00193576</td><td>ts      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.47871</td><td style=\"text-align: right;\">0.0820928</td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00007</td><td>TERMINATED</td><td>172.16.5.4:19936</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.00496551</td><td>ss      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2.3569 </td><td style=\"text-align: right;\">0.0821506</td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00008</td><td>TERMINATED</td><td>172.16.5.4:20098</td><td>avg   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00177454</td><td>ds      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.05448</td><td style=\"text-align: right;\">0.0817517</td></tr>\n",
       "<tr><td>hyperopt_with_tune_427ba_00009</td><td>TERMINATED</td><td>172.16.5.4:20187</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                    16</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0532413 </td><td>ts      </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1.62191</td><td style=\"text-align: right;\">0.404299 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19936)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 1 | convnet   | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f9e7d023c10>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20098)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 25.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 25.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f92741f0990>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20187)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 25.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 25.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fad4c4efa90>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20300)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00001:\n",
      "  date: 2022-11-02_19-48-24\n",
      "  done: false\n",
      "  experiment_id: f68bc3dda16e47ebb0b70b971c496fd8\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.2553430199623108\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20098\n",
      "  time_since_restore: 0.19302988052368164\n",
      "  time_this_iter_s: 0.19302988052368164\n",
      "  time_total_s: 0.19302988052368164\n",
      "  timestamp: 1667418504\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00001\n",
      "  warmup_time: 0.004018068313598633\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00002:\n",
      "  date: 2022-11-02_19-48-33\n",
      "  done: false\n",
      "  experiment_id: a2696495f3114a64b8ccb2f321e66e82\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.35328397154808044\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20187\n",
      "  time_since_restore: 0.2319190502166748\n",
      "  time_this_iter_s: 0.2319190502166748\n",
      "  time_total_s: 0.2319190502166748\n",
      "  timestamp: 1667418513\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00002\n",
      "  warmup_time: 0.003734588623046875\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00000:\n",
      "  date: 2022-11-02_19-48-14\n",
      "  done: false\n",
      "  experiment_id: bb0742ae9bc94bcd93f428b12e214986\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.5722008943557739\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 19936\n",
      "  time_since_restore: 0.34542083740234375\n",
      "  time_this_iter_s: 0.34542083740234375\n",
      "  time_total_s: 0.34542083740234375\n",
      "  timestamp: 1667418494\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00000\n",
      "  warmup_time: 0.0038030147552490234\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f96ab678ed0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 1 | convnet         | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 49.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 49.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 99.8 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 99.8 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 0.399     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00003:\n",
      "  date: 2022-11-02_19-48-44\n",
      "  done: false\n",
      "  experiment_id: 7ba77036dec240a49a8975082bbc40d3\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.3331250250339508\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20300\n",
      "  time_since_restore: 0.6545829772949219\n",
      "  time_this_iter_s: 0.6545829772949219\n",
      "  time_total_s: 0.6545829772949219\n",
      "  timestamp: 1667418524\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00003\n",
      "  warmup_time: 0.003753662109375\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00001:\n",
      "  date: 2022-11-02_19-48-44\n",
      "  done: true\n",
      "  experiment_id: f68bc3dda16e47ebb0b70b971c496fd8\n",
      "  experiment_tag: 1_aggr=avg,batch_size=128,1=16,0=5,0=0.2000,0=2,lr=0.0093,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.1689630150794983\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20098\n",
      "  time_since_restore: 21.117652893066406\n",
      "  time_this_iter_s: 0.17719626426696777\n",
      "  time_total_s: 21.117652893066406\n",
      "  timestamp: 1667418524\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00001\n",
      "  warmup_time: 0.004018068313598633\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f9274289a90>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0.102     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00002:\n",
      "  date: 2022-11-02_19-48-45\n",
      "  done: true\n",
      "  experiment_id: a2696495f3114a64b8ccb2f321e66e82\n",
      "  experiment_tag: 2_aggr=avg,batch_size=64,1=16,0=5,0=0.1000,0=2,lr=0.0409,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.16322383284568787\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20187\n",
      "  time_since_restore: 11.572925329208374\n",
      "  time_this_iter_s: 0.2880833148956299\n",
      "  time_total_s: 11.572925329208374\n",
      "  timestamp: 1667418525\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00002\n",
      "  warmup_time: 0.003734588623046875\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00004:\n",
      "  date: 2022-11-02_19-48-45\n",
      "  done: false\n",
      "  experiment_id: f68bc3dda16e47ebb0b70b971c496fd8\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08031076192855835\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20098\n",
      "  time_since_restore: 0.5161828994750977\n",
      "  time_this_iter_s: 0.5161828994750977\n",
      "  time_total_s: 0.5161828994750977\n",
      "  timestamp: 1667418525\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00004\n",
      "  warmup_time: 0.004018068313598633\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 1 | convnet   | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 25.3 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 25.3 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0.101     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fac202b5550>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00005:\n",
      "  date: 2022-11-02_19-48-46\n",
      "  done: false\n",
      "  experiment_id: a2696495f3114a64b8ccb2f321e66e82\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.24731788039207458\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20187\n",
      "  time_since_restore: 0.5986545085906982\n",
      "  time_this_iter_s: 0.5986545085906982\n",
      "  time_total_s: 0.5986545085906982\n",
      "  timestamp: 1667418526\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00005\n",
      "  warmup_time: 0.003734588623046875\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00003:\n",
      "  date: 2022-11-02_19-48-46\n",
      "  done: true\n",
      "  experiment_id: 7ba77036dec240a49a8975082bbc40d3\n",
      "  experiment_tag: 3_aggr=max,batch_size=64,1=32,0=5,0=0.2000,0=2,lr=0.0405,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.2467697113752365\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20300\n",
      "  time_since_restore: 2.491631507873535\n",
      "  time_this_iter_s: 0.40781545639038086\n",
      "  time_total_s: 2.491631507873535\n",
      "  timestamp: 1667418526\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00003\n",
      "  warmup_time: 0.003753662109375\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f96aa08a450>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 50.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 50.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m 0.200     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20300)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00000:\n",
      "  date: 2022-11-02_19-48-46\n",
      "  done: true\n",
      "  experiment_id: bb0742ae9bc94bcd93f428b12e214986\n",
      "  experiment_tag: 0_aggr=max,batch_size=32,1=32,0=3,0=0.1000,0=2,lr=0.0374,strand=ds\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.18403266370296478\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 19936\n",
      "  time_since_restore: 32.3855984210968\n",
      "  time_this_iter_s: 0.4744994640350342\n",
      "  time_total_s: 32.3855984210968\n",
      "  timestamp: 1667418526\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00000\n",
      "  warmup_time: 0.0038030147552490234\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00004:\n",
      "  date: 2022-11-02_19-48-47\n",
      "  done: true\n",
      "  experiment_id: f68bc3dda16e47ebb0b70b971c496fd8\n",
      "  experiment_tag: 4_aggr=avg,batch_size=128,1=16,0=5,0=0.1000,0=4,lr=0.0042,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08033579587936401\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20098\n",
      "  time_since_restore: 2.0299134254455566\n",
      "  time_this_iter_s: 0.342104434967041\n",
      "  time_total_s: 2.0299134254455566\n",
      "  timestamp: 1667418527\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00004\n",
      "  warmup_time: 0.004018068313598633\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f9e7cf35b50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 1 | convnet   | BasicConv1D               | 416   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 50.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=19936)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00006:\n",
      "  date: 2022-11-02_19-48-47\n",
      "  done: false\n",
      "  experiment_id: 7ba77036dec240a49a8975082bbc40d3\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08191581070423126\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20300\n",
      "  time_since_restore: 0.4550936222076416\n",
      "  time_this_iter_s: 0.4550936222076416\n",
      "  time_total_s: 0.4550936222076416\n",
      "  timestamp: 1667418527\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00006\n",
      "  warmup_time: 0.003753662109375\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f92742a3810>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 1 | convnet   | BasicConv1D               | 672   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 25.3 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 25.3 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m 0.101     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20098)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00005:\n",
      "  date: 2022-11-02_19-48-47\n",
      "  done: true\n",
      "  experiment_id: a2696495f3114a64b8ccb2f321e66e82\n",
      "  experiment_tag: 5_aggr=max,batch_size=64,1=32,0=5,0=0.2000,0=4,lr=0.0211,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08899134397506714\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20187\n",
      "  time_since_restore: 2.015625\n",
      "  time_this_iter_s: 0.3429703712463379\n",
      "  time_total_s: 2.015625\n",
      "  timestamp: 1667418527\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00005\n",
      "  warmup_time: 0.003734588623046875\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00007:\n",
      "  date: 2022-11-02_19-48-47\n",
      "  done: false\n",
      "  experiment_id: bb0742ae9bc94bcd93f428b12e214986\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.09257969260215759\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 19936\n",
      "  time_since_restore: 0.6348183155059814\n",
      "  time_this_iter_s: 0.6348183155059814\n",
      "  time_total_s: 0.6348183155059814\n",
      "  timestamp: 1667418527\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00007\n",
      "  warmup_time: 0.0038030147552490234\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fac1a42a610>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=20187)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_427ba_00009:\n",
      "  date: 2022-11-02_19-48-48\n",
      "  done: false\n",
      "  experiment_id: a2696495f3114a64b8ccb2f321e66e82\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.6045937538146973\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20187\n",
      "  time_since_restore: 0.5278398990631104\n",
      "  time_this_iter_s: 0.5278398990631104\n",
      "  time_total_s: 0.5278398990631104\n",
      "  timestamp: 1667418528\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00009\n",
      "  warmup_time: 0.003734588623046875\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00008:\n",
      "  date: 2022-11-02_19-48-48\n",
      "  done: false\n",
      "  experiment_id: f68bc3dda16e47ebb0b70b971c496fd8\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08401686698198318\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20098\n",
      "  time_since_restore: 1.2598989009857178\n",
      "  time_this_iter_s: 1.2598989009857178\n",
      "  time_total_s: 1.2598989009857178\n",
      "  timestamp: 1667418528\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 427ba_00008\n",
      "  warmup_time: 0.004018068313598633\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00009:\n",
      "  date: 2022-11-02_19-48-49\n",
      "  done: true\n",
      "  experiment_id: a2696495f3114a64b8ccb2f321e66e82\n",
      "  experiment_tag: 9_aggr=avg,batch_size=128,1=16,0=3,0=0.2000,0=2,lr=0.0532,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.4042985439300537\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20187\n",
      "  time_since_restore: 1.6219110488891602\n",
      "  time_this_iter_s: 0.328702449798584\n",
      "  time_total_s: 1.6219110488891602\n",
      "  timestamp: 1667418529\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00009\n",
      "  warmup_time: 0.003734588623046875\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00006:\n",
      "  date: 2022-11-02_19-48-49\n",
      "  done: true\n",
      "  experiment_id: 7ba77036dec240a49a8975082bbc40d3\n",
      "  experiment_tag: 6_aggr=max,batch_size=64,1=16,0=5,0=0.2000,0=2,lr=0.0019,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08209282904863358\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20300\n",
      "  time_since_restore: 2.4787120819091797\n",
      "  time_this_iter_s: 0.36729955673217773\n",
      "  time_total_s: 2.4787120819091797\n",
      "  timestamp: 1667418529\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00006\n",
      "  warmup_time: 0.003753662109375\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00007:\n",
      "  date: 2022-11-02_19-48-49\n",
      "  done: true\n",
      "  experiment_id: bb0742ae9bc94bcd93f428b12e214986\n",
      "  experiment_tag: 7_aggr=max,batch_size=64,1=32,0=3,0=0.1000,0=2,lr=0.0050,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.08215063065290451\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 19936\n",
      "  time_since_restore: 2.356898546218872\n",
      "  time_this_iter_s: 0.42998719215393066\n",
      "  time_total_s: 2.356898546218872\n",
      "  timestamp: 1667418529\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00007\n",
      "  warmup_time: 0.0038030147552490234\n",
      "  \n",
      "Result for hyperopt_with_tune_427ba_00008:\n",
      "  date: 2022-11-02_19-48-50\n",
      "  done: true\n",
      "  experiment_id: f68bc3dda16e47ebb0b70b971c496fd8\n",
      "  experiment_tag: 8_aggr=avg,batch_size=32,1=32,0=5,0=0.2000,0=4,lr=0.0018,strand=ds\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.081751748919487\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 20098\n",
      "  time_since_restore: 3.0544772148132324\n",
      "  time_this_iter_s: 0.575023889541626\n",
      "  time_total_s: 3.0544772148132324\n",
      "  timestamp: 1667418530\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 427ba_00008\n",
      "  warmup_time: 0.004018068313598633\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 19:48:50,609\tINFO tune.py:759 -- Total run time: 48.85 seconds (48.52 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "res = hyperopt(\n",
    "    tune_config,\n",
    "    sdata=sdata,\n",
    "    algorithm=\"BayesOptSearch\",\n",
    "    target_keys=\"activity_0\",\n",
    "    train_key=\"train_val\",\n",
    "    epochs=5,\n",
    "    gpus=0,\n",
    "    num_workers=0,\n",
    "    algorithm_kwargs=algo_kwargs,\n",
    "    scheduler_kwargs=scheduler_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': 'CNN',\n",
       " 'input_len': 100,\n",
       " 'output_dim': 1,\n",
       " 'strand': 'ds',\n",
       " 'aggr': 'avg',\n",
       " 'lr': 0.00044313707981392264,\n",
       " 'batch_size': 32,\n",
       " 'conv_kwargs': {'channels': [4, 16],\n",
       "  'conv_kernels': [5],\n",
       "  'pool_kernels': [4],\n",
       "  'dropout_rates': [0.1]},\n",
       " 'fc_kwargs': {'hidden_dims': [32]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tune.with_parameters(\n",
    "    hyperopt_with_tune,\n",
    "    sdata=sdata,\n",
    "    target_keys=\"activity_0\",\n",
    "    train_key=\"train_val\",\n",
    "    epochs=10,\n",
    "    gpus=0,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": tune.choice([\"ss\", \"ds\", \"ts\"]),\n",
    "  \"aggr\": tune.choice([\"max\", \"avg\"]),\n",
    "  \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "  \"batch_size\": tune.choice([32, 64, 128]),\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, 16],\n",
    "    \"conv_kernels\": [tune.choice([3, 5])],\n",
    "    \"pool_kernels\": [tune.choice([2, 4])],\n",
    "    \"dropout_rates\": [tune.choice([0.1, 0.2])]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:14:11,580\tWARNING services.py:1893 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.97gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-11-01 20:14:12,966\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "/home/vscode/.local/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py:644: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/ray-air/key-concepts.html#session\n",
      "\n",
      "  DeprecationWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-01 20:14:57 (running for 00:00:43.66)<br>Memory usage on this node: 4.7/7.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.59 GiB heap, 0.0/1.79 GiB objects<br>Current best trial: c133d_00009 with loss=0.07941604405641556 and parameters={'arch': 'CNN', 'input_len': 100, 'output_dim': 1, 'strand': 'ts', 'aggr': 'avg', 'lr': 0.0006544351060732511, 'batch_size': 128, 'conv_kwargs': {'channels': [4, 16], 'conv_kernels': [5], 'pool_kernels': [4], 'dropout_rates': [0.1]}, 'fc_kwargs': {'hidden_dims': [32]}}<br>Result logdir: /workspaces/EUGENe/tests/notebooks/implement/eugene_logs/test<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th>aggr  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_kwargs/conv_...</th><th style=\"text-align: right;\">  conv_kwargs/dropo...</th><th style=\"text-align: right;\">  conv_kwargs/pool_...</th><th style=\"text-align: right;\">         lr</th><th>strand  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyperopt_with_tune_c133d_00000</td><td>TERMINATED</td><td>172.16.5.4:6119</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.000515969</td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        27.7774 </td><td style=\"text-align: right;\">0.0812072</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00001</td><td>TERMINATED</td><td>172.16.5.4:6237</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0022927  </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        19.5342 </td><td style=\"text-align: right;\">0.0842177</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00002</td><td>TERMINATED</td><td>172.16.5.4:6354</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0146573  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        10.6593 </td><td style=\"text-align: right;\">0.0802305</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00003</td><td>TERMINATED</td><td>172.16.5.4:6459</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0372549  </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.73107</td><td style=\"text-align: right;\">0.0891374</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00004</td><td>TERMINATED</td><td>172.16.5.4:6459</td><td>max   </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0954421  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.44144</td><td style=\"text-align: right;\">0.0802142</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00005</td><td>TERMINATED</td><td>172.16.5.4:6119</td><td>max   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0656628  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         3.69808</td><td style=\"text-align: right;\">0.0802464</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00006</td><td>TERMINATED</td><td>172.16.5.4:6354</td><td>avg   </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.00775331 </td><td>ss      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         3.4115 </td><td style=\"text-align: right;\">0.085105 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00007</td><td>TERMINATED</td><td>172.16.5.4:6237</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000389489</td><td>ds      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.96298</td><td style=\"text-align: right;\">0.082369 </td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00008</td><td>TERMINATED</td><td>172.16.5.4:6354</td><td>max   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                   0.2</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">0.0735737  </td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.9596 </td><td style=\"text-align: right;\">0.0837441</td></tr>\n",
       "<tr><td>hyperopt_with_tune_c133d_00009</td><td>TERMINATED</td><td>172.16.5.4:6119</td><td>avg   </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                   0.1</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.000654435</td><td>ts      </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1.9034 </td><td style=\"text-align: right;\">0.079416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3984575a10>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\u001b[2m\u001b[36m(pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 1 | convnet   | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0.101     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f81bace6f90>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 24.6 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.0 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0.200     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3741e5b810>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6459)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00001:\n",
      "  date: 2022-11-01_20-14-33\n",
      "  done: false\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08753872662782669\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 0.20910406112670898\n",
      "  time_this_iter_s: 0.20910406112670898\n",
      "  time_total_s: 0.20910406112670898\n",
      "  timestamp: 1667333673\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00001\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00000:\n",
      "  date: 2022-11-01_20-14-24\n",
      "  done: false\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.1006212830543518\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 0.761347770690918\n",
      "  time_this_iter_s: 0.761347770690918\n",
      "  time_total_s: 0.761347770690918\n",
      "  timestamp: 1667333664\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00000\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00002:\n",
      "  date: 2022-11-01_20-14-41\n",
      "  done: false\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.3482355773448944\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 0.2775743007659912\n",
      "  time_this_iter_s: 0.2775743007659912\n",
      "  time_total_s: 0.2775743007659912\n",
      "  timestamp: 1667333681\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00002\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fae4c3cdf10>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00003:\n",
      "  date: 2022-11-01_20-14-50\n",
      "  done: false\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.49401620030403137\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 0.2556467056274414\n",
      "  time_this_iter_s: 0.2556467056274414\n",
      "  time_total_s: 0.2556467056274414\n",
      "  timestamp: 1667333690\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00003\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00003:\n",
      "  date: 2022-11-01_20-14-51\n",
      "  done: true\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  experiment_tag: 3_aggr=avg,batch_size=128,0=5,0=0.2000,0=4,lr=0.0373,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08913742005825043\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 1.7310707569122314\n",
      "  time_this_iter_s: 0.11130189895629883\n",
      "  time_total_s: 1.7310707569122314\n",
      "  timestamp: 1667333691\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00003\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7fae46dbe290>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6459)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00000:\n",
      "  date: 2022-11-01_20-14-51\n",
      "  done: true\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  experiment_tag: 0_aggr=max,batch_size=128,0=3,0=0.2000,0=2,lr=0.0005,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08120723813772202\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 27.777360677719116\n",
      "  time_this_iter_s: 0.3084733486175537\n",
      "  time_total_s: 27.777360677719116\n",
      "  timestamp: 1667333691\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00000\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f397e7beed0>]\n",
      "Result for hyperopt_with_tune_c133d_00002:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: true\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  experiment_tag: 2_aggr=avg,batch_size=64,0=5,0=0.1000,0=2,lr=0.0147,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08023054152727127\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 10.659316539764404\n",
      "  time_this_iter_s: 0.34508705139160156\n",
      "  time_total_s: 10.659316539764404\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00002\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.1 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0.100     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00004:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: false\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.10064372420310974\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 0.5659947395324707\n",
      "  time_this_iter_s: 0.5659947395324707\n",
      "  time_total_s: 0.5659947395324707\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00004\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3741f59250>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00005:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: false\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.49822548031806946\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 0.40572214126586914\n",
      "  time_this_iter_s: 0.40572214126586914\n",
      "  time_total_s: 0.40572214126586914\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00005\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00006:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: false\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.08371169865131378\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 0.2671666145324707\n",
      "  time_this_iter_s: 0.2671666145324707\n",
      "  time_total_s: 0.2671666145324707\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00006\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00001:\n",
      "  date: 2022-11-01_20-14-52\n",
      "  done: true\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  experiment_tag: 1_aggr=max,batch_size=32,0=3,0=0.1000,0=2,lr=0.0023,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08421774953603745\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 19.534242153167725\n",
      "  time_this_iter_s: 0.34664058685302734\n",
      "  time_total_s: 19.534242153167725\n",
      "  timestamp: 1667333692\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00001\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f81bad89390>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   | Name      | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0 | hp_metric | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 1 | convnet   | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 2 | fcn       | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 12.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 12.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m 0.051     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6237)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00007:\n",
      "  date: 2022-11-01_20-14-53\n",
      "  done: false\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.12046148627996445\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 0.38976120948791504\n",
      "  time_this_iter_s: 0.38976120948791504\n",
      "  time_total_s: 0.38976120948791504\n",
      "  timestamp: 1667333693\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00007\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00006:\n",
      "  date: 2022-11-01_20-14-55\n",
      "  done: true\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  experiment_tag: 6_aggr=avg,batch_size=64,0=5,0=0.2000,0=4,lr=0.0078,strand=ss\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08510497957468033\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 3.4114990234375\n",
      "  time_this_iter_s: 0.24724721908569336\n",
      "  time_total_s: 3.4114990234375\n",
      "  timestamp: 1667333695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00006\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00005:\n",
      "  date: 2022-11-01_20-14-55\n",
      "  done: true\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  experiment_tag: 5_aggr=max,batch_size=64,0=3,0=0.1000,0=4,lr=0.0657,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.0802464485168457\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 3.6980841159820557\n",
      "  time_this_iter_s: 0.38837718963623047\n",
      "  time_total_s: 3.6980841159820557\n",
      "  timestamp: 1667333695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00005\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f3741f971d0>]\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Dropping 0 sequences with NaN targets.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m No transforms given, assuming just need to tensorize.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m [<ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x7f39846b4fd0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 1 | convnet         | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 336   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 12.4 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.4 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 25.4 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m 0.102     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   | Name            | Type                      | Params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0 | hp_metric       | R2Score                   | 0     \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 1 | convnet         | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 2 | reverse_convnet | BasicConv1D               | 208   \n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 3 | fcn             | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 4 | reverse_fcn     | BasicFullyConnectedModule | 25.2 K\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m --------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 50.7 K    Total params\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m 0.203     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m Global seed set to 13\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m /home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6354)\u001b[0m   f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00007:\n",
      "  date: 2022-11-01_20-14-55\n",
      "  done: true\n",
      "  experiment_id: 5cc423fa139b475bb55725e4c27d3c03\n",
      "  experiment_tag: 7_aggr=max,batch_size=128,0=5,0=0.2000,0=4,lr=0.0004,strand=ds\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08236898481845856\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6237\n",
      "  time_since_restore: 2.9629783630371094\n",
      "  time_this_iter_s: 0.21939635276794434\n",
      "  time_total_s: 2.9629783630371094\n",
      "  timestamp: 1667333695\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00007\n",
      "  warmup_time: 0.0030231475830078125\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00009:\n",
      "  date: 2022-11-01_20-14-56\n",
      "  done: false\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.10714306682348251\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 0.2818934917449951\n",
      "  time_this_iter_s: 0.2818934917449951\n",
      "  time_total_s: 0.2818934917449951\n",
      "  timestamp: 1667333696\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00009\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00008:\n",
      "  date: 2022-11-01_20-14-56\n",
      "  done: false\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.41163933277130127\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 0.378922700881958\n",
      "  time_this_iter_s: 0.378922700881958\n",
      "  time_total_s: 0.378922700881958\n",
      "  timestamp: 1667333696\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: c133d_00008\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(hyperopt_with_tune pid=6119)\u001b[0m Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for hyperopt_with_tune_c133d_00004:\n",
      "  date: 2022-11-01_20-14-57\n",
      "  done: true\n",
      "  experiment_id: 0e2662855e564568801e80eac2f05e4a\n",
      "  experiment_tag: 4_aggr=max,batch_size=32,0=3,0=0.1000,0=2,lr=0.0954,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08021415770053864\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6459\n",
      "  time_since_restore: 5.441444158554077\n",
      "  time_this_iter_s: 0.3250539302825928\n",
      "  time_total_s: 5.441444158554077\n",
      "  timestamp: 1667333697\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00004\n",
      "  warmup_time: 0.0030121803283691406\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00009:\n",
      "  date: 2022-11-01_20-14-57\n",
      "  done: true\n",
      "  experiment_id: a96250e996b1433ebcf48bb5b6a3d41b\n",
      "  experiment_tag: 9_aggr=avg,batch_size=128,0=5,0=0.1000,0=4,lr=0.0007,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.07941604405641556\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6119\n",
      "  time_since_restore: 1.903404712677002\n",
      "  time_this_iter_s: 0.1506028175354004\n",
      "  time_total_s: 1.903404712677002\n",
      "  timestamp: 1667333697\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00009\n",
      "  warmup_time: 0.002969980239868164\n",
      "  \n",
      "Result for hyperopt_with_tune_c133d_00008:\n",
      "  date: 2022-11-01_20-14-57\n",
      "  done: true\n",
      "  experiment_id: 2033aa1d5f7047aaabba2e6f654f3661\n",
      "  experiment_tag: 8_aggr=max,batch_size=128,0=3,0=0.2000,0=2,lr=0.0736,strand=ts\n",
      "  hostname: codespaces-97ce9f\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.08374407887458801\n",
      "  node_ip: 172.16.5.4\n",
      "  pid: 6354\n",
      "  time_since_restore: 1.959604263305664\n",
      "  time_this_iter_s: 0.11938953399658203\n",
      "  time_total_s: 1.959604263305664\n",
      "  timestamp: 1667333697\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: c133d_00008\n",
      "  warmup_time: 0.0029430389404296875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 20:14:57,962\tINFO tune.py:759 -- Total run time: 43.97 seconds (43.63 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    config=tune_config,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=10,\n",
    "    local_dir=eu.settings.logging_dir,\n",
    "    keep_checkpoints_num=1,\n",
    "    checkpoint_score_attr=\"min-val_loss\",\n",
    "    name=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>config/output_dim</th>\n",
       "      <th>config/strand</th>\n",
       "      <th>config/aggr</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/batch_size</th>\n",
       "      <th>config/conv_kwargs/channels</th>\n",
       "      <th>config/conv_kwargs/conv_kernels</th>\n",
       "      <th>config/conv_kwargs/pool_kernels</th>\n",
       "      <th>config/conv_kwargs/dropout_rates</th>\n",
       "      <th>config/fc_kwargs/hidden_dims</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c133d_00000</th>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.308473</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>a96250e996b1433ebcf48bb5b6a3d41b</td>\n",
       "      <td>2022-11-01_20-14-51</td>\n",
       "      <td>1667333691</td>\n",
       "      <td>27.777361</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00001</th>\n",
       "      <td>0.084218</td>\n",
       "      <td>0.346641</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>5cc423fa139b475bb55725e4c27d3c03</td>\n",
       "      <td>2022-11-01_20-14-52</td>\n",
       "      <td>1667333692</td>\n",
       "      <td>19.534242</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>max</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>32</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00002</th>\n",
       "      <td>0.080231</td>\n",
       "      <td>0.345087</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2033aa1d5f7047aaabba2e6f654f3661</td>\n",
       "      <td>2022-11-01_20-14-52</td>\n",
       "      <td>1667333692</td>\n",
       "      <td>10.659317</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00003</th>\n",
       "      <td>0.089137</td>\n",
       "      <td>0.111302</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0e2662855e564568801e80eac2f05e4a</td>\n",
       "      <td>2022-11-01_20-14-51</td>\n",
       "      <td>1667333691</td>\n",
       "      <td>1.731071</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00004</th>\n",
       "      <td>0.080214</td>\n",
       "      <td>0.325054</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0e2662855e564568801e80eac2f05e4a</td>\n",
       "      <td>2022-11-01_20-14-57</td>\n",
       "      <td>1667333697</td>\n",
       "      <td>5.441444</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.095442</td>\n",
       "      <td>32</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00005</th>\n",
       "      <td>0.080246</td>\n",
       "      <td>0.388377</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>a96250e996b1433ebcf48bb5b6a3d41b</td>\n",
       "      <td>2022-11-01_20-14-55</td>\n",
       "      <td>1667333695</td>\n",
       "      <td>3.698084</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00006</th>\n",
       "      <td>0.085105</td>\n",
       "      <td>0.247247</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2033aa1d5f7047aaabba2e6f654f3661</td>\n",
       "      <td>2022-11-01_20-14-55</td>\n",
       "      <td>1667333695</td>\n",
       "      <td>3.411499</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ss</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>64</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00007</th>\n",
       "      <td>0.082369</td>\n",
       "      <td>0.219396</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>5cc423fa139b475bb55725e4c27d3c03</td>\n",
       "      <td>2022-11-01_20-14-55</td>\n",
       "      <td>1667333695</td>\n",
       "      <td>2.962978</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ds</td>\n",
       "      <td>max</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00008</th>\n",
       "      <td>0.083744</td>\n",
       "      <td>0.119390</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2033aa1d5f7047aaabba2e6f654f3661</td>\n",
       "      <td>2022-11-01_20-14-57</td>\n",
       "      <td>1667333697</td>\n",
       "      <td>1.959604</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>max</td>\n",
       "      <td>0.073574</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.2]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c133d_00009</th>\n",
       "      <td>0.079416</td>\n",
       "      <td>0.150603</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>a96250e996b1433ebcf48bb5b6a3d41b</td>\n",
       "      <td>2022-11-01_20-14-57</td>\n",
       "      <td>1667333697</td>\n",
       "      <td>1.903405</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>ts</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>128</td>\n",
       "      <td>[4, 16]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s  done timesteps_total episodes_total  \\\n",
       "trial_id                                                                       \n",
       "c133d_00000  0.081207          0.308473  True            None           None   \n",
       "c133d_00001  0.084218          0.346641  True            None           None   \n",
       "c133d_00002  0.080231          0.345087  True            None           None   \n",
       "c133d_00003  0.089137          0.111302  True            None           None   \n",
       "c133d_00004  0.080214          0.325054  True            None           None   \n",
       "c133d_00005  0.080246          0.388377  True            None           None   \n",
       "c133d_00006  0.085105          0.247247  True            None           None   \n",
       "c133d_00007  0.082369          0.219396  True            None           None   \n",
       "c133d_00008  0.083744          0.119390  True            None           None   \n",
       "c133d_00009  0.079416          0.150603  True            None           None   \n",
       "\n",
       "             training_iteration                     experiment_id  \\\n",
       "trial_id                                                            \n",
       "c133d_00000                  10  a96250e996b1433ebcf48bb5b6a3d41b   \n",
       "c133d_00001                  10  5cc423fa139b475bb55725e4c27d3c03   \n",
       "c133d_00002                  10  2033aa1d5f7047aaabba2e6f654f3661   \n",
       "c133d_00003                  10  0e2662855e564568801e80eac2f05e4a   \n",
       "c133d_00004                  10  0e2662855e564568801e80eac2f05e4a   \n",
       "c133d_00005                  10  a96250e996b1433ebcf48bb5b6a3d41b   \n",
       "c133d_00006                  10  2033aa1d5f7047aaabba2e6f654f3661   \n",
       "c133d_00007                  10  5cc423fa139b475bb55725e4c27d3c03   \n",
       "c133d_00008                  10  2033aa1d5f7047aaabba2e6f654f3661   \n",
       "c133d_00009                  10  a96250e996b1433ebcf48bb5b6a3d41b   \n",
       "\n",
       "                            date   timestamp  time_total_s  ...  \\\n",
       "trial_id                                                    ...   \n",
       "c133d_00000  2022-11-01_20-14-51  1667333691     27.777361  ...   \n",
       "c133d_00001  2022-11-01_20-14-52  1667333692     19.534242  ...   \n",
       "c133d_00002  2022-11-01_20-14-52  1667333692     10.659317  ...   \n",
       "c133d_00003  2022-11-01_20-14-51  1667333691      1.731071  ...   \n",
       "c133d_00004  2022-11-01_20-14-57  1667333697      5.441444  ...   \n",
       "c133d_00005  2022-11-01_20-14-55  1667333695      3.698084  ...   \n",
       "c133d_00006  2022-11-01_20-14-55  1667333695      3.411499  ...   \n",
       "c133d_00007  2022-11-01_20-14-55  1667333695      2.962978  ...   \n",
       "c133d_00008  2022-11-01_20-14-57  1667333697      1.959604  ...   \n",
       "c133d_00009  2022-11-01_20-14-57  1667333697      1.903405  ...   \n",
       "\n",
       "             config/output_dim config/strand config/aggr  config/lr  \\\n",
       "trial_id                                                              \n",
       "c133d_00000                  1            ts         max   0.000516   \n",
       "c133d_00001                  1            ss         max   0.002293   \n",
       "c133d_00002                  1            ts         avg   0.014657   \n",
       "c133d_00003                  1            ss         avg   0.037255   \n",
       "c133d_00004                  1            ts         max   0.095442   \n",
       "c133d_00005                  1            ts         max   0.065663   \n",
       "c133d_00006                  1            ss         avg   0.007753   \n",
       "c133d_00007                  1            ds         max   0.000389   \n",
       "c133d_00008                  1            ts         max   0.073574   \n",
       "c133d_00009                  1            ts         avg   0.000654   \n",
       "\n",
       "             config/batch_size  config/conv_kwargs/channels  \\\n",
       "trial_id                                                      \n",
       "c133d_00000                128                      [4, 16]   \n",
       "c133d_00001                 32                      [4, 16]   \n",
       "c133d_00002                 64                      [4, 16]   \n",
       "c133d_00003                128                      [4, 16]   \n",
       "c133d_00004                 32                      [4, 16]   \n",
       "c133d_00005                 64                      [4, 16]   \n",
       "c133d_00006                 64                      [4, 16]   \n",
       "c133d_00007                128                      [4, 16]   \n",
       "c133d_00008                128                      [4, 16]   \n",
       "c133d_00009                128                      [4, 16]   \n",
       "\n",
       "             config/conv_kwargs/conv_kernels config/conv_kwargs/pool_kernels  \\\n",
       "trial_id                                                                       \n",
       "c133d_00000                              [3]                             [2]   \n",
       "c133d_00001                              [3]                             [2]   \n",
       "c133d_00002                              [5]                             [2]   \n",
       "c133d_00003                              [5]                             [4]   \n",
       "c133d_00004                              [3]                             [2]   \n",
       "c133d_00005                              [3]                             [4]   \n",
       "c133d_00006                              [5]                             [4]   \n",
       "c133d_00007                              [5]                             [4]   \n",
       "c133d_00008                              [3]                             [2]   \n",
       "c133d_00009                              [5]                             [4]   \n",
       "\n",
       "            config/conv_kwargs/dropout_rates  config/fc_kwargs/hidden_dims  \n",
       "trial_id                                                                    \n",
       "c133d_00000                            [0.2]                          [32]  \n",
       "c133d_00001                            [0.1]                          [32]  \n",
       "c133d_00002                            [0.1]                          [32]  \n",
       "c133d_00003                            [0.2]                          [32]  \n",
       "c133d_00004                            [0.1]                          [32]  \n",
       "c133d_00005                            [0.1]                          [32]  \n",
       "c133d_00006                            [0.2]                          [32]  \n",
       "c133d_00007                            [0.2]                          [32]  \n",
       "c133d_00008                            [0.2]                          [32]  \n",
       "c133d_00009                            [0.1]                          [32]  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"arch\": \"CNN\",\n",
    "  \"input_len\": 100,\n",
    "  \"output_dim\": 1,\n",
    "  \"strand\": \"ss\",\n",
    "  \"aggr\": None,\n",
    "  \"lr\": 1e-3,\n",
    "  \"batch_size\": 64,\n",
    "  \"conv_kwargs\": {\n",
    "    \"channels\": [4, 16],\n",
    "    \"conv_kernels\": [3],\n",
    "    \"pool_kernels\": [2],\n",
    "    \"dropout_rates\": [0.1]\n",
    "  },\n",
    "  \"fc_kwargs\": {\n",
    "    \"hidden_dims\": [32]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (hp_metric): R2Score()\n",
       "  (convnet): BasicConv1D(\n",
       "    (module): Sequential(\n",
       "      (0): Conv1d(4, 16, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fcn): BasicFullyConnectedModule(\n",
       "    (module): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = eu.models.get_model(config[\"arch\"], config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | hp_metric | R2Score                   | 0     \n",
      "1 | convnet   | BasicConv1D               | 208   \n",
      "2 | fcn       | BasicFullyConnectedModule | 25.2 K\n",
      "--------------------------------------------------------\n",
      "25.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.4 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db39ac295884fc4ab30b08726c7347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6998e9dcf774edd9482d9ee4ee27c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e173f9d1cce04d7cbe68e85f769ec5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902c16a330e4eafaa522a4f20c84309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31248cdf86df4f4ba14871769d63418c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2fb05421e44d89ad1576bbfcb0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d448ea3a99a47ce8d79d8ccb5727589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bf81645394473826ec9f52af0b798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622dcc399beb4d7fae9aafa75f67bb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ef77a592948b798eaa1f4d3dbf7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770edc647520430087e5f84a16a98f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1839eb1ca04a8692da9fb1483bb6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eu.train.fit(\n",
    "    model = model,\n",
    "    sdata = sdata,\n",
    "    target_keys = \"activity_0\", \n",
    "    train_key = \"train_val\",\n",
    "    epochs = 10,\n",
    "    gpus = 0,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loss', 'time_this_iter_s', 'done', 'timesteps_total', 'episodes_total',\n",
       "       'training_iteration', 'experiment_id', 'date', 'timestamp',\n",
       "       'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore',\n",
       "       'timesteps_since_restore', 'iterations_since_restore', 'warmup_time',\n",
       "       'experiment_tag', 'config/arch', 'config/input_len',\n",
       "       'config/output_dim', 'config/strand', 'config/aggr', 'config/lr',\n",
       "       'config/batch_size', 'config/conv_kwargs/channels',\n",
       "       'config/conv_kwargs/conv_kernels', 'config/conv_kwargs/pool_kernels',\n",
       "       'config/conv_kwargs/dropout_rates', 'config/fc_kwargs/hidden_dims'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.results_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(config, train_dataset, val_dataset, epochs=10, gpus=0):\n",
    "  model =  eu.models.FCN(\n",
    "    input_len=100, \n",
    "    output_dim=1, \n",
    "    lr=config[\"lr\"]\n",
    "    )\n",
    "  train_dl = train_dataset.to_dataloader(batch_size=config[\"batch_size\"])\n",
    "  val_dl = val_dataset.to_dataloader(batch_size=config[\"batch_size\"])\n",
    "  trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    gpus=gpus,\n",
    "    progress_bar_refresh_rate=0,\n",
    "    callbacks=[callback])\n",
    "  trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = tune.with_parameters(\n",
    "    train_tune,\n",
    "    train_dataset=sdataset_train,\n",
    "    val_dataset=sdataset_val,\n",
    "    epochs=10,\n",
    "    gpus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 03:34:19,999\tERROR experiment_analysis.py:486 -- No checkpoints have been found for trial train_tune_86a07_00000.\n"
     ]
    }
   ],
   "source": [
    "best_trial = analysis.best_trial  # Get best trial\n",
    "best_config = analysis.best_config  # Get best trial's hyperparameters\n",
    "best_logdir = analysis.best_logdir  # Get best trial's logdir\n",
    "best_checkpoint = analysis.best_checkpoint  # Get best trial's best checkpoint\n",
    "best_result = analysis.best_result  # Get best trial's last results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_tune_86a07_00000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.00010000831284081109, 'batch_size': 32}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vscode/ray_results/test/train_tune_86a07_00000_0_batch_size=32,lr=0.0001_2022-10-19_03-31-32'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.11025925725698471,\n",
       " 'time_this_iter_s': 0.11937999725341797,\n",
       " 'done': True,\n",
       " 'timesteps_total': None,\n",
       " 'episodes_total': None,\n",
       " 'training_iteration': 10,\n",
       " 'trial_id': '86a07_00000',\n",
       " 'experiment_id': '299e91b7003d4457b77be9e3b61a2294',\n",
       " 'date': '2022-10-19_03-31-51',\n",
       " 'timestamp': 1666150311,\n",
       " 'time_total_s': 10.0547194480896,\n",
       " 'pid': 23527,\n",
       " 'hostname': 'codespaces-97ce9f',\n",
       " 'node_ip': '172.16.5.4',\n",
       " 'config': {'lr': 0.00010000831284081109, 'batch_size': 32},\n",
       " 'time_since_restore': 10.0547194480896,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 10,\n",
       " 'warmup_time': 0.0036079883575439453,\n",
       " 'experiment_tag': '0_batch_size=32,lr=0.0001'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'Z\\xe4~\\xb8\\x0c\\\\\\x08\\x1b`', b'\\x93\\xa5x\\xa8\\xf9\\x18\\xaf \\xc7\\x85\\x1f\\xd3\\x93x\\x86k9\\xcd\\xfe C\\xf5\\x80\\xd7\\xd2\\x85\\xaeE\\x0e_\\xbeHF\\xe6\\xc2p\\xe0`W\\xab\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00', b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 /\\x934\\x9f\\xd5\\x8eo7\\x8d\\xe5rv\\xbc1z^B\\x17%\\x86\\x00\\x9cn\\x82\\xebF\\xf6*Qy\\t\\x1e']\n",
      "Bad pipe message: %s [b\"q8\\xf8\\xce\\xd9O\\xe02\\xaeh\\x96\\xe3\\ru'\\xa7\\xff\\xba O\\xe0\\xf9\\x88\\xe3\\x04{\\x1em\\xf6\\xe3Zt\\xb5\\x8e\\xa3-\\xd1\\xba=\\xd1`-\\xfdC\\xca\\xde$\\x1a\\x85V\\xc6\\x00\\x08\\x13\\x02\\x13\"]\n",
      "Bad pipe message: %s [b'\\x12\\xae\\xc8\\xf1gV\\r\\x002\\xe63\\x8d\\x9e\\x1b\\x14>O\\xcc\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0']\n",
      "Bad pipe message: %s [b\"$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\"]\n",
      "Bad pipe message: %s [b\"\\xce\\x0e\\xd6\\xb2]1\\xfd\\x82\\xb3\\x8b\\xbc\\x17Mr\\xfc\\x8f\\x80H\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\", b'\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'A=\\xdf\\xba{.\\xce\\xaa\\x8f\\xde\\x19\\x18T\\x19\\xa6\\x9c;\\xae\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t12']\n",
      "Bad pipe message: %s [b'0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b'\\xa4\\t\\xd2\\xfc\\xa5=\\x83\\x0c\\x1dE\\t\\xf5\\xd8\\xf1\\xca\\xadmL\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0']\n",
      "Bad pipe message: %s [b'\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c']\n",
      "Bad pipe message: %s [b'\\xdd\\xd0R\\xa0\\x8b\\x89&y\\x8b\\x1a\\xde\\xf03\\x12\\xa3\\xfa\\x07\\xfb\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007']\n",
      "Bad pipe message: %s [b'g\\x15<\\x7f\\xa9\\xecn\\xff\\x06\\x1d\\x14)\\x95sq\\xc8^\\xc2\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x00']\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02']\n",
      "Bad pipe message: %s [b\"\\x0c\\x93\\x9bEn\\xe04&\\x12H\\x08A\\x8fH;\\xfd\\xc7!\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\", b'\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 ']\n"
     ]
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
