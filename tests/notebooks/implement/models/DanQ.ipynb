{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661f6b58",
   "metadata": {},
   "source": [
    "# Testing `ResidualBind` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64213c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *11/05/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook for testing out the custom `ResidualBind` model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DanQ(nn.Module):\n",
    "    \"\"\"DanQ model from Quang and Xie, 2016; \n",
    "        see <https://academic.oup.com/nar/article/44/11/e107/2468300> \n",
    "        and <https://github.com/uci-cbcl/DanQ/blob/master/DanQ_train.py>\n",
    "        and <https://github.com/FunctionLab/selene/blob/master/models/danQ.py>\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, d=320,\n",
    "                 conv1_filters=None, learn_conv1_filters=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d != 320:\n",
    "            print(\"NB: number of convolutional filters in original DanQ model is 320; current number of convolutional filters is not set to 320\")\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.init_conv1_filters = conv1_filters\n",
    "        \n",
    "        assert (not (conv1_filters is None and not learn_conv1_filters)), \"initial conv1_filters cannot be set to None while learn_conv1_filters is set to False\"\n",
    "        \n",
    "        # Layer 1 (convolutional), constituent parts\n",
    "        if conv1_filters is not None:\n",
    "            if learn_conv1_filters: # continue modifying existing conv1_filters through learning\n",
    "                self.conv1_filters = torch.nn.Parameter( torch.Tensor(conv1_filters) )\n",
    "            else:\n",
    "                self.register_buffer(\"conv1_filters\", torch.Tensor(conv1_filters))\n",
    "        else:\n",
    "            self.conv1_filters = torch.nn.Parameter(torch.zeros(d, 4, 26))\n",
    "            torch.nn.init.kaiming_normal_(self.conv1_filters)\n",
    "        self.activation1 = nn.ReLU() # name the first-layer activation function for hook purposes\n",
    "        self.maxpool1 = nn.MaxPool1d(13)\n",
    "        \n",
    "        # Layer 2 (bi-directional LSTM), constituent parts\n",
    "        self.bdlstm2 = nn.LSTM(d, d, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Layer 3 (fully connected), constituent parts\n",
    "        self.fc3 = nn.LazyLinear(925, bias=False)\n",
    "        \n",
    "        # Output layer (fully connected), constituent parts\n",
    "        self.fc4 = nn.Linear(925, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def get_which_conv_layers_transferred(self):\n",
    "        layers = []\n",
    "        if self.init_conv1_filters is not None:\n",
    "            layers.append(1)\n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        out = torch.conv1d(x, self.conv1_filters, stride=1, padding=(self.conv1_filters.shape[-1]//2))\n",
    "        out = self.activation1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # Layer 2\n",
    "        out = torch.transpose(out, 1, 2) # make dims (batch, seq, features) to comply with bi-dir. LSTM\n",
    "        out, _ = self.bdlstm2(out) # see <https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>\n",
    "        out = self.dropout5(out)\n",
    "        out = torch.transpose(out, 1, 2) # change dims back to (batch, features, seq)\n",
    "        \n",
    "        # Layer 3\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc4(out) \n",
    "        y_pred = self.sigmoid(out)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "From selele\n",
    "class DanQ(nn.Module):\n",
    "    def __init__(self, sequence_length, n_genomic_features):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence_length : int\n",
    "            Input sequence length\n",
    "        n_genomic_features : int\n",
    "            Total number of features to predict\n",
    "        \"\"\"\n",
    "        super(DanQ, self).__init__()\n",
    "        self.nnet = nn.Sequential(\n",
    "            nn.Conv1d(4, 320, kernel_size=26),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(\n",
    "                kernel_size=13, stride=13),\n",
    "            nn.Dropout(0.2))\n",
    "\n",
    "        self.bdlstm = nn.Sequential(\n",
    "            nn.LSTM(\n",
    "                320, 320, num_layers=1, batch_first=True, bidirectional=True))\n",
    "\n",
    "        self._n_channels = math.floor(\n",
    "            (sequence_length - 25) / 13)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self._n_channels * 640, 925),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(925, n_genomic_features),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of a batch.\n",
    "        \"\"\"\n",
    "        out = self.nnet(x)\n",
    "        reshape_out = out.transpose(0, 1).transpose(0, 2)\n",
    "        out, _ = self.bdlstm(reshape_out)\n",
    "        out = out.transpose(0, 1)\n",
    "        reshape_out = out.contiguous().view(\n",
    "            out.size(0), 640 * self._n_channels)\n",
    "        predict = self.classifier(reshape_out)\n",
    "        return predict\n",
    "\n",
    "def criterion():\n",
    "    return nn.BCELoss()\n",
    "\n",
    "def get_optimizer(lr):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Basset(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5405, 0.4398],\n",
       "        [0.6291, 0.5908],\n",
       "        [0.5014, 0.6883],\n",
       "        [0.3922, 0.7281],\n",
       "        [0.5423, 0.5404],\n",
       "        [0.4932, 0.5351],\n",
       "        [0.5355, 0.4664],\n",
       "        [0.4262, 0.4420],\n",
       "        [0.4865, 0.6399],\n",
       "        [0.4958, 0.5314]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 4, 100)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b89b94102a47d3a29709cf5f5a8de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "Missing logger folder: /workspaces/EUGENe/tests/notebooks/implement/models/eugene_logs/ssResidualBind_regression\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name           | Type                      | Params\n",
      "-------------------------------------------------------------\n",
      "0 | hp_metric      | R2Score                   | 0     \n",
      "1 | conv           | BasicConv1D               | 4.5 K \n",
      "2 | residual_block | ResidualModule            | 83.8 K\n",
      "3 | average_pool   | AvgPool1d                 | 0     \n",
      "4 | dropout        | Dropout                   | 0     \n",
      "5 | flatten        | Flatten                   | 0     \n",
      "6 | fc             | BasicFullyConnectedModule | 2.0 M \n",
      "-------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.320     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15103dc447749308845d19db0efb48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be41c66e0c334c92a7ae592ab87011c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a7e9ff522f4e0c9760a547f1ca3254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eu.train.fit(model, sdata, target_keys=\"activity_0\", epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
