{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661f6b58",
   "metadata": {},
   "source": [
    "# Testing `Basset` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64213c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *11/06/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook for testing out the custom `Basset` model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base import BaseModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from eugene.models.base._utils import GetFlattenDim\n",
    "from eugene.models.base import BasicFullyConnectedModule, BasicConv1D\n",
    "\n",
    "\n",
    "class Basset(BaseModel):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_len: int = 1000,\n",
    "        output_dim = 1, \n",
    "        strand = \"ss\",\n",
    "        task = \"binary_classification\",\n",
    "        aggr = None,\n",
    "        loss_fxn = \"bce\",\n",
    "        conv_kwargs = {},\n",
    "        fc_kwargs = {},\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            input_len, \n",
    "            output_dim, \n",
    "            strand=strand, \n",
    "            task=task, \n",
    "            aggr=aggr, \n",
    "            loss_fxn=loss_fxn, \n",
    "            **kwargs\n",
    "        )\n",
    "        self.conv_kwargs, self.fc_kwargs = self.kwarg_handler(conv_kwargs, fc_kwargs)\n",
    "        self.convnet = BasicConv1D(\n",
    "            input_len=input_len, \n",
    "            **self.conv_kwargs)\n",
    "        self.fcn = BasicFullyConnectedModule(\n",
    "            input_dim=self.convnet.flatten_dim, \n",
    "            output_dim=output_dim, \n",
    "            **self.fc_kwargs\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_rev_comp=None):\n",
    "        x = self.convnet(x)\n",
    "        x = x.view(x.size(0), self.convnet.flatten_dim)\n",
    "        x = self.fcn(x)\n",
    "        return x\n",
    "        \n",
    "    def kwarg_handler(self, conv_kwargs, fc_kwargs):\n",
    "        \"\"\"Sets default kwargs for conv and fc modules if not specified\"\"\"\n",
    "        conv_kwargs.setdefault(\"channels\", [4, 300, 200, 200])\n",
    "        conv_kwargs.setdefault(\"conv_kernels\", [19, 11, 7])\n",
    "        conv_kwargs.setdefault(\"conv_strides\", [1, 1, 1])\n",
    "        conv_kwargs.setdefault(\"padding\", [9, 5, 3])\n",
    "        conv_kwargs.setdefault(\"pool_kernels\", [3, 4, 4])\n",
    "        conv_kwargs.setdefault(\"omit_final_pool\", False)\n",
    "        conv_kwargs.setdefault(\"dropout_rates\", 0.0)\n",
    "        conv_kwargs.setdefault(\"batchnorm\", True)\n",
    "        conv_kwargs.setdefault(\"activation\", \"relu\")\n",
    "        fc_kwargs.setdefault(\"hidden_dims\", [1000, 164])\n",
    "        fc_kwargs.setdefault(\"dropout_rate\", 0.0)\n",
    "        fc_kwargs.setdefault(\"batchnorm\", True)\n",
    "        fc_kwargs.setdefault(\"activation\", \"relu\")\n",
    "        return conv_kwargs, fc_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Basset(task=\"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 4, 1000)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = eu.dl.SeqData(seqs=eu.utils.random_seqs(1000, 1000))\n",
    "sdata.make_names_unique()\n",
    "sdata[\"activity\"] = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f05b7aa4199416fadc8cf4a26fdcf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "Missing logger folder: /workspaces/EUGENe/tests/notebooks/implement/models/eugene_logs/ssBasset_regression\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | hp_metric | R2Score                   | 0     \n",
      "1 | convnet   | BasicConv1D               | 964 K \n",
      "2 | fcn       | BasicFullyConnectedModule | 4.2 M \n",
      "--------------------------------------------------------\n",
      "5.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 M     Total params\n",
      "20.530    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8d8c2a2ff14098b540b4bb7dc481ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dc6d54b1d74dd3bb27a1aa19b4026b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907db38488874bb5b72b680e66a00c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([32, 200, 20])\n",
      "torch.Size([8, 200, 20])\n"
     ]
    }
   ],
   "source": [
    "eu.train.fit(model, sdata, target_keys=\"activity\", epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base import BaseModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from eugene.models.base._utils import GetFlattenDim\n",
    "from eugene.models.base import BasicFullyConnectedModule, BasicConv1D\n",
    "\n",
    "class FactorizedBasset(BaseModel):\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tinput_len: int = 1000,\n",
    "\t\toutput_dim = 1, \n",
    "\t\tstrand = \"ss\",\n",
    "\t\ttask = \"binary_classification\",\n",
    "\t\taggr = None,\n",
    "\t\tloss_fxn = \"bce\",\n",
    "\t\tconv1_kwargs = {},\n",
    "\t\tconv2_kwargs = {},\n",
    "\t\tconv3_kwargs = {},\n",
    "\t\tmaxpool_kernels = None,\n",
    "\t\tfc_kwargs = {},\n",
    "\t\t**kwargs\n",
    "\t):\n",
    "\t\tsuper().__init__(\n",
    "\t\t\tinput_len, \n",
    "\t\t\toutput_dim, \n",
    "\t\t\tstrand=strand, \n",
    "\t\t\ttask=task, \n",
    "\t\t\taggr=aggr, \n",
    "\t\t\tloss_fxn=loss_fxn, \n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\tself.conv1_kwargs, self.conv2_kwargs, self.conv3_kwargs, self.maxpool_kernels, self.fc_kwargs = self.kwarg_handler(\n",
    "\t\t\tconv1_kwargs, \n",
    "\t\t\tconv2_kwargs, \n",
    "\t\t\tconv3_kwargs, \n",
    "\t\t\tmaxpool_kernels, \n",
    "\t\t\tfc_kwargs\n",
    "\t\t)\n",
    "\t\tself.convnet1 = BasicConv1D(\n",
    "\t\t\tinput_len=input_len, \n",
    "\t\t\t**self.conv1_kwargs\n",
    "\t\t)\n",
    "\t\tself.maxpool1 = nn.MaxPool1d(self.maxpool_kernels[0])\n",
    "\t\tself.out1 = self.convnet1.flatten_dim/self.convnet1.out_channels // self.maxpool_kernels[0]\n",
    "\t\tself.convnet2 = BasicConv1D(\n",
    "\t\t\tinput_len=self.out1,\n",
    "\t\t\t**self.conv2_kwargs\n",
    "\t\t)\n",
    "\t\tself.maxpool2 = nn.MaxPool1d(self.maxpool_kernels[1])\n",
    "\t\tself.out2 = self.convnet2.flatten_dim/self.convnet2.out_channels // self.maxpool_kernels[1]\n",
    "\t\tself.convnet3 = BasicConv1D(\n",
    "\t\t\tinput_len=self.out2,\n",
    "\t\t\t**self.conv3_kwargs\n",
    "\t\t)\n",
    "\t\tself.maxpool3 = nn.MaxPool1d(self.maxpool_kernels[2])\n",
    "\t\tself.out3 = self.convnet3.flatten_dim/self.convnet3.out_channels // self.maxpool_kernels[2]\n",
    "\t\tself.fcnet_in = int(self.out3*self.convnet3.out_channels)\n",
    "\t\tself.fcnet = BasicFullyConnectedModule(\n",
    "\t\t\tinput_dim=self.fcnet_in,\n",
    "\t\t\toutput_dim=output_dim, \n",
    "\t\t\t**self.fc_kwargs\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x, x_rev_comp=None):\n",
    "\t\tx = self.convnet1(x)\n",
    "\t\tx = self.maxpool1(x)\n",
    "\t\tx = self.convnet2(x)\n",
    "\t\tx = self.maxpool2(x)\n",
    "\t\tx = self.convnet3(x)\n",
    "\t\tx = self.maxpool3(x)\n",
    "\t\tx = x.view(x.size(0), self.fcnet_in)\n",
    "\t\tx = self.fcnet(x)\n",
    "\t\treturn x\n",
    "        \n",
    "\tdef kwarg_handler(self, conv1_kwargs, conv2_kwargs, conv3_kwargs, maxpool_kernels, fc_kwargs):\n",
    "\t\t\"\"\"Sets default kwargs for conv and fc modules if not specified\"\"\"\n",
    "\t\tconv1_kwargs.setdefault(\"channels\", [4, 48, 64, 100, 150, 300])\n",
    "\t\tconv1_kwargs.setdefault(\"conv_kernels\", [3, 3, 3, 7, 7])\n",
    "\t\tconv1_kwargs.setdefault(\"conv_strides\", [1, 1, 1, 1, 1])\n",
    "\t\tconv1_kwargs.setdefault(\"padding\", [1, 1, 1, 3, 3])\n",
    "\t\tconv1_kwargs.setdefault(\"pool_kernels\", None)\n",
    "\t\tconv1_kwargs.setdefault(\"dropout_rates\", 0.0)\n",
    "\t\tconv1_kwargs.setdefault(\"batchnorm\", True)\n",
    "\t\tconv1_kwargs.setdefault(\"activation\", \"relu\")\n",
    "\t\tconv2_kwargs.setdefault(\"channels\", [300, 200, 200, 200])\n",
    "\t\tconv2_kwargs.setdefault(\"conv_kernels\", [7, 3, 3])\n",
    "\t\tconv2_kwargs.setdefault(\"conv_strides\", [1, 1, 1])\n",
    "\t\tconv2_kwargs.setdefault(\"padding\", [3, 1, 1])\n",
    "\t\tconv2_kwargs.setdefault(\"pool_kernels\", None)\n",
    "\t\tconv2_kwargs.setdefault(\"dropout_rates\", 0.0)\n",
    "\t\tconv2_kwargs.setdefault(\"batchnorm\", True)\n",
    "\t\tconv2_kwargs.setdefault(\"activation\", \"relu\")\n",
    "\t\tconv3_kwargs.setdefault(\"channels\", [200, 200])\n",
    "\t\tconv3_kwargs.setdefault(\"conv_kernels\", [7])\n",
    "\t\tconv3_kwargs.setdefault(\"conv_strides\", [1])\n",
    "\t\tconv3_kwargs.setdefault(\"padding\", [3])\n",
    "\t\tconv3_kwargs.setdefault(\"pool_kernels\", None)\n",
    "\t\tconv3_kwargs.setdefault(\"dropout_rates\", 0.0)\n",
    "\t\tconv3_kwargs.setdefault(\"batchnorm\", True)\n",
    "\t\tconv3_kwargs.setdefault(\"activation\", \"relu\")\n",
    "\t\tmaxpool_kernels = [3, 4, 4] if maxpool_kernels is None else maxpool_kernels\n",
    "\t\tfc_kwargs.setdefault(\"hidden_dims\", [1000, 164])\n",
    "\t\tfc_kwargs.setdefault(\"dropout_rate\", 0.0)\n",
    "\t\tfc_kwargs.setdefault(\"batchnorm\", True)\n",
    "\t\tfc_kwargs.setdefault(\"activation\", \"relu\")\n",
    "\t\treturn conv1_kwargs, conv2_kwargs, conv3_kwargs, maxpool_kernels, fc_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FactorizedBasset(input_len=1000, task=\"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1525],\n",
       "        [-0.0843],\n",
       "        [-0.1764],\n",
       "        [ 0.1329],\n",
       "        [-0.6349],\n",
       "        [ 1.2232],\n",
       "        [ 0.1299],\n",
       "        [-0.6046],\n",
       "        [ 0.1690],\n",
       "        [-0.8385]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 4, 1000)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = eu.dl.SeqData(seqs=eu.utils.random_seqs(1000, 1000))\n",
    "sdata.make_names_unique()\n",
    "sdata[\"activity\"] = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746b2bd2564540568e2aded7df8d727b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | hp_metric | R2Score                   | 0     \n",
      "1 | convnet1  | BasicConv1D               | 450 K \n",
      "2 | maxpool1  | MaxPool1d                 | 0     \n",
      "3 | convnet2  | BasicConv1D               | 661 K \n",
      "4 | maxpool2  | MaxPool1d                 | 0     \n",
      "5 | convnet3  | BasicConv1D               | 280 K \n",
      "6 | maxpool3  | MaxPool1d                 | 0     \n",
      "7 | fcnet     | BasicFullyConnectedModule | 4.2 M \n",
      "--------------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.244    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c56f12912f483e9f5cdaebe196b119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95f390107eb4a2481c5a34316f6e342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7be9bfcf954271932c479362077a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eu.train.fit(model, sdata, target_keys=\"activity\", epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
