{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661f6b58",
   "metadata": {},
   "source": [
    "# Testing `ResidualBind` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64213c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *11/05/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook for testing out the custom `ResidualBind` model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evoaug\n",
    "class Basset(nn.Module):\n",
    "    \"\"\"Basset model from Kelley et al., 2016; \n",
    "        see <https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115.abstract>\n",
    "        and <https://github.com/davek44/Basset/blob/master/data/models/pretrained_params.txt>\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, d=300, \n",
    "                 conv1_filters=None, learn_conv1_filters=True,\n",
    "                 conv2_filters=None, learn_conv2_filters=True,\n",
    "                 conv3_filters=None, learn_conv3_filters=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d != 300:\n",
    "            print(\"NB: number of first-layer convolutional filters in original Basset model is 300; current number of first-layer convolutional filters is not set to 300\")\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.init_conv1_filters = conv1_filters\n",
    "        self.init_conv2_filters = conv2_filters\n",
    "        self.init_conv3_filters = conv3_filters\n",
    "        \n",
    "        assert (not (conv1_filters is None and not learn_conv1_filters)), \"initial conv1_filters cannot be set to None while learn_conv1_filters is set to False\"\n",
    "        assert (not (conv2_filters is None and not learn_conv2_filters)), \"initial conv2_filters cannot be set to None while learn_conv2_filters is set to False\"\n",
    "        assert (not (conv3_filters is None and not learn_conv3_filters)), \"initial conv3_filters cannot be set to None while learn_conv3_filters is set to False\"\n",
    "        \n",
    "        # Layer 1 (convolutional), constituent parts\n",
    "        if conv1_filters is not None:\n",
    "            if learn_conv1_filters: # continue modifying existing conv1_filters through learning\n",
    "                self.conv1_filters = nn.Parameter( torch.Tensor(conv1_filters) )\n",
    "            else:\n",
    "                self.register_buffer(\"conv1_filters\", torch.Tensor(conv1_filters))\n",
    "        else:\n",
    "            self.conv1_filters = nn.Parameter(torch.zeros(d, 4, 19))\n",
    "            nn.init.kaiming_normal_(self.conv1_filters)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(d)\n",
    "        self.activation1 = nn.ReLU() # name the first-layer activation function for hook purposes\n",
    "        self.maxpool1 = nn.MaxPool1d(3)\n",
    "        \n",
    "        # Layer 2 (convolutional), constituent parts\n",
    "        if conv2_filters is not None:\n",
    "            if learn_conv2_filters: # continue modifying existing conv2_filters through learning\n",
    "                self.conv2_filters = nn.Parameter( torch.Tensor(conv2_filters) )\n",
    "            else:\n",
    "                self.register_buffer(\"conv2_filters\", torch.Tensor(conv2_filters))\n",
    "        else:\n",
    "            self.conv2_filters = nn.Parameter(torch.zeros(200, d, 11))\n",
    "            nn.init.kaiming_normal_(self.conv2_filters)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(200)\n",
    "        self.maxpool2 = nn.MaxPool1d(4)\n",
    "        \n",
    "        # Layer 3 (convolutional), constituent parts\n",
    "        if conv3_filters is not None:\n",
    "            if learn_conv3_filters: # continue modifying existing conv3_filters through learning\n",
    "                self.conv3_filters = nn.Parameter( torch.Tensor(conv3_filters) )\n",
    "            else:\n",
    "                self.register_buffer(\"conv3_filters\", torch.Tensor(conv3_filters))\n",
    "        else:\n",
    "            self.conv3_filters = nn.Parameter(torch.zeros(200, 200, 7))\n",
    "            nn.init.kaiming_normal_(self.conv3_filters)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.maxpool3 = nn.MaxPool1d(4)\n",
    "        \n",
    "        # Layer 4 (fully connected), constituent parts\n",
    "        self.fc4 = nn.LazyLinear(1000, bias=False)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        # Layer 5 (fully connected), constituent parts\n",
    "        self.fc5 = nn.Linear(1000, 1000, bias=False)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        # Output layer (fully connected), constituent parts\n",
    "        self.fc6 = nn.Linear(1000, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def get_which_conv_layers_transferred(self):\n",
    "        layers = []\n",
    "        if self.init_conv1_filters is not None:\n",
    "            layers.append(1)\n",
    "        if self.init_conv2_filters is not None:\n",
    "            layers.append(2)\n",
    "        if self.init_conv3_filters is not None:\n",
    "            layers.append(3)\n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        cnn = torch.conv1d(x, self.conv1_filters, stride=1, padding=(self.conv1_filters.shape[-1]//2))\n",
    "        cnn = self.batchnorm1(cnn)\n",
    "        cnn = self.activation1(cnn)\n",
    "        cnn = self.maxpool1(cnn)\n",
    "        \n",
    "        # Layer 2\n",
    "        cnn = torch.conv1d(cnn, self.conv2_filters, stride=1, padding=(self.conv2_filters.shape[-1]//2))\n",
    "        cnn = self.batchnorm2(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.maxpool2(cnn)\n",
    "        \n",
    "        # Layer 3\n",
    "        cnn = torch.conv1d(cnn, self.conv3_filters, stride=1, padding=(self.conv3_filters.shape[-1]//2))\n",
    "        cnn = self.batchnorm3(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.maxpool3(cnn)\n",
    "        \n",
    "        # Layer 4\n",
    "        cnn = self.flatten(cnn)\n",
    "        cnn = self.fc4(cnn)\n",
    "        cnn = self.batchnorm4(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.dropout3(cnn)\n",
    "        \n",
    "        # Layer 5\n",
    "        cnn = self.fc5(cnn)\n",
    "        cnn = self.batchnorm5(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.dropout3(cnn)\n",
    "        \n",
    "        # Output layer\n",
    "        cnn = self.fc6(cnn) \n",
    "        y_pred = self.sigmoid(cnn)\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Basset(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yuzu\n",
    "class Basset(torch.nn.Module):\n",
    "\tdef __init__(self, n_inputs, seq_len=None, random_state=0):\n",
    "\t\tsuper(Basset, self).__init__()\n",
    "\t\ttorch.manual_seed(random_state)\n",
    "\n",
    "\t\tself.conv1 = torch.nn.Conv1d(4, 300, kernel_size=19, padding=9)\n",
    "\t\tself.relu1 = torch.nn.ReLU()\n",
    "\t\tself.bn1 = torch.nn.BatchNorm1d(300)\n",
    "\t\tself.maxpool1 = torch.nn.MaxPool1d(3)\n",
    "\n",
    "\t\tself.conv2 = torch.nn.Conv1d(300, 200, kernel_size=11, padding=5)\n",
    "\t\tself.relu2 = torch.nn.ReLU()\n",
    "\t\tself.bn2 = torch.nn.BatchNorm1d(200)\n",
    "\t\tself.maxpool2 = torch.nn.MaxPool1d(4)\n",
    "\n",
    "\t\tself.conv3 = torch.nn.Conv1d(200, 200, kernel_size=7, padding=3)\n",
    "\t\tself.relu3 = torch.nn.ReLU()\n",
    "\t\tself.bn3 = torch.nn.BatchNorm1d(200)\n",
    "\t\tself.maxpool3 = torch.nn.MaxPool1d(4)\n",
    "\n",
    "\t\tself.reshape = Flatten()\n",
    "\n",
    "\t\tself.fc1 = torch.nn.Linear((seq_len // 3 // 4 // 4) * 200, 1000)\n",
    "\t\tself.relu4 = torch.nn.ReLU()\n",
    "\t\tself.bn4 = torch.nn.BatchNorm1d(1000)\n",
    "\n",
    "\t\tself.fc2 = torch.nn.Linear(1000, 1000)\n",
    "\t\tself.relu5 = torch.nn.ReLU()\n",
    "\t\tself.bn5 = torch.nn.BatchNorm1d(1000)\n",
    "\t\t\n",
    "\n",
    "\t\tself.fc3 = torch.nn.Linear(1000, 164)\n",
    "\t\tself.unsqueeze = Unsqueeze(1)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tX = self.maxpool1(self.bn1(self.relu1(self.conv1(X))))\n",
    "\t\t\tX = self.maxpool2(self.bn2(self.relu2(self.conv2(X))))\n",
    "\t\t\tX = self.maxpool3(self.bn3(self.relu3(self.conv3(X))))\n",
    "\n",
    "\t\t\tX = self.reshape(X)\n",
    "\n",
    "\t\t\tX = self.bn4(self.relu4(self.fc1(X)))\n",
    "\t\t\tX = self.bn5(self.relu5(self.fc2(X)))\n",
    "\t\t\tX = self.fc3(X)\n",
    "\t\t\tX = self.unsqueeze(X)\n",
    "\t\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedBasset(torch.nn.Module):\n",
    "\tdef __init__(self, n_inputs, seq_len=None, random_state=0):\n",
    "\t\tsuper(FactorizedBasset, self).__init__()\n",
    "\t\ttorch.manual_seed(random_state)\n",
    "\n",
    "\t\t# \n",
    "\t\tself.conv11 = torch.nn.Conv1d(n_inputs, 48, kernel_size=3, padding=1)\n",
    "\t\tself.bn11 = torch.nn.BatchNorm1d(48)\n",
    "\t\tself.relu11 = torch.nn.ReLU()\n",
    "\t\t\n",
    "\t\tself.conv12 = torch.nn.Conv1d(48, 64, kernel_size=3, padding=1)\n",
    "\t\tself.bn12 = torch.nn.BatchNorm1d(64)\n",
    "\t\tself.relu12 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.conv13 = torch.nn.Conv1d(64, 100, kernel_size=3, padding=1)\n",
    "\t\tself.bn13 = torch.nn.BatchNorm1d(100)\n",
    "\t\tself.relu13 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.conv14 = torch.nn.Conv1d(100, 150, kernel_size=7, padding=3)\n",
    "\t\tself.bn14 = torch.nn.BatchNorm1d(150)\n",
    "\t\tself.relu14 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.conv15 = torch.nn.Conv1d(150, 300, kernel_size=7, padding=3)\n",
    "\t\tself.bn15 = torch.nn.BatchNorm1d(300)\n",
    "\t\tself.relu15 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.mp1 = torch.nn.MaxPool1d(3)\n",
    "\t\t#\n",
    "\n",
    "\t\tself.conv21 = torch.nn.Conv1d(300, 200, kernel_size=7, padding=3)\n",
    "\t\tself.bn21 = torch.nn.BatchNorm1d(200)\n",
    "\t\tself.relu21 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.conv22 = torch.nn.Conv1d(200, 200, kernel_size=3, padding=1)\n",
    "\t\tself.bn22 = torch.nn.BatchNorm1d(200)\n",
    "\t\tself.relu22 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.conv23 = torch.nn.Conv1d(200, 200, kernel_size=3, padding=1)\n",
    "\t\tself.bn23 = torch.nn.BatchNorm1d(200)\n",
    "\t\tself.relu23 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.mp2 = torch.nn.MaxPool1d(4)\n",
    "\t\t#\n",
    "\n",
    "\t\tself.conv3 = torch.nn.Conv1d(200, 200, kernel_size=7, padding=3)\n",
    "\t\tself.bn3 = torch.nn.BatchNorm1d(200)\n",
    "\t\tself.relu3 = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.mp3 = torch.nn.MaxPool1d(4)\n",
    "\n",
    "\t\tself.flatten = Flatten()\n",
    "\t\tself.fc1 = torch.nn.Linear((seq_len // 3 // 4 // 4) * 200, 1000)\n",
    "\t\tself.relu4 = torch.nn.ReLU()\n",
    "\t\tself.bn4 = torch.nn.BatchNorm1d(1000)\n",
    "\t\tself.fc2 = torch.nn.Linear(1000, 1000)\n",
    "\t\tself.relu5 = torch.nn.ReLU()\n",
    "\t\tself.bn5 = torch.nn.BatchNorm1d(1000)\n",
    "\t\tself.fc3 = torch.nn.Linear(1000, 164)\n",
    "\t\tself.unsqueeze = Unsqueeze(1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tx = self.relu11(self.bn11(self.conv11(x)))\n",
    "\t\t\tx = self.relu12(self.bn12(self.conv12(x)))\n",
    "\t\t\tx = self.relu13(self.bn13(self.conv13(x)))\n",
    "\t\t\tx = self.relu14(self.bn14(self.conv14(x)))\n",
    "\t\t\tx = self.relu15(self.bn15(self.conv15(x)))\n",
    "\t\t\tx = self.mp1(x)\n",
    "\n",
    "\t\t\tx = self.relu21(self.bn21(self.conv21(x)))\n",
    "\t\t\tx = self.relu22(self.bn22(self.conv22(x)))\n",
    "\t\t\tx = self.relu23(self.bn23(self.conv23(x)))\n",
    "\t\t\tx = self.mp2(x)\n",
    "\n",
    "\t\t\tx = self.relu3(self.bn3(self.conv3(x)))\n",
    "\t\t\tx = self.mp3(x)\n",
    "\n",
    "\t\t\tx = self.flatten(x)\n",
    "\t\t\tx = self.bn4(self.relu4(self.fc1(x)))\n",
    "\t\t\tx = self.bn5(self.relu5(self.fc2(x)))\n",
    "\t\t\tx = self.fc3(x)\n",
    "\t\t\tx = self.unsqueeze(x)\n",
    "\t\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5405, 0.4398],\n",
       "        [0.6291, 0.5908],\n",
       "        [0.5014, 0.6883],\n",
       "        [0.3922, 0.7281],\n",
       "        [0.5423, 0.5404],\n",
       "        [0.4932, 0.5351],\n",
       "        [0.5355, 0.4664],\n",
       "        [0.4262, 0.4420],\n",
       "        [0.4865, 0.6399],\n",
       "        [0.4958, 0.5314]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 4, 100)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b89b94102a47d3a29709cf5f5a8de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n"
     ]
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "Missing logger folder: /workspaces/EUGENe/tests/notebooks/implement/models/eugene_logs/ssResidualBind_regression\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name           | Type                      | Params\n",
      "-------------------------------------------------------------\n",
      "0 | hp_metric      | R2Score                   | 0     \n",
      "1 | conv           | BasicConv1D               | 4.5 K \n",
      "2 | residual_block | ResidualModule            | 83.8 K\n",
      "3 | average_pool   | AvgPool1d                 | 0     \n",
      "4 | dropout        | Dropout                   | 0     \n",
      "5 | flatten        | Flatten                   | 0     \n",
      "6 | fc             | BasicFullyConnectedModule | 2.0 M \n",
      "-------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.320     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15103dc447749308845d19db0efb48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/vscode/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be41c66e0c334c92a7ae592ab87011c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a7e9ff522f4e0c9760a547f1ca3254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eu.train.fit(model, sdata, target_keys=\"activity_0\", epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
