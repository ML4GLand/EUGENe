import torch
import torch.nn as nn
import torch.nn.functional as F
from ..base import _layers as layers
from ..base import _blocks as blocks
from ..base import _towers as towers


class DeepBind(nn.Module):
    """DeepBind architecture implemented from Alipanahi et al 2015 in PyTorch

    This is a flexible implementation of the original DeepBind architecture that allows users
    to modify the number of convolutional layers, the number of fully connected layers, and
    many more hyperparameters. If parameters for the CNN and FCN are not passed in, the model
    will be instantiated with the parameters described in Alipanahi et al 2015.

    Like the original DeepBind models, this model can be used for both DNA and RNA binding. For DNA,
    we implemented the "dna" mode which only uses the max pooling of the representation generated by
    the convolutional layers. For RNA, we implemented the "rbp" mode which uses both the max and average
    pooling of the representation generated by the convolutional layers.

    Parameters
    ----------
    input_len : int
        Length of input sequence
    output_dim : int
        Number of output classes
    conv_kwargs : dict
        Keyword arguments for convolutional layers. These come from the
        models.Conv1DTower class. See the documentation for that class for more
        information on what arguments are available. If not specified,
        the default parameters from Alipanahi et al 2015 will be used.
    dense_kwargs : dict
        Keyword arguments for fully connected layers. These come from the
        models.DenseBlock class. See the documentation for that class for more
        information on what arguments are available. If not specified,
    mode : str
        Mode of model, either "dna" or "rbp". Controls the pooling layers.
        if "dna", only max pooling is used. If "rbp", both max and average
        pooling are used.
    """

    def __init__(
        self,
        input_len: int,
        output_dim: int,
        conv_kwargs: dict = {},
        dense_kwargs: dict = {},
        mode: str = "rbp",
    ):
        super(DeepBind, self).__init__()

        # Set the attributes
        self.input_len = input_len
        self.output_dim = output_dim
        self.mode = mode
        self.mode_dict = {"dna": 1, "rbp": 2}
        self.mode_multiplier = self.mode_dict[self.mode]
        self.conv_kwargs, self.dense_kwargs = self.kwarg_handler(
            conv_kwargs, dense_kwargs
        )

        # Create the blocks
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.pool_dim = self.conv1d_tower.output_len
        self.max_pool = nn.MaxPool1d(kernel_size=self.pool_dim)
        self.avg_pool = nn.AvgPool1d(kernel_size=self.pool_dim)
        self.dense_block = blocks.DenseBlock(
            input_dim=self.conv1d_tower.out_channels * self.mode_multiplier,
            output_dim=output_dim,
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        if self.mode == "rbp":
            x = torch.cat((self.max_pool(x), self.avg_pool(x)), dim=1)
            x = x.view(x.size(0), self.conv1d_tower.out_channels * 2)
        elif self.mode == "dna":
            x = self.max_pool(x)
            x = x.view(x.size(0), self.conv1d_tower.out_channels)
        x = self.dense_block(x)
        return x

    def kwarg_handler(self, conv_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [16])
        conv_kwargs.setdefault("conv_kernels", [16])
        conv_kwargs.setdefault("pool_types", None)
        conv_kwargs.setdefault("dropout_rates", 0.25)
        conv_kwargs.setdefault("batchnorm", False)
        dense_kwargs.setdefault("hidden_dims", [32])
        dense_kwargs.setdefault("dropout_rates", 0.25)
        dense_kwargs.setdefault("batchnorm", False)
        return conv_kwargs, dense_kwargs


class ResidualBind(nn.Module):
    """ResidualBind architecture implemented from Koo et al 2021 in PyTorch

    This is a flexible reimplementation of the original ResidualBind architecture
    that allows users to tweak  hyperparameters. If parameters for the CNN and FCN 
    are not passed in, the model will be instantiated with the parameters described 
    in Koo et al 2021.

    Parameters
    ----------
    input_len : int
        Length of input sequence
    output_dim : int
        Number of output neurons
    input_chanels : int, optional
        Number of input channels, by default 4
    conv_channels : list, optional
        Number of channels in the first convolutional layer, by default [96]
    conv_kernel_size : list, optional
        Kernel size of the first convolutional layer, by default [11]
    conv_stride_size : list, optional
        Stride size of the first convolutional layer, by default [1]
    conv_dilation_rate : list, optional
        Dilation rate of the first convolutional layer, by default [1]
    conv_padding : str, optional
        Padding of the first convolutional layer, by default "valid"
    conv_activation : str, optional
        Activation function of the first convolutional layer, by default "relu"
    conv_batchnorm : bool, optional
        Whether to use batchnorm in the first convolutional layer, by default True
    conv_batchnorm_first : bool, optional
        Whether to use batchnorm before or after the activation in the first convolutional layer, by default True
    conv_dropout_rates : float, optional
        Dropout rate of the first convolutional layer, by default 0.1
    conv_biases : bool, optional
        Whether to use biases in the first convolutional layer, by default False
    residual_channels : list, optional
        Number of channels in the residual blocks, by default [96, 96, 96]
    residual_kernel_size : list, optional
        Kernel size of the residual blocks, by default [3, 3, 3]
    residual_stride_size : list, optional
        Stride size of the residual blocks, by default [1, 1, 1]
    residual_dilation_rate : list, optional
        Dilation rate of the residual blocks, by default [1, 2, 4]
    residual_padding : str, optional
        Padding of the residual blocks, by default "same"
    residual_activation : str, optional
        Activation function of the residual blocks, by default "relu"
    residual_batchnorm : bool, optional
        Whether to use batchnorm in the residual blocks, by default True
    residual_batchnorm_first : bool, optional
        Whether to use batchnorm before or after the activation in the residual blocks, by default True
    residual_dropout_rates : float, optional
        Dropout rate of the residual blocks, by default 0.1
    residual_biases : bool, optional
        Whether to use biases in the residual blocks, by default False
    pool_kernel_size : int, optional
        Kernel size of the average pooling layer, by default 10
    pool_dropout_rate : float, optional
        Dropout rate of the average pooling layer, by default 0.2
    dense_hidden_dims : list, optional
        Number of neurons in the fully connected layers, by default [256]
    dense_activation : str, optional
        Activation function of the fully connected layers, by default "relu"
    dense_batchnorm : bool, optional
        Whether to use batchnorm in the fully connected layers, by default True
    dense_batchnorm_first : bool, optional
        Whether to use batchnorm before or after the activation in the fully connected layers, by default True
    dense_dropout_rates : float, optional
        Dropout rate of the fully connected layers, by default 0.5
    dense_biases : bool, optional
        Whether to use biases in the fully connected layers, by default False
    """
    def __init__(
        self,
        input_len,
        output_dim,
        input_chanels=4,
        conv_channels=[96],
        conv_kernel_size=[11],
        conv_stride_size=[1],
        conv_dilation_rate=[1],
        conv_padding="valid",
        conv_activation="relu",
        conv_batchnorm=True,
        conv_batchnorm_first=True,
        conv_dropout_rates=0.1,
        conv_biases=False,
        residual_channels=[96, 96, 96],
        residual_kernel_size=[3, 3, 3],
        residual_stride_size=[1, 1, 1],
        residual_dilation_rate=[1, 2, 4],
        residual_padding="same",
        residual_activation="relu",
        residual_batchnorm=True,
        residual_batchnorm_first=True,
        residual_dropout_rates=0.1,
        residual_biases=False,
        pool_kernel_size=10,
        pool_dropout_rate=0.2,
        dense_hidden_dims=[256],
        dense_activation="relu",
        dense_batchnorm=True,
        dense_batchnorm_first=True,
        dense_dropout_rates=0.5,
        dense_biases=False,
    ):
        super(ResidualBind, self).__init__()

        # Set the attributes
        self.input_len = input_len
        self.output_dim = output_dim
        if isinstance(conv_channels, int):
            conv_channels = [conv_channels]

        # Pass through normal conv
        self.conv1d_tower = towers.Conv1DTower(
            input_len=input_len,
            input_channels=input_chanels,
            conv_channels=conv_channels,
            conv_kernels=conv_kernel_size,
            conv_strides=conv_stride_size,
            conv_dilations=conv_dilation_rate,
            conv_padding=conv_padding,
            conv_biases=conv_biases,
            activations=conv_activation,
            pool_types=[None],
            dropout_rates=conv_dropout_rates,
            batchnorm=conv_batchnorm,
            batchnorm_first=conv_batchnorm_first,
        )

        # Pass through residual block
        res_block_input_len = self.conv1d_tower.output_len
        self.residual_block = layers.Residual(
            towers.Conv1DTower(
                input_len=res_block_input_len,
                input_channels=self.conv1d_tower.out_channels,
                conv_channels=residual_channels,
                conv_kernels=residual_kernel_size,
                conv_strides=residual_stride_size,
                conv_dilations=residual_dilation_rate,
                conv_padding=residual_padding,
                conv_biases=residual_biases,
                activations=residual_activation,
                pool_types=None,
                dropout_rates=residual_dropout_rates,
                batchnorm=residual_batchnorm,
                batchnorm_first=residual_batchnorm_first,
            )
        )
        self.average_pool = nn.AvgPool1d(pool_kernel_size, stride=1)
        self.dropout = nn.Dropout(pool_dropout_rate)
        self.flatten = nn.Flatten()
        self.dense_block = blocks.DenseBlock(
            input_dim=self.residual_block.wrapped.out_channels
            * (res_block_input_len - pool_kernel_size + 1),
            output_dim=output_dim,
            hidden_dims=dense_hidden_dims,
            activations=dense_activation,
            batchnorm=dense_batchnorm,
            batchnorm_first=dense_batchnorm_first,
            dropout_rates=dense_dropout_rates,
            biases=dense_biases,
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = self.residual_block(x)
        x = self.average_pool(x)
        x = self.dropout(x)
        x = self.flatten(x)
        x = self.dense_block(x)
        return x


class Kopp21CNN(nn.Module):
    """Custom convolutional model used in Kopp et al. 2021 paper

    PyTorch implementation of the TensorFlow model described here:
    https://github.com/wkopp/janggu_usecases/tree/master/01_jund_prediction

    Parameters
    ----------
    input_len : int
        Length of the input sequence.
    output_dim : int
        Dimension of the output.
    aggr : str, optional
        Aggregation method. Either "concat", "max", or "avg". By default "max" for this model.
    filters : list, optional
        Number of filters in the convolutional layers.
    conv_kernel_size : list, optional
        Kernel size of the convolutional layers.
    maxpool_kernel_size : int, optional
        Kernel size of the maxpooling layer.
    stride : int, optional
        Stride of the convolutional layers.
    """

    def __init__(
        self,
        input_len: int,
        output_dim: int,
        aggr: str = "max",
        filters: list = [10, 8],
        conv_kernel_size: list = [11, 3],
        maxpool_kernel_size: int = 30,
        stride: int = 1,
    ):
        super(Kopp21CNN, self).__init__()

        # Set the attributes
        self.input_len = input_len
        self.output_dim = output_dim
        self.aggr = aggr
        self.revcomp = layers.RevComp()
        self.conv = nn.Conv1d(4, filters[0], conv_kernel_size[0], stride=stride)
        self.maxpool = nn.MaxPool1d(kernel_size=maxpool_kernel_size, stride=stride)
        self.batchnorm = nn.BatchNorm1d(filters[0])
        self.conv2 = nn.Conv1d(
            filters[0], filters[1], conv_kernel_size[1], stride=stride
        )
        self.batchnorm2 = nn.BatchNorm1d(filters[1])
        self.linear = nn.Linear(filters[1], self.output_dim)

    def forward(self, x):
        x_rev_comp = self.revcomp(x)
        x_fwd = F.relu(self.conv(x))
        x_rev_comp = F.relu(self.conv(x_rev_comp))
        if self.aggr == "concat":
            x = torch.cat((x_fwd, x_rev_comp), dim=2)
        elif self.aggr == "max":
            x = torch.max(x_fwd, x_rev_comp)
        elif self.aggr == "avg":
            x = (x_fwd + x_rev_comp) / 2
        elif self.aggr is None:
            x = torch.cat((x_fwd, x_rev_comp), dim=1)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool1d(x, x.shape[2])
        x = self.batchnorm2(x)
        x = x.view(x.shape[0], -1)
        x = self.linear(x)
        return x
