import torch
import torch.nn as nn
import torch.nn.functional as F
from ..base import _layers as layers
from ..base import _blocks as blocks
from ..base import _towers as towers


class DeepBind(nn.Module):
    """
    DeepBind model implemented from Alipanahi et al 2015 in PyTorch

    DeepBind is a model that takes in a DNA or RNA sequence and outputs a probability of
    binding for a given DNA transcription factor or RNA binding protein respectively.
    This is a flexible implementation of the original DeepBind architecture that allows users
    to modify the number of convolutional layers, the number of fully connected layers, and
    many more hyperparameters. If parameters for the CNN and FCN are not passed in, the model
    will be instantiated with the parameters described in Alipanahi et al 2015.

    Like the original DeepBind models, this model can be used for both DNA and RNA binding. For DNA,
    we implemented the "dna" mode which only uses the max pooling of the representation generated by
    the convolutional layers. For RNA, we implemented the "rbp" mode which uses both the max and average
    pooling of the representation generated by the convolutional layers.

    Parameters
    ----------
    input_len : int
        Length of input sequence
    output_dim : int
        Number of output classes
    mode : str
        Mode of model, either "dna" or "rbp"
    strand : str
        Strand of model, either "ss", "ds", or "ts"
    task : str
        Task of model, either "regression" or "classification"
    aggr : str
        Aggregation method of model, either "max" or "avg"
    loss_fxn : str
        Loss function of model, either "mse" or "cross_entropy"
    optimizer : str
        Optimizer of model, either "adam" or "sgd"
    lr : float
        Learning rate of model
    scheduler : str
        Scheduler of model, either "lr_scheduler" or "plateau"
    scheduler_patience : int
        Scheduler patience of model
    mp_kwargs : dict
        Keyword arguments for multiprocessing
    conv_kwargs : dict
        Keyword arguments for convolutional layers
    dense_kwargs : dict
        Keyword arguments for fully connected layers
    """

    def __init__(
        self,
        input_len: int,
        output_dim: int,
        conv_kwargs: dict = {},
        dense_kwargs: dict = {},
        mode: str = "rbp",
    ):
        super(DeepBind, self).__init__()

        # Set the attributes
        self.input_len = input_len
        self.output_dim = output_dim
        self.mode = mode
        self.mode_dict = {"dna": 1, "rbp": 2}
        self.mode_multiplier = self.mode_dict[self.mode]
        self.conv_kwargs, self.dense_kwargs = self.kwarg_handler(
            conv_kwargs, dense_kwargs
        )

        # Create the blocks
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.pool_dim = self.conv1d_tower.output_len
        self.max_pool = nn.MaxPool1d(kernel_size=self.pool_dim)
        self.avg_pool = nn.AvgPool1d(kernel_size=self.pool_dim)
        self.dense_block = blocks.DenseBlock(
            input_dim=self.conv1d_tower.out_channels * self.mode_multiplier,
            output_dim=output_dim,
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        if self.mode == "rbp":
            x = torch.cat((self.max_pool(x), self.avg_pool(x)), dim=1)
            x = x.view(x.size(0), self.conv1d_tower.out_channels * 2)
        elif self.mode == "dna":
            x = self.max_pool(x)
            x = x.view(x.size(0), self.conv1d_tower.out_channels)
        x = self.dense_block(x)
        return x

    def kwarg_handler(self, conv_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [16])
        conv_kwargs.setdefault("conv_kernels", [16])
        conv_kwargs.setdefault("pool_types", None)
        conv_kwargs.setdefault("dropout_rates", 0.25)
        conv_kwargs.setdefault("batchnorm", False)
        dense_kwargs.setdefault("hidden_dims", [32])
        dense_kwargs.setdefault("dropout_rates", 0.25)
        dense_kwargs.setdefault("batchnorm", False)
        return conv_kwargs, dense_kwargs


class ResidualBind(nn.Module):
    # TODO: clean this up
    def __init__(
        self,
        input_len,
        output_dim,
        input_chanels=4,
        conv_channels=[96],
        conv_kernel_size=[11],
        conv_stride_size=[1],
        conv_dilation_rate=[1],
        conv_padding="valid",
        conv_activation="relu",
        conv_batchnorm=True,
        conv_batchnorm_first=True,
        conv_dropout_rates=0.1,
        conv_biases=False,
        residual_channels=[96, 96, 96],
        residual_kernel_size=[3, 3, 3],
        residual_stride_size=[1, 1, 1],
        residual_dilation_rate=[1, 2, 4],
        residual_padding="same",
        residual_activation="relu",
        residual_batchnorm=True,
        residual_batchnorm_first=True,
        residual_dropout_rates=0.1,
        residual_biases=False,
        pool_kernel_size=10,
        pool_dropout_rate=0.2,
        dense_hidden_dims=[256],
        dense_activation="relu",
        dense_batchnorm=True,
        dense_batchnorm_first=True,
        dense_dropout_rates=0.5,
        dense_biases=False,
    ):
        super(ResidualBind, self).__init__()

        # Set the attributes
        self.input_len = input_len
        self.output_dim = output_dim
        if isinstance(conv_channels, int):
            conv_channels = [conv_channels]

        # Pass through normal conv
        self.conv1d_tower = towers.Conv1DTower(
            input_len=input_len,
            input_channels=input_chanels,
            conv_channels=conv_channels,
            conv_kernels=conv_kernel_size,
            conv_strides=conv_stride_size,
            conv_dilations=conv_dilation_rate,
            conv_padding=conv_padding,
            conv_biases=conv_biases,
            activations=conv_activation,
            pool_types=[None],
            dropout_rates=conv_dropout_rates,
            batchnorm=conv_batchnorm,
            batchnorm_first=conv_batchnorm_first,
        )

        # Pass through residual block
        res_block_input_len = self.conv1d_tower.output_len
        self.residual_block = layers.Residual(
            towers.Conv1DTower(
                input_len=res_block_input_len,
                input_channels=self.conv1d_tower.out_channels,
                conv_channels=residual_channels,
                conv_kernels=residual_kernel_size,
                conv_strides=residual_stride_size,
                conv_dilations=residual_dilation_rate,
                conv_padding=residual_padding,
                conv_biases=residual_biases,
                activations=residual_activation,
                pool_types=None,
                dropout_rates=residual_dropout_rates,
                batchnorm=residual_batchnorm,
                batchnorm_first=residual_batchnorm_first,
            )
        )
        self.average_pool = nn.AvgPool1d(pool_kernel_size, stride=1)
        self.dropout = nn.Dropout(pool_dropout_rate)
        self.flatten = nn.Flatten()
        self.dense_block = blocks.DenseBlock(
            input_dim=self.residual_block.wrapped.out_channels
            * (res_block_input_len - pool_kernel_size + 1),
            output_dim=output_dim,
            hidden_dims=dense_hidden_dims,
            activations=dense_activation,
            batchnorm=dense_batchnorm,
            batchnorm_first=dense_batchnorm_first,
            dropout_rates=dense_dropout_rates,
            biases=dense_biases,
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = self.residual_block(x)
        x = self.average_pool(x)
        x = self.dropout(x)
        x = self.flatten(x)
        x = self.dense_block(x)
        return x


class Kopp21CNN(nn.Module):
    """
    Custom convolutional model used in Kopp et al. 2021 paper

    PyTorch implementation of the TensorFlow model described here:
    https://github.com/wkopp/janggu_usecases/tree/master/01_jund_prediction

    This model can only be run in "ds" mode. The reverse complement must be included in the Dataloader
    Parameters
    ----------
    input_len : int
        Length of the input sequence.
    output_dim : int
        Dimension of the output.
    strand : str, optional
        Strand of the input. This model is only implemented for "ds"
    task : str, optional
        Task for this model. By default "binary_classification" for this mode
    aggr : str, optional
        Aggregation method. Either "concat", "max", or "avg". By default "max" for this model.
    filters : list, optional
        Number of filters in the convolutional layers.
    conv_kernel_size : list, optional
        Kernel size of the convolutional layers.
    maxpool_kernel_size : int, optional
        Kernel size of the maxpooling layer.
    stride : int, optional
        Stride of the convolutional layers.
    """

    def __init__(
        self,
        input_len: int,
        output_dim: int,
        aggr: str = "max",
        filters: list = [10, 8],
        conv_kernel_size: list = [11, 3],
        maxpool_kernel_size: int = 30,
        stride: int = 1,
    ):
        super(Kopp21CNN, self).__init__()

        # Set the attributes
        self.input_len = input_len
        self.output_dim = output_dim
        self.aggr = aggr
        self.revcomp = layers.RevComp()
        self.conv = nn.Conv1d(4, filters[0], conv_kernel_size[0], stride=stride)
        self.maxpool = nn.MaxPool1d(kernel_size=maxpool_kernel_size, stride=stride)
        self.batchnorm = nn.BatchNorm1d(filters[0])
        self.conv2 = nn.Conv1d(
            filters[0], filters[1], conv_kernel_size[1], stride=stride
        )
        self.batchnorm2 = nn.BatchNorm1d(filters[1])
        self.linear = nn.Linear(filters[1], self.output_dim)

    def forward(self, x):
        x_rev_comp = self.revcomp(x)
        x_fwd = F.relu(self.conv(x))
        x_rev_comp = F.relu(self.conv(x_rev_comp))
        if self.aggr == "concat":
            x = torch.cat((x_fwd, x_rev_comp), dim=2)
        elif self.aggr == "max":
            x = torch.max(x_fwd, x_rev_comp)
        elif self.aggr == "avg":
            x = (x_fwd + x_rev_comp) / 2
        elif self.aggr is None:
            x = torch.cat((x_fwd, x_rev_comp), dim=1)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool1d(x, x.shape[2])
        x = self.batchnorm2(x)
        x = x.view(x.shape[0], -1)
        x = self.linear(x)
        return x
