import torch
import torch.nn as nn
import torch.nn.functional as F
from .base import SequenceModel 
from .base import _layers as layers
from .base import _blocks as blocks
from .base import _towers as towers

# TF binding predictors
class DeepBind(SequenceModel):
    """
    DeepBind model implemented from Alipanahi et al 2015 in PyTorch

    DeepBind is a model that takes in a DNA or RNA sequence and outputs a probability of 
    binding for a given DNA transcription factor or RNA binding protein respectively.
    This is a flexible implementation of the original DeepBind architecture that allows users
    to modify the number of convolutional layers, the number of fully connected layers, and
    many more hyperparameters. If parameters for the CNN and FCN are not passed in, the model
    will be instantiated with the parameters described in Alipanahi et al 2015.

    Like the original DeepBind models, this model can be used for both DNA and RNA binding. For DNA,
    we implemented the "dna" mode which only uses the max pooling of the representation generated by 
    the convolutional layers. For RNA, we implemented the "rbp" mode which uses both the max and average
    pooling of the representation generated by the convolutional layers.

    Parameters
    ----------
    input_len : int
        Length of input sequence
    output_dim : int
        Number of output classes
    mode : str
        Mode of model, either "dna" or "rbp"
    strand : str
        Strand of model, either "ss", "ds", or "ts"
    task : str
        Task of model, either "regression" or "classification"
    aggr : str
        Aggregation method of model, either "max" or "avg"
    loss_fxn : str
        Loss function of model, either "mse" or "cross_entropy"
    optimizer : str
        Optimizer of model, either "adam" or "sgd"
    lr : float
        Learning rate of model
    scheduler : str
        Scheduler of model, either "lr_scheduler" or "plateau"
    scheduler_patience : int
        Scheduler patience of model
    mp_kwargs : dict
        Keyword arguments for multiprocessing
    conv_kwargs : dict
        Keyword arguments for convolutional layers
    dense_kwargs : dict
        Keyword arguments for fully connected layers
    """
    def __init__(
        self,
        input_len: int,
        output_dim: int,
        conv_kwargs: dict = {},
        dense_kwargs: dict = {},
        task: str = "regression",
        loss_fxn: str ="mse",
        mode: str = "rbp",
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task, 
            loss_fxn=loss_fxn, 
            **kwargs
        )
        self.mode = mode
        self.mode_dict = {"dna": 1, "rbp": 2}
        self.mode_multiplier = self.mode_dict[self.mode]
        self.conv_kwargs, self.dense_kwargs = self.kwarg_handler(conv_kwargs, dense_kwargs)
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.pool_dim = self.conv1d_tower.output_len
        self.max_pool = nn.MaxPool1d(kernel_size=self.pool_dim)
        self.avg_pool = nn.AvgPool1d(kernel_size=self.pool_dim)
        self.dense_block = blocks.DenseBlock(
            input_dim=self.conv1d_tower.out_channels * self.mode_multiplier,
            output_dim=output_dim,
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        if self.mode == "rbp":
            x = torch.cat((self.max_pool(x), self.avg_pool(x)), dim=1)
            x = x.view(x.size(0), self.conv1d_tower.out_channels * 2)
        elif self.mode == "dna":
            x = self.max_pool(x)
            x = x.view(x.size(0), self.conv1d_tower.out_channels)
        x = self.dense_block(x)
        return x

    def kwarg_handler(self, conv_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [16])
        conv_kwargs.setdefault("conv_kernels", [16])
        conv_kwargs.setdefault("pool_types", None)
        conv_kwargs.setdefault("dropout_rates", 0.25)
        conv_kwargs.setdefault("batchnorm", False)
        dense_kwargs.setdefault("hidden_dims", [32])
        dense_kwargs.setdefault("dropout_rates", 0.25)
        dense_kwargs.setdefault("batchnorm", False)
        return conv_kwargs, dense_kwargs 

class ResidualBind( SequenceModel):
    def __init__(
        self,
        input_len,
        output_dim,
        task="regression",
        input_chanels=4,
        conv_channels=[96],
        conv_kernel_size=[11],
        conv_stride_size=[1],
        conv_dilation_rate=[1],
        conv_padding="valid",
        conv_activation="relu",
        conv_batchnorm=True,
        conv_batchnorm_first=True,
        conv_dropout_rates=0.1,
        conv_biases=False,
        residual_channels=[96, 96, 96],
        residual_kernel_size=[3, 3, 3],
        residual_stride_size=[1, 1, 1],
        residual_dilation_rate=[1, 2, 4],
        residual_padding="same",
        residual_activation="relu",
        residual_batchnorm=True,
        residual_batchnorm_first=True,
        residual_dropout_rates=0.1,
        residual_biases=False,
        pool_kernel_size=10,
        pool_dropout_rate=0.2,
        dense_hidden_dims=[256],
        dense_activation="relu",
        dense_batchnorm=True,
        dense_batchnorm_first=True,
        dense_dropout_rates=0.5,
        dense_biases=False,
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task,
            **kwargs
        )
        if isinstance(conv_channels, int):
            conv_channels = [conv_channels]
        
        # Pass through normal conv
        self.conv1d_tower = towers.Conv1DTower(
            input_len=input_len,
            input_channels=input_chanels,
            conv_channels=conv_channels,
            conv_kernels=conv_kernel_size,
            conv_strides=conv_stride_size,
            conv_dilations=conv_dilation_rate,
            conv_padding=conv_padding,
            conv_biases=conv_biases,
            activations=conv_activation,
            pool_types=[None],
            dropout_rates=conv_dropout_rates,
            batchnorm=conv_batchnorm,
            batchnorm_first=conv_batchnorm_first
        )
        
        # Pass through residual block
        res_block_input_len = self.conv1d_tower.output_len
        self.residual_block = layers.Residual(
            towers.Conv1DTower(
                input_len=res_block_input_len,
                input_channels=self.conv1d_tower.out_channels,
                conv_channels=residual_channels,
                conv_kernels=residual_kernel_size,
                conv_strides=residual_stride_size,
                conv_dilations=residual_dilation_rate,
                conv_padding=residual_padding,
                conv_biases=residual_biases,
                activations=residual_activation,
                pool_types=None,
                dropout_rates=residual_dropout_rates,
                batchnorm=residual_batchnorm,
                batchnorm_first=residual_batchnorm_first
                )
        )
        self.average_pool = nn.AvgPool1d(pool_kernel_size, stride=1)
        self.dropout = nn.Dropout(pool_dropout_rate)
        self.flatten = nn.Flatten()
        self.dense_block = blocks.DenseBlock(
            input_dim=self.residual_block.wrapped.out_channels*(res_block_input_len-pool_kernel_size+1),
            output_dim=output_dim,
            hidden_dims=dense_hidden_dims,
            activations=dense_activation,
            batchnorm=dense_batchnorm,
            batchnorm_first=dense_batchnorm_first,
            dropout_rates=dense_dropout_rates,
            biases=dense_biases
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = self.residual_block(x)
        x = self.average_pool(x)
        x = self.dropout(x)
        x = self.flatten(x)
        x = self.dense_block(x)
        return x

class Kopp21CNN(SequenceModel):
    """
    Custom convolutional model used in Kopp et al. 2021 paper

    PyTorch implementation of the TensorFlow model described here:
    https://github.com/wkopp/janggu_usecases/tree/master/01_jund_prediction

    This model can only be run in "ds" mode. The reverse complement must be included in the Dataloader
    Parameters
    ----------
    input_len : int
        Length of the input sequence.
    output_dim : int
        Dimension of the output.
    strand : str, optional
        Strand of the input. This model is only implemented for "ds"
    task : str, optional
        Task for this model. By default "binary_classification" for this mode
    aggr : str, optional
        Aggregation method. Either "concat", "max", or "avg". By default "max" for this model.
    filters : list, optional
        Number of filters in the convolutional layers. 
    conv_kernel_size : list, optional
        Kernel size of the convolutional layers.
    maxpool_kernel_size : int, optional
        Kernel size of the maxpooling layer.
    stride : int, optional
        Stride of the convolutional layers.
    """
    def __init__(
        self,
        input_len: int,
        output_dim: int,
        task: str = "binary_classification",
        loss_fxn: str = "bce",
        aggr: str = "max",
        filters: list = [10, 8],
        conv_kernel_size: list = [11, 3],
        maxpool_kernel_size: int = 30,
        stride: int = 1,
        **kwargs
    ):
        super().__init__(
            input_len,
            output_dim,
            task=task,
            loss_fxn=loss_fxn,
            **kwargs
        )
        self.aggr = aggr
        self.revcomp = layers.RevComp()
        self.conv = nn.Conv1d(4, filters[0], conv_kernel_size[0], stride=stride)
        self.maxpool = nn.MaxPool1d(kernel_size=maxpool_kernel_size, stride=stride)
        self.batchnorm = nn.BatchNorm1d(filters[0])
        self.conv2 = nn.Conv1d(filters[0], filters[1], conv_kernel_size[1], stride=stride)
        self.batchnorm2 = nn.BatchNorm1d(filters[1])
        self.linear = nn.Linear(filters[1], self.output_dim)

    def forward(self, x):
        x_rev_comp = self.revcomp(x)
        x_fwd = F.relu(self.conv(x))
        x_rev_comp = F.relu(self.conv(x_rev_comp))
        if self.aggr == "concat":
            x = torch.cat((x_fwd, x_rev_comp), dim=2)
        elif self.aggr == "max":
            x = torch.max(x_fwd, x_rev_comp)
        elif self.aggr == "avg":
            x = (x_fwd + x_rev_comp) / 2
        elif self.aggr is None:
            x = torch.cat((x_fwd, x_rev_comp), dim=1)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool1d(x, x.shape[2])
        x = self.batchnorm2(x)
        x = x.view(x.shape[0], -1)
        x = self.linear(x)
        return x

# Multi-task regulatory feature classifiers
class DeepSEA(SequenceModel):
    """DeepSEA model implementation for EUGENe
    
    Default parameters are those specified in the DeepSEA paper. We currently do not implement a "ds" or "ts" model
    for DeepSEA.

    Parameters
    ----------
    input_len:
        int, input sequence length
    channels:
        list-like or int, channel width for each conv layer. If int each of the three layers will be the same channel width
    conv_kernels:
        list-like or int, conv kernel size for each conv layer. If int will be the same for all conv layers
    pool_kernels:
        list-like or int, maxpooling kernel size for the first two conv layers. If int will be the same for all conv layers
    dropout_rates:
        list-like or float, dropout rates for each conv layer. If int will be the same for all conv layers
    """
    def __init__(
        self,
        input_len: int,
        output_dim: int,
        conv_kwargs: dict = {},
        dense_kwargs: dict = {},
        task: str = "regression",
        loss_fxn: str = "mse",
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task, 
            loss_fxn=loss_fxn, 
            **kwargs
        )
        self.conv_kwargs, self.dense_kwargs = self.kwarg_handler(conv_kwargs, dense_kwargs)
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.dense_block = blocks.DenseBlock(
            input_dim=self.conv1d_tower.flatten_dim, 
            output_dim=output_dim, 
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = x.view(x.size(0), self.conv1d_tower.flatten_dim)
        x = self.dense_block(x)
        return x

    def kwarg_handler(self, conv_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [320, 480, 960])
        conv_kwargs.setdefault("conv_kernels", [8, 8, 8])
        conv_kwargs.setdefault("pool_types", ["max", "max", None])
        conv_kwargs.setdefault("pool_kernels", [4, 4, None])
        conv_kwargs.setdefault("activations", "relu")
        conv_kwargs.setdefault("dropout_rates", [0.2, 0.2, 0.5])
        conv_kwargs.setdefault("batchnorm", False)
        dense_kwargs.setdefault("hidden_dims", [925])
        return conv_kwargs,dense_kwargs 

class Basset(SequenceModel):
    """
    """
    def __init__(
        self, 
        input_len: int,
        output_dim: int, 
        task = "multilabel_classification",
        loss_fxn = "bce",
        conv_kwargs = {},
        dense_kwargs = {},
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task, 
            loss_fxn=loss_fxn, 
            **kwargs
        )
        self.conv_kwargs, self.dense_kwargs = self.kwarg_handler(conv_kwargs, dense_kwargs)
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.dense_block = blocks.DenseBlock(
            input_dim=self.conv1d_tower.flatten_dim, 
            output_dim=output_dim, 
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = x.view(x.size(0), self.conv1d_tower.flatten_dim)
        x = self.dense_block(x)
        return x
        
    def kwarg_handler(self, conv_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [300, 200, 200])
        conv_kwargs.setdefault("conv_kernels", [19, 11, 7])
        conv_kwargs.setdefault("conv_strides", [1, 1, 1])
        conv_kwargs.setdefault("conv_padding", [9, 5, 3])
        conv_kwargs.setdefault("pool_kernels", [3, 4, 4])
        conv_kwargs.setdefault("dropout_rates", 0.0)
        conv_kwargs.setdefault("activations", "relu")
        conv_kwargs.setdefault("batchnorm", True)
        conv_kwargs.setdefault("batchnorm_first", True)
        dense_kwargs.setdefault("hidden_dims", [1000, 164])
        dense_kwargs.setdefault("dropout_rates", 0.0)
        dense_kwargs.setdefault("batchnorm", True)
        dense_kwargs.setdefault("batchnorm_first", True)
        dense_kwargs.setdefault("activations", "relu")
        return conv_kwargs,dense_kwargs 

class FactorizedBasset(SequenceModel):
	def __init__(
		self, 
		input_len: int = 1000,
		output_dim = 1, 
        conv1_kwargs = {},
		conv2_kwargs = {},
		conv3_kwargs = {},
		maxpool_kernels = None,
		dense_kwargs = {},
		task = "binary_classification",
		loss_fxn = "bce",
		**kwargs
	):
		super().__init__(
			input_len, 
			output_dim, 
			task=task, 
			loss_fxn=loss_fxn, 
			**kwargs
		)
		self.conv1_kwargs, self.conv2_kwargs, self.conv3_kwargs, self.maxpool_kernels, self.dense_kwargs = self.kwarg_handler(
			conv1_kwargs, 
			conv2_kwargs, 
			conv3_kwargs, 
			maxpool_kernels, 
		    dense_kwargs	
		)
		self.conv1d_tower1 = towers.Conv1DTower(
			input_len=input_len, 
            input_channels=4,
			**self.conv1_kwargs
		)
		self.conv1d_tower2 = towers.Conv1DTower(
			input_len=self.conv1d_tower1.output_len,
            input_channels=self.conv1d_tower1.out_channels,
			**self.conv2_kwargs
		)
		self.conv1d_tower3 = towers.Conv1DTower(
			input_len=self.conv1d_tower2.output_len,
            input_channels=self.conv1d_tower2.out_channels,
			**self.conv3_kwargs
		)
		self.dense_block = blocks.DenseBlock(
			input_dim=self.conv1d_tower3.flatten_dim,
			output_dim=output_dim, 
			**self.dense_kwargs
		)

	def forward(self, x):
		x = self.conv1d_tower1(x)
		x = self.conv1d_tower2(x)
		x = self.conv1d_tower3(x)
		x = x.view(x.size(0), self.conv1d_tower3.flatten_dim)
		x = self.dense_block(x)
		return x
        
	def kwarg_handler(self, conv1_kwargs, conv2_kwargs, conv3_kwargs, maxpool_kernels, dense_kwargs):
		"""Sets default kwargs FactorizedBasset"""
		conv1_kwargs.setdefault("conv_channels", [48, 64, 100, 150, 300])
		conv1_kwargs.setdefault("conv_kernels", [3, 3, 3, 7, 7])
		conv1_kwargs.setdefault("conv_strides", [1, 1, 1, 1, 1])
		conv1_kwargs.setdefault("conv_padding", [1, 1, 1, 3, 3])
		conv1_kwargs.setdefault("pool_types", [None, None, None, None, "max"])
		conv1_kwargs.setdefault("pool_kernels", [None, None, None, None, 3])
		conv1_kwargs.setdefault("dropout_rates", 0.0)
		conv1_kwargs.setdefault("batchnorm", True)
		conv1_kwargs.setdefault("batchnorm_first", True)
		conv1_kwargs.setdefault("activations", "relu")
		conv2_kwargs.setdefault("conv_channels", [200, 200, 200])
		conv2_kwargs.setdefault("conv_kernels", [7, 3, 3])
		conv2_kwargs.setdefault("conv_strides", [1, 1, 1])
		conv2_kwargs.setdefault("conv_padding", [3, 1, 1])
		conv2_kwargs.setdefault("pool_types", [None, None, "max"])
		conv2_kwargs.setdefault("pool_kernels", [None, None, 4])
		conv2_kwargs.setdefault("dropout_rates", 0.0)
		conv2_kwargs.setdefault("batchnorm", True)
		conv2_kwargs.setdefault("batchnorm_first", True)
		conv2_kwargs.setdefault("activations", "relu")
		conv3_kwargs.setdefault("conv_channels", [200])
		conv3_kwargs.setdefault("conv_kernels", [7])
		conv3_kwargs.setdefault("conv_strides", [1])
		conv3_kwargs.setdefault("conv_padding", [3])
		conv3_kwargs.setdefault("pool_types", ["max"])
		conv3_kwargs.setdefault("pool_kernels", [4])
		conv3_kwargs.setdefault("dropout_rates", 0.0)
		conv3_kwargs.setdefault("batchnorm", True)
		conv3_kwargs.setdefault("batchnorm_first", True)
		conv3_kwargs.setdefault("activations", "relu")
		dense_kwargs.setdefault("hidden_dims", [1000, 164])
		dense_kwargs.setdefault("dropout_rates", 0.0)
		dense_kwargs.setdefault("batchnorm", True)
		dense_kwargs.setdefault("batchnorm_first", True)
		dense_kwargs.setdefault("activations", "relu")
		return conv1_kwargs, conv2_kwargs, conv3_kwargs, maxpool_kernels,dense_kwargs 

class DanQ(SequenceModel):
    """DanQ model from Quang and Xie, 2016;

    Parameters
    ----------
    input_len:
        The length of the input sequence.
    output_dim:
        The dimension of the output.
    task:
        The task of the model.
    dense_kwargs:
        The keyword arguments for the fully connected layer.
    """
    def __init__(
        self,
        input_len: int,
        output_dim: int,
        conv_kwargs: dict = {},
        recurrent_kwargs: dict = {},
        dense_kwargs: dict = {},
        task: str = "regression",
        loss_fxn: str = "mse",
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task, 
            loss_fxn=loss_fxn, 
            **kwargs
        ) 
        self.conv_kwargs, self.recurrent_kwargs, self.dense_kwargs = self.kwarg_handler(
            conv_kwargs, recurrent_kwargs, dense_kwargs
        )
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.recurrent_block = blocks.RecurrentBlock(
            input_dim=self.conv1d_tower.out_channels, 
            **self.recurrent_kwargs
        )
        self.dense_block = blocks.DenseBlock(
            input_dim=self.recurrent_block.out_channels,
            output_dim=output_dim, 
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = x.transpose(1, 2)
        out, _ = self.recurrent_block(x)
        out = self.dense_block(out[:, -1, :])
        return out

    def kwarg_handler(self, conv_kwargs, recurrent_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [320])
        conv_kwargs.setdefault("conv_kernels", [26])
        conv_kwargs.setdefault("conv_strides", [1])
        conv_kwargs.setdefault("conv_padding", "same")
        conv_kwargs.setdefault("pool_kernels", [13])
        conv_kwargs.setdefault("dropout_rates", 0.2)
        conv_kwargs.setdefault("activations", "relu")
        recurrent_kwargs.setdefault("unit_type", "lstm")
        recurrent_kwargs.setdefault("hidden_dim", 320)
        recurrent_kwargs.setdefault("bidirectional", True)
        recurrent_kwargs.setdefault("batch_first", True)
        dense_kwargs.setdefault("hidden_dims", [925])
        dense_kwargs.setdefault("dropout_rates", 0.5)
        dense_kwargs.setdefault("batchnorm", False)
        return conv_kwargs, recurrent_kwargs, dense_kwargs

# CRE activity predictors
class Jores21CNN(SequenceModel):
    """
    Custom convolutional model used in Jores et al. 2021 paper

    PyTorch implementation of the TensorFlow model described here:
    https://github.com/tobjores/Synthetic-Promoter-Designs-Enabled-by-a-Comprehensive-Analysis-of-Plant-Core-Promoters

    This model only uses a single strand, but applies convolutions and the reverse complement of the convolutional fitler
    to the same sequence.

    Parameters
    ----------
    input_len : int
        Length of the input sequence.
    output_dim : int
        Dimension of the output.
    strand : str, optional
        Strand of the input. Only ss is supported for this model
    task : str, optional
        Task of the model. Either "regression" or "classification".
    aggr : str, optional
        Aggregation method. Does not apply to this model and will be ignored
    filters : int, optional
        Number of filters in the convolutional layers.
    kernel_size : int, optional
        Kernel size of the convolutional layers.
    layers : int, optional
        Number of convolutional layers.
    stride : int, optional
        Stride of the convolutional layers.
    dropout : float, optional
        Dropout probability.
    hidden_dim : int, optional
        Dimension of the hidden layer.
    """
    def __init__(
        self,
        input_len: int,
        output_dim: int,
        task: str = "regression",
        loss_fxn: str = "mse",
        filters: int = 128,
        kernel_size: int = 13,
        layers: int = 2,
        stride: int = 1,
        dropout: float = 0.15,
        hidden_dim: int = 64,
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task,  
            loss_fxn=loss_fxn,
            **kwargs
        )
        self.biconv = towers.BiConv1DTower(
            filters=filters,
            kernel_size=kernel_size,
            layers=layers,
            stride=stride,
            dropout_rate=dropout,
        )
        self.conv = nn.Conv1d(
            in_channels=filters,
            out_channels=filters,
            kernel_size=kernel_size,
            stride=stride,
            padding="same",
        )
        self.dropout = nn.Dropout(p=dropout)
        self.fc = nn.Linear(in_features=input_len * filters, out_features=hidden_dim)
        self.batchnorm = nn.BatchNorm1d(num_features=hidden_dim)
        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=output_dim)

    def forward(self, x, x_rev_comp=None):
        x = self.biconv(x)
        x = self.conv(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc(x.view(x.shape[0], -1))
        x = self.batchnorm(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

class DeepSTARR(SequenceModel):
    """DeepSTARR model from de Almeida et al., 2022; see <https://www.nature.com/articles/s41588-022-01048-5>

    Parameters
    """
    def __init__(
        self, 
        input_len: int,
        output_dim: int, 
        task = "regression",
        loss_fxn = "mse",
        conv_kwargs = {},
        dense_kwargs = {},
        **kwargs
    ):
        super().__init__(
            input_len, 
            output_dim, 
            task=task, 
            loss_fxn=loss_fxn, 
            **kwargs
        )
        self.conv_kwargs, self.dense_kwargs = self.kwarg_handler(conv_kwargs, dense_kwargs)
        self.conv1d_tower = towers.Conv1DTower(**self.conv_kwargs)
        self.dense_block = blocks.DenseBlock(
            input_dim=self.conv1d_tower.flatten_dim, 
            output_dim=output_dim, 
            **self.dense_kwargs
        )

    def forward(self, x):
        x = self.conv1d_tower(x)
        x = x.view(x.size(0), self.conv1d_tower.flatten_dim)
        x = self.dense_block(x)
        return x
        
    def kwarg_handler(self, conv_kwargs, dense_kwargs):
        """Sets default kwargs for conv and fc modules if not specified"""
        conv_kwargs.setdefault("input_len", self.input_len)
        conv_kwargs.setdefault("input_channels", 4)
        conv_kwargs.setdefault("conv_channels", [246, 60, 60, 120])
        conv_kwargs.setdefault("conv_kernels", [7, 3, 5, 3])
        conv_kwargs.setdefault("conv_strides", [1, 1, 1, 1])
        conv_kwargs.setdefault("conv_padding", "same")
        conv_kwargs.setdefault("pool_kernels", [2, 2, 2, 2])
        conv_kwargs.setdefault("dropout_rates", 0.0)
        conv_kwargs.setdefault("batchnorm", True)
        conv_kwargs.setdefault("batchnorm_first", True)
        dense_kwargs.setdefault("hidden_dims", [256, 256])
        dense_kwargs.setdefault("dropout_rates", 0.4)
        dense_kwargs.setdefault("batchnorm", True)
        dense_kwargs.setdefault("batchnorm_first", True)
        return conv_kwargs, dense_kwargs 
    