{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a88efe-ebb3-424c-8704-fb4ee189af2c",
   "metadata": {},
   "source": [
    "# Adding a model to EUGENe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d058003-a667-4b45-9ddc-e47efe48e0e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *10/05/2022*<br>\n",
    "**Last Updated**: 10/08/2022\n",
    "***\n",
    "**Description:**\n",
    "EUGENe offers several customizable architectures as built-in, including flexible fully connected, convolutional, recurrent, hybrid architectures and seminal DeepBind and DeepSEA architectures. We also provide implementations of models introduced in Jores et al and Kopp et al. However, this set of provided modules may not be sufficient for a users training task and many users may need to add custom architectures to the library. This can be achieved in a few straightforward steps outlined below. We also provide a walkthrough on how to add a model on the EUGENe readthedocs page and available on GitHub.\n",
    "\n",
    "This tutorial is intended to show how to add a model to EUGENe. It's a pretty simple process and allows the model to be utiilized throughout the EUGENe pipeline.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eugene as eu\n",
    "\n",
    "eu.settings.dataset_dir = \"./tutorial_datasets\"\n",
    "eu.settings.logging_dir = \"./tutorial_logs\"\n",
    "eu.settings.output_dir = \"./tutorial_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Review the `BaseModel` class and check out some examples in the `_custom_models.py` file\n",
    "In order to fully integrate models into the EUGENe pipeline, it is recommended that you make your model a subclass of the [`BaseModel` class](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#basemodel-a-pytorch-lightning-template-for-deep-models). Though many of EUGENe's functions work under the assumption that the model is a subclass of a `torch.nn.Module`, many other functions assume a structure dictated by the `BaseModel` class. For most of the rest of this tutorial, we assume that you are inheriting from `BaseModel`.\n",
    "\n",
    "Before you begin implementing anything it is recommended that you take a look at the (`BaseModel` class attributes)[https://github.com/adamklie/EUGENe/blob/main/eugene/models/base/_base_model.py]. These are the attributes that you will need to instantiate for any EUGENe model. I also find that it helps to see a few examples, which you can find in the [`_custom_models.py` file](https://github.com/adamklie/EUGENe/blob/main/eugene/models/_custom_models.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a model class\n",
    "* This should be a Python class that at the very least inherits from `torch.nn.Module` (but ideally should inherit from the `BaseModel` class).\n",
    "\n",
    "* For naming your model class, use the last name of the first author followed by the year of publication (`NameYY`) if your model is associated with a publication. It can also be useful to add the type of model you are implementing. For example, if the model is a CNN that was published in 2021 by the author \"Jane Doe\", the function should be named `Doe21CNN`. If your model is not associated with a publication, feel free to come up with your own name, but use the camel case convention (`ModelName`).\n",
    "* At the minimum, this class should contain two functions, an  `__init__()` and a `forward()`.\n",
    "* The `__init__()` function will set-up the way the model architecture is initialized. To use BaseModel functionality, a  user must first make a call to `super.init()` in the first line. The BaseModel class expects the user to include:\n",
    "    \n",
    "    - **input_len**: Expected input length\n",
    "        - In most cases, this should be the length of the longest input sequence. See the `preprocess` module for more details on how different length inputs are handled.\n",
    "    \n",
    "    <br>\n",
    "\n",
    "    - **output_dim**: The expected output dimension\n",
    "        - The number of output neurons. One for single task regression and binary classification, multiple for multi-task regression, and the number of classes for multi-class classification.\n",
    "    \n",
    "    <br>\n",
    "\n",
    "    - **strand**: The input type broken into three categories (described below)\n",
    "        - *ss*: or single stranded models only take in one direction of the double stranded DNA (usually the 5’—>3’ direction)\n",
    "        - *ds*: or double stranded models ingest both the forward and reverse strand (3’—>5’ reverse complement of forward) through the same set of layers. They aggregate the representations from these inputs according to the `aggr` argument and the error is backpropogated through this shared architecture\n",
    "        - *ts*: or twin stranded models ingest both the forward and reverse strand (3’—>5’ reverse complement of forward) through a two sets of identically shaped layers. That is, two separate twin models handle each input and the representation learned from these different architectures is aggregated according to `aggr`.\n",
    "\n",
    "    <br>\n",
    "\n",
    "    - **task**: The type of task we are trying to model\n",
    "        - We currently support single task and multitask regression. Passing in \"regression\" into this argument with different output_dim’s handles these cases.\n",
    "        - We currently support binary and multiclass classification. Binary can be run with \"binary_classification\" and multiclass can be run with \"multiclass_classification\"\n",
    "\n",
    "    <br>\n",
    "\n",
    "    - **aggr**: The way to aggregate information from multiple stranded inputs (*ds* and *ss* models)\n",
    "        - \"avg\": take the average value of each output neuron across the strands\n",
    "        - \"max\" : take the max value for each output neuron across the strands\n",
    "        - \"concat\" : concat the representation learned prior to the output. For networks that have multiple modules (e.g. `Hybrid` models, you can separate the different possible concatenations by adding a suffix (e.g. \"concat_cnn\" means concatenate the representation learned after the CNN module of a `Hybrid` model) \n",
    "\n",
    "    <br>\n",
    "\n",
    "    - **loss_fxn** : The loss function to use. We currently support: \n",
    "        - \"mse\": mean squared error\n",
    "        - \"poisson\": poisson negative log likelihood loss\n",
    "        - \"bce\": binary cross entropy loss\n",
    "        - \"cross_entropy\": cross entropy loss\n",
    "\n",
    "    <br>\n",
    "\n",
    "* Current models in EUGENe assume a single stranded (ss), regression model (regression) that is trained to optimize mean squared error (mse) by default.\n",
    "\n",
    "* `forward`\n",
    "    - The requirement of the forward function are that it can handle at least a single strand as input of length `input_len` and that it outputs vector of values of dimension equivalent to `output_dim`. \n",
    "    - To be compatible with EUGENe’’s baseline training functionality, the forward function should take in both the forward (x) and reverse strand (x_rev) as arguments. Note that the model needs to take in `x` and `x_rev_comp` as arguments with `x_rev_comp` defaulting to `None`. Even if your model takes in only the forward strand (i.e. does not use \"ds\" or \"ts\" modes), this needs to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from eugene.models.base import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mBaseModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstrand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'regression'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maggr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloss_fxn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lr_scheduler'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscheduler_patience\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhp_metric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base model class to be inherited by all models in EUGENe\n",
      "\n",
      "Paramters:\n",
      "----------\n",
      "\n",
      "input_len (int):\n",
      "    length of input sequence\n",
      "output_dim (int):\n",
      "    number of output dimensions\n",
      "strand (str):\n",
      "    strand of the input sequence\n",
      "task (str):\n",
      "    task of the model\n",
      "aggr (str):\n",
      "    aggregation method for the input sequence\n",
      "loss_fxn (str):\n",
      "    loss function to use\n",
      "hp_metric (str):\n",
      "    metric to use for hyperparameter tuning\n",
      "kwargs (dict):\n",
      "    additional arguments to pass to the model\n",
      "\u001b[0;31mFile:\u001b[0m           /mnt/beegfs/users/aklie/projects/EUGENe/eugene/models/base/_base_model.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     FCN, CNN, RNN, Hybrid, DeepBind, DeepSEA, Jores21CNN, Kopp21CNN, TutorialCNN, TutorialCNN, ...\n"
     ]
    }
   ],
   "source": [
    "BaseModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorialCNN(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_len: int,\n",
    "        output_dim: int,\n",
    "        strand: str = \"ss\",\n",
    "        task: str = \"regression\",\n",
    "        aggr: str = \"avg\",\n",
    "        loss_fxn: str = \"mse\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Don't worry that we don't pass in the class name to the super call (as is standard for creating new\n",
    "        # nn.Module subclasses). This is handled by inherting BaseModel\n",
    "        super().__init__(\n",
    "            input_len, \n",
    "            output_dim, \n",
    "            strand=strand, \n",
    "            task=task, \n",
    "            aggr=aggr, \n",
    "            loss_fxn=loss_fxn,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Define the layers of the model\n",
    "        self.conv1 = nn.Conv1d(4, 30, 21)\n",
    "        self.dense = nn.Linear(30, 1)\n",
    "        self.sigmoid = nn.Sigmoid()        \n",
    "            \n",
    "            \n",
    "    # Define the forward pass of the model/\n",
    "    # Note how you need to use the x_rev_comp argument if you want to use the reverse complement of the sequence, \n",
    "    # but this can be ignored if the model is only meant to take in a single strand as input\n",
    "    def forward(self, x, x_rev_comp=None):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, x.size()[-1]).flatten(1, -1)\n",
    "        x = self.dense(x)\n",
    "        x = self.sigmoid(x)\n",
    "        if self.strand == \"ds\":\n",
    "            x_rev_comp = F.relu(self.conv1(x_rev_comp))\n",
    "            x_rev_comp = F.max_pool1d(x_rev_comp, x_rev_comp.size()[-1]).flatten(1, -1)\n",
    "            x_rev_comp = self.dense(x_rev_comp)\n",
    "            x_rev_comp = self.sigmoid(x_rev_comp)\n",
    "            x = (x + x_rev_comp / 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test the forward pass\n",
    "Its often helpful to run a simple forward pass of the model with some dummy data to make sure all your matrix multiplication and other operations are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of strand\n",
    "x_len = 66\n",
    "\n",
    "# Generate some random input\n",
    "x = torch.randn(10, 4, x_len)\n",
    "x_rev = torch.randn(10, 4, x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "model = TutorialCNN(input_len=x_len, output_dim=1, strand=\"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5254],\n",
       "        [0.4977],\n",
       "        [0.5097],\n",
       "        [0.5046],\n",
       "        [0.5201],\n",
       "        [0.4770],\n",
       "        [0.5340],\n",
       "        [0.5367],\n",
       "        [0.3727],\n",
       "        [0.3681]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, x_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test a PL trainer\n",
    "If your model is a BaseModel instance and you are working with SeqData objects, this is as simple as a call to the `fit` function within the `train` model.\n",
    "\n",
    "If you want to work directly with PL trainers, this is a little more complicated but still not too bad! You just need to create an appropriate DataLoader for your implementation (which can be converted form a SeqData object) and pass the model and the dataloader to a PL trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `train.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbafa338adc940ca96b4e874177eed0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One-hot encoding sequences:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | hp_metric | R2Score | 0     \n",
      "1 | conv1     | Conv1d  | 2.6 K \n",
      "2 | dense     | Linear  | 31    \n",
      "3 | sigmoid   | Sigmoid | 0     \n",
      "--------------------------------------\n",
      "2.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqData object modified:\n",
      "\tohe_seqs: None -> 1000 ohe_seqs added\n",
      "SeqData object modified:\n",
      "\tohe_rev_seqs: None -> 1000 ohe_rev_seqs added\n",
      "SeqData object modified:\n",
      "    seqs_annot:\n",
      "        + train_val\n",
      "Dropping 0 sequences with NaN targets.\n",
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /cellar/users/aklie/tutorial_logs/test_fit/add_model_tutorial/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436c42baf499410f9f376fc1c627cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n",
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5253a971ea4bc9a8839c764049afaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a417d5212c124c088fd88e436a942a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdata = eu.datasets.random1000()\n",
    "eu.pp.ohe_seqs_sdata(sdata)\n",
    "eu.pp.reverse_complement_seqs_sdata(sdata)\n",
    "eu.pp.train_test_split_sdata(sdata)\n",
    "eu.train.fit(model, sdata, target_keys=\"activity_0\", epochs=1, name=\"test_fit\", version=\"add_model_tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly using PL trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct access to PL trainer\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and val\n",
    "sdata_train = sdata[sdata[\"train_val\"]]\n",
    "sdata_val = sdata[~sdata[\"train_val\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No transforms given, assuming just need to tensorize.\n",
      "No transforms given, assuming just need to tensorize.\n"
     ]
    }
   ],
   "source": [
    "# Make some dataloaders\n",
    "sdataloader_train = sdata_train.to_dataset(target_keys=\"activiy_0\").to_dataloader(batch_size=32)\n",
    "sdataloader_val = sdata_val.to_dataset(target_keys=\"activity_0\").to_dataloader(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n"
     ]
    }
   ],
   "source": [
    "# Define a trainer by hand\n",
    "trainer = Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model, train_dataloaders=sdataloader_train, val_dataloaders=sdataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Adding you model to EUGENe\n",
    "Once you are happy with how your model seems to be working, you can add it to the appropriate `.py` file within EUGENe. \n",
    "- `_base_models.py`: This is meant for implementations of flexible architectures that are at the core of deep learning across fields. This might be something like a vanilla autoencoder, where you can change the number of hidden layers and units in the encoder or decoder.\n",
    "\n",
    "- `_sota_models.py`: These are often specific instances of the the above Base Models. Often these models have architectures that don't quite fit within the mold of the Base Models (e.g. DeepBind models that concatenate global and average pooling layers), but can also just be calls to Base Models with a specific configuration of hyperparameters (an example of the latter might be the DeepSEA architecture, which could be created with a specific call to a CNN). There also must be some basis for calling this a SOTA architecture. I realize this is somewhat arbitrary and I could probably have endless debates with people about what this means, but I typically use the rule that I know a SOTA architecture when I see one (e.g. if you are reading this you probably know what DeepSEA is).\n",
    "\n",
    "- `_custom_models.py`: These are custom architectures that don't really fall under the Base Models or SOTA Models. These might be published models that were successful on a particular dataset, or your own custom architecture you just want to be able to use and test within EUGENe. A note for the latter. In order for the a custom model to make it into a future release of EUGENe, there should be some basis for its inclusion. That is, you should be able to demonstrate the utility of the architecure on some real world data.\n",
    "\n",
    "I've already went ahead and added the `TutorialCNN` to the Custom Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create a unit test for your model\n",
    "* In order for your model to make it into the next EUGENe release, it needs to have a unit test within the [`test_models.py` file](https://github.com/adamklie/EUGENe/blob/main/tests/test_models.py). At a minimum this unit test should test the instantiation of your model and the training procedure of your choice on some dummy data. Check out the unit tests already there for more examples.\n",
    "\n",
    "As is the general rule for testing, the more \"units\" you can test the better. Feel free to add other tests as well. One other area that might be a little tricky is making sure the convolutional filters of your model are seen by the `generate_pfms` function in the `interpret` module. \n",
    "\n",
    "Don't forget to actually run your tests as well! This can be done with the following command\n",
    "\n",
    "```bash\n",
    "pytest tests/test_models.py -k \"test_TutorialCNN\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Document your function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your happy with your model, you've tested it and it's working as expected, you can add documentation to the function. This is done by adding a docstring to the function. The docstring should be formatted in [numpydoc](https://numpydoc.readthedocs.io/en/latest/format.html) format.\n",
    "\n",
    "```python\n",
    "\"\"\"Tutorial CNN model\n",
    "\n",
    "    This is a very simple one layer convolutional model for testing purposes. It is featured in testing and tutorial\n",
    "    notebooks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_len : int\n",
    "        Length of the input sequence.\n",
    "    output_dim : int\n",
    "        Dimension of the output.\n",
    "    strand : str, optional\n",
    "        Strand of the input. Only ss is supported for this model\n",
    "    task : str, optional\n",
    "        Task of the model. Either \"regression\" or \"classification\".\n",
    "    aggr : str, optional\n",
    "        Aggregation method. This model only supports \"avg\"\n",
    "    loss_fxn : str, optional\n",
    "        Loss function.\n",
    "    **kwargs\n",
    "        Keyword arguments to pass to the BaseModel class.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Add information to the EUGENe model Notion database\n",
    "If you want to help me in my never ending quest/addiction to organize things, please consider adding the details of your new model to [this](https://www.notion.so/44cca45b45cd41c2b06b74b9ca6242da?v=235befda27d54f9eaa85dafeaad1be3b) Notion database. Check out the examples already there for how to format your entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. (Optional) Submit a pull request\n",
    "You only need to do this if you want to share your model with the world (which is strongly encouraged)!\n",
    "\n",
    "Once you've completed all of the above steps, you can submit a pull request to the EUGENe repository. We will review your pull request and merge it into the main branch if everything looks good. If there are any issues, we will let you know and you can make the necessary changes. Once your pull request is merged, your model will be available in the next release of EUGENe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. More advanced training techniques\n",
    "The beauty of using PyTorch Lightning under the hood is that the framework has allowed us to create an abstraction from the basic training details that become boilerplate with most models while giving us the flexibility to make changes in a simple modular manner. You can find a slightly more involved description of advanced training in the documentation for EUGENe and we will be adding functionality and examples in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "Hopefully this guide was helpful in getting you started with adding your own model to EUGENe. If you have any questions, feel free to open a GitHub issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
