{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a88efe-ebb3-424c-8704-fb4ee189af2c",
   "metadata": {},
   "source": [
    "# Adding a dataset to EUGENe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d058003-a667-4b45-9ddc-e47efe48e0e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *09/29/2022*\n",
    "***\n",
    "**Description:**\n",
    "This tutorial is intended to show how to add a dataset to EUGENe. It's a pretty simple process, but it's important to follow the steps in order to ensure that the dataset is properly formatted and can be used by the EUGENe pipeline.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eugene as eu\n",
    "\n",
    "eu.settings.dataset_dir = \"./tutorial_datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Identify your dataset\n",
    "EUGENe currently supports handling any dataset where you have a set of DNA or RNA sequences and a set of labels for those sequences. A couple notes:\n",
    "\n",
    "* EUGENe currently supports directly reading from CSV files, numpy compressed files, FASTA files, BED files, BAM files and BigWig files. If you have a dataset in a different format, you will need to convert it to one of these formats and then use EUGENe to read it.\n",
    "* BED, BAM and BigWig have labels that are inherent to those files, so you often you don't need to provide a separate file with labels. However, you will need to have the right combination of files to get the labels you want. See the readthe docs page on dataloading for more information.\n",
    "* The labels can be any type of information, but should be aligned to your sequences in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a loader function\n",
    "* This should be a normal Python function that should be named by the following convention:\n",
    "    - If your dataset is associated with a publication, use the last name of the first author followed by the year of publication (nameYY). For example, if the dataset was published in 2021 by the author \"Jane Doe\", the function should be named `doe21`.\n",
    "    - If your dataset is not associated with a publication, you can come up with the name of the dataset followed by the year of creation (nameYY). For example, if the dataset was created in 2021 and you've named it \"eugene\", the function should be named `eugene21`.\n",
    "* Implementing the function\n",
    "    - At minimum, the function should return a SeqData object to the user that contains either `seqs` or `ohe_seqs`.\n",
    "    - The SeqData should also have `names` that can be from the dataset or just `seq1`, `seq2`, etc.\n",
    "    - The SeqData should also have targets to predict in the `seqs_annot` field. These can be any type of information, but should be aligned to your sequences in some way.\n",
    "    - The dataset should be downloaded to the users current machine and then loaded in. You can use whatever method you want to download the dataset, but we offer a helper function (`try_download_urls`) that will try to download the dataset from a list of URLs. If the dataset is not available at any of the URLs, it will raise an error.\n",
    "    - You are welcome to create your own functions for loading in different files and datatypes, but we have several dataload functions in the `_io.py` script that you can use to load in different types of files. See the read the docs page on the `dataload` module for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eugene helpers I most often use for adding a new dataset\n",
    "from eugene.datasets._utils import try_download_urls\n",
    "from eugene.dataload import read_csv\n",
    "from eugene.dataload import SeqData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, I will show how I added the dataset available at https://zenodo.org/record/6863861#.YzcG9exKglU. This dataset comes from Farley et al (2015), so I will name corresponding function `farley15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for downloading the farley15\n",
    "def farley15(\n",
    "    return_sdata=True, \n",
    "    **kwargs: dict\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # We typically start with a url list. We had to create a Zenodo archive for this dataset\n",
    "    urls_list = [\n",
    "        \"https://zenodo.org/record/6863861/files/farley2015_seqs.csv?download=1\",\n",
    "        \"https://zenodo.org/record/6863861/files/farley2015_seqs_annot.csv?download=1\",\n",
    "    ]\n",
    "\n",
    "    # We then use a helper function to try downloading the files\n",
    "    paths = try_download_urls([0, 1], urls_list, \"farley15\")\n",
    "\n",
    "    # If specified, we return a SeqData object\n",
    "    if return_sdata:\n",
    "        # Here we just read in the first csv file\n",
    "        path = paths[0]\n",
    "        seq_col = \"Enhancer\"\n",
    "        data = read_csv(\n",
    "            path,\n",
    "            sep=\",\",\n",
    "            seq_col=seq_col,\n",
    "            auto_name=True,\n",
    "            return_dataframe=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        # Make some cosmetic tweaks, build a SeqData and return it\n",
    "        n_digits = len(str(len(data) - 1))\n",
    "        ids = np.array([\"seq{num:0{width}}\".format(num=i, width=n_digits) for i in range(len(data))])\n",
    "        sdata = SeqData(\n",
    "            seqs=data[seq_col],\n",
    "            names=ids,\n",
    "            seqs_annot=data[[\"Barcode\", \"Biological Replicate 1 (RPM)\", \"Biological Replicate 2 (RPM)\"]],\n",
    "        )\n",
    "        return sdata\n",
    "    \n",
    "    # Otherwise we just point the user to where we downloaded the file\n",
    "    else:\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test your function\n",
    "* You should test your function to make sure that it works as expected. \n",
    "    - You can first do this by running the function in a Jupyter notebook and then checking the output. \n",
    "    - We also require that you write a test function that will be run by our continuous integration (CI) pipeline. This test function should be named `test_{name of loader function}`. For example, the test function for the `farley15` function would be named `test_farley15`. This function should be added to the `test_dataload.py` script in the `tests` folder.\n",
    "    - Test multiple aspects of your function\n",
    "        * Does it load in the correct number of sequences?\n",
    "        * Does it load in the targets?\n",
    "        * Does it have proper names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_farley15():\n",
    "    sdata = farley15()\n",
    "    assert sdata.n_obs == 163708\n",
    "    assert \"seq163707\" == sdata.names[-1]\n",
    "    assert sdata.seqs_annot.shape == (163708, 3)\n",
    "    sdata_path = farley15(return_sdata=False)[0]\n",
    "    assert os.path.exists(sdata_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset farley15 farley2015_seqs.csv has already been downloaded.\n",
      "Dataset farley15 farley2015_seqs_annot.csv has already been downloaded.\n",
      "Dataset farley15 farley2015_seqs.csv has already been downloaded.\n",
      "Dataset farley15 farley2015_seqs_annot.csv has already been downloaded.\n"
     ]
    }
   ],
   "source": [
    "test_farley15()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've confirmed everything works as anticipated, you can add your function to the `datasets` module.\n",
    "This involves just copying the function into the `_datasets.py` script in the `datasets` module. Then add the function to the import statement in the module's `__init__.py` script.\n",
    "\n",
    "Don't forget to add your test function to the `test_datasets.py` script in the `tests` folder as well! You can then run the tests to make sure everything works as expected.\n",
    "\n",
    "```bash\n",
    "pytest tests/test_datasets.py -k \"test_farley15\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Document your function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your happy with your function, you've tested it and it's working as expected, you can add documentation to the function. This is done by adding a docstring to the function. The docstring should be formatted in numpydoc format and should contain a parameters and a returns section. Don't worry too much about adding details for the dataset, that is done in the next step! You can see examples of this in the `_datasets.py` script. You can also see examples of this in the `datasets` module's read the docs page.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Reads in the farley15 dataset.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "return_sdata : bool, optional\n",
    "    If True, return SeqData object for the farley15 dataset. The default is True.\n",
    "    If False, return the paths to any downloaded files.\n",
    "**kwargs : kwargs, dict\n",
    "    Keyword arguments to pass to read_csv.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "sdata : SeqData\n",
    "    SeqData object for the farley15 dataset.\n",
    "\"\"\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add information to `datasets.csv`\n",
    "The last thing to do is to add your dataset to the `datasets.csv` file in the `datasets` folder. This file helps keep track of the datasets that are available in EUGENe and allows users to view them. The columns in the file are as follows:\n",
    "* name (required): The name of the dataset. This should be the same as the name of the function.\n",
    "* n_seqs (required): The number of sequences in the dataset.\n",
    "* n_targets (required): The number of targets in the dataset.\n",
    "* metadata (optional): Any additional information about the dataset stored in the `seqs_annot` field of the SeqData object.\n",
    "* url (required): The URL pointing to the dataset. This doesn't necessarily have to be the URL where the dataset is hosted, but it should include these URLs if available.\n",
    "* description (required): A short description of the dataset. At minimum, this should include the type of data (DNA or RNA), the type of targets (e.g. binding affinity, gene expression, etc.) and a few sentences about how the dataset was generated. If there is a publication associated with the dataset, you can include a link to the publication in the description.\n",
    "* author (required): Your name and email address. This is so we can contact you if there are any issues with the dataset.\n",
    "* info_page (optional): A URL pointing to a page with more information about the dataset. I often build Notion pages for my datasets, but you can use whatever you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit a pull request\n",
    "Once you've completed all of the above steps, you can submit a pull request to the EUGENe repository. We will review your pull request and merge it into the main branch if everything looks good. If there are any issues, we will let you know and you can make the necessary changes. Once your pull request is merged, your dataset will be available in the next release of EUGENe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "Hopefully this guide was helpful in getting you started with adding your own dataset to EUGENe. If you have any questions, feel free to open a GitHub issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
